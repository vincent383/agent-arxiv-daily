## Updated on 2024.07.24
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#agent>agent</a></li>
    <li><a href=#llm>llm</a></li>
    <li><a href=#rag>rag</a></li>
  </ol>
</details>

## agent

|Publish Date|Title|Authors|PDF|Code|abstract|
|---|---|---|---|---|---|
|**2024-07-23**|**LawLuo: A Chinese Law Firm Co-run by LLM Agents**|Jingyun Sun et.al.|[2407.16252](http://arxiv.org/abs/2407.16252)|null|大型语言模型(LLM)凭借其卓越的文本理解和生成能力，在为非法律背景用户提供法律咨询服务方面展现出巨大潜力。然而，现有的中文法律LLM局限于单一轮模型与用户的对话，无法复现律师事务所中多名员工共同参与咨询的协作模式，这阻碍了真实咨询体验的实现。此外，现有中文法律LLM存在关键局限：(1)对指令微调数据质量控制不足；(2)用户模糊查询导致模型产生幻觉的可能性增加；以及(3)在多轮对话中，模型遵循指令的能力下降。为应对这些挑战，我们提出了一种创新的法律对话框架，利用多个LLM代理的协作能力，命名为LawLuo。该框架包括四个代理：接待员、律师、秘书和老板，各自承担不同的功能，协同向用户提供全面的法律咨询服务。此外，我们构建了两个高质量的法律对话数据集，KINLED和MURLED，并使用这些数据集对ChatGLM-3-6b进行了微调。我们提出了一种名为ToLC的法律查询澄清算法。实验结果表明，LawLuo在三个方面超越了基线LLM，包括GPT-4：律师般的语言风格、法律建议的实用性以及法律知识的准确性。我们的代码和数据集可在https://github.com/NEFUJing/LawLuo获取。|
|**2024-07-21**|**Multi-Agent Causal Discovery Using Large Language Models**|Hao Duong Le et.al.|[2407.15073](http://arxiv.org/abs/2407.15073)|null|大型语言模型(LLM)通过利用从广泛文本语料库中获得的丰富专家知识，在因果发现任务中展现出巨大潜力。然而，LLM在多智能体因果发现方面的能力尚未得到充分探索。本文提出了一种通用框架，旨在探究这一潜力。首先，元智能体模型完全依赖于LLM智能体之间的推理和讨论来进行因果发现。其次，编码智能体模型则利用智能体编写、规划和执行代码的能力，借助先进的统计库进行因果发现。第三，混合模型结合了元智能体模型和编码智能体模型的方法，整合了多个智能体的统计分析能力和推理技能。我们提出的框架通过有效利用LLM的专家知识、推理能力、多智能体协作以及统计因果方法，取得了有希望的结果。通过探索LLM在多智能体环境下的潜力，我们旨在为后续研究奠定基础，以利用LLM的多智能体特性解决与因果相关的问题。|
|**2024-07-19**|**KoMA: Knowledge-driven Multi-agent Framework for Autonomous Driving with Large Language Models**|Kemou Jiang et.al.|[2407.14239](http://arxiv.org/abs/2407.14239)|null|大型语言模型(LLM)作为自主代理为通过知识驱动的方式应对现实世界挑战开辟了新的途径。这些LLM增强的方法在泛化和可解释性方面表现出色。然而，驾驶任务的复杂性往往需要多个、异构代理的合作，突显了LLM驱动的代理需要进行合作知识共享和认知协同的需求。尽管LLM前景广阔，但当前的应用主要集中在单一代理场景。为了拓宽知识驱动策略的视野并加强自主代理的泛化能力，我们提出了KoMA框架，该框架包括多代理交互、多步规划、共享内存和基于排名的反思模块，以增强多代理在复杂驾驶场景中的决策制定。基于框架生成的驾驶场景文本描述，多代理交互模块使LLM代理能够分析和推断周围车辆的意图，类似于人类的认知过程。多步规划模块使LLM代理能够逐层分析并获得最终行动决策，确保短期行动决策的目标一致性。共享内存模块可以积累集体经验，做出更优决策，而基于排名的反思模块则能评估和改进代理行为，旨在提高驾驶安全性和效率。KoMA框架不仅增强了自动驾驶代理的鲁棒性和适应性，而且显著提升了它们在各种场景下的泛化能力。实证结果证明了我们方法的优越性，特别是在处理复杂、不可预测的驾驶环境时，无需进行大量重新训练。  请注意，上述翻译已尽可能地遵循了您的要求，未包含任何无关内容，并且在输出的内容中避免使用了","字符。如果您有进一步的要求或需要对翻译进行调整，请随时告知。|
|**2024-07-17**|**Leveraging Environment Interaction for Automated PDDL Generation and Planning with Large Language Models**|Sadegh Mahdavi et.al.|[2407.12979](http://arxiv.org/abs/2407.12979)|null|大型语言模型（LLMs）在各种自然语言任务中展现出卓越的性能，但在需要结构化推理的规划问题上往往遇到挑战。为解决这一局限，已提议将规划问题转换为规划领域定义语言（PDDL）作为潜在解决方案，以利用自动化规划器。然而，准确生成PDDL文件通常需要人工输入或校正，这既耗时又昂贵。在这篇论文中，我们提出了一种新颖的方法，该方法结合了LLMs和环境反馈，能够自动生成PDDL领域的描述文件和问题描述文件，无需人类干预。我们的方法引入了一个迭代精炼过程，该过程生成多个问题PDDL候选，并基于与环境交互获得的反馈逐步完善领域PDDL。为了指导精炼过程，我们开发了探索行走（Exploration Walk，简称EW）指标，它为LLMs提供了丰富的反馈信号来更新PDDL文件。我们在PDDL环境中评估了我们的方法，平均任务解决率达到了66%，相比之下，GPT-4内在规划采用链式思考提示的任务解决率为29%。我们的工作使使用LLMs和环境反馈自动建模规划环境成为可能，消除了在PDDL生成过程中对人工干预的需求，为在具有挑战性的问题中构建更可靠的LLM代理铺平了道路。|
|**2024-07-16**|**Review-Feedback-Reason (ReFeR): A Novel Framework for NLG Evaluation and Reasoning**|Yaswanth Narsupalli et.al.|[2407.12877](http://arxiv.org/abs/2407.12877)|null|评估自然语言生成（NLG）输出的质量，如大型语言模型（LLMs）所产生的一样，面临着显著的挑战。传统的方法要么涉及资源密集型的人工评估，要么使用与人工判断相关性较低的自动指标。在本研究中，我们提出了一种名为“评论-反馈-推理”（ReFeR）的新型NLG评价框架，利用LLM代理。我们通过两个现有的基准数据集上对多种NLG任务进行严格测试，证明了ReFeR的有效性。该提出的框架不仅提高了NLG评价的准确性，超越了以往的基准约20%，而且还产生了建设性的反馈，并显著改善了集体推理能力。此反馈随后被用于创建指令微调数据集，当用于微调较小的模型，如Mistral-7B时，使其成为极其优秀的评估者，与人类评估的相关性和性能几乎与GPT-3.5相当。我们通过将其应用于三个推理基准测试中，展示了我们方法的有效性，在这些测试中，它超越了大多数最先进的方法，并且在平均上，其推理能力优于GPT-3.5 Turbo约11.67%和GPT-4约1%。|
|**2024-07-17**|**AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases**|Zhaorun Chen et.al.|[2407.12784](http://arxiv.org/abs/2407.12784)|**[link](https://github.com/BillChan226/AgentPoison)**|**大型语言模型(LLM)代理在各种应用中展现出卓越性能，这主要归功于其先进的推理能力、利用外部知识和工具、调用API以及执行行动以与环境互动的能力。当前的代理通常采用记忆模块或检索增强生成(RAG)机制，从知识库中检索具有相似嵌入的过往知识和实例，以指导任务规划和执行。然而，对未经验证的知识库的依赖引发了对其安全性和可信度的重大担忧。为了揭示这些脆弱性，我们提出了一种创新的红队方法AgentPoison，这是首次针对通用和RAG基LLM代理的后门攻击，通过污染其长期记忆或RAG知识库来实现。具体而言，我们将触发生成过程构造成受约束优化问题，以优化后门触发器，将其映射到独特的嵌入空间，从而确保只要用户指令包含优化后的后门触发器，恶意示例就会以高概率从被污染的记忆或知识库中检索出来。同时，不含触发器的良性指令仍能保持正常性能。与传统的后门攻击不同，AgentPoison无需额外的模型训练或微调，且优化后的后门触发器展现出优越的可转移性、上下文连贯性和隐蔽性。广泛的实验表明，在攻击三种真实世界中的LLM代理——基于RAG的自动驾驶代理、知识密集型问答代理和医疗保健EHR代理时，AgentPoison的有效性。在每种代理上，AgentPoison实现了超过80%的平均攻击成功率，对良性性能的影响极小(不到1%)，且毒化率低于0.1%。**|
|**2024-07-16**|**InferAct: Inferring Safe Actions for LLM-Based Agents Through Preemptive Evaluation and Human Feedback**|Haishuo Fang et.al.|[2407.11843](http://arxiv.org/abs/2407.11843)|null|在现实应用中部署基于大语言模型(LLM)的代理时，防止高风险或不可逆转错误的发生是至关重要的。然而，现有研究在预判LLM代理执行推理路径的安全性方面存在不足，导致安全可靠运行的保障出现缺口。为寻求更优解决方案，本文提出了一种名为InferAct的新方法，该方法利用了LLM的“心理理论”能力，能够在关键行动（如自动在线交易或网购中的“立即购买”）执行前主动检测潜在错误。同时，InferAct能够整合人类反馈，以避免不可逆风险，并提升行为代理的决策过程。通过在三个广泛应用的任务上的实验，证明了InferAct的有效性。该方案提出了一种创新的方法和实质性的贡献，旨在开发出可在涉及关键决策的不同环境中安全部署的LLM代理。|
|**2024-07-16**|**How Personality Traits Influence Negotiation Outcomes? A Simulation based on Large Language Models**|Yin Jou Huang et.al.|[2407.11549](http://arxiv.org/abs/2407.11549)|null|心理证据揭示了人格特质对决策的影响。例如，随和性通常与谈判中的积极结果相关，而神经质则往往导致较差的结果。本文介绍了一个以大型语言模型（LLM）代理人为中心的模拟框架，这些代理人被赋予合成的人格特质，在讨价还价的领域进行谈判，并具有可定制的个性和目标。实验结果表明，基于LLM的模拟的行为倾向能够重现人类谈判中观察到的行为模式。我们的贡献是双重的：首先，我们提出了一种模拟方法，研究了LLM代理人的语言和经济能力之间的对齐；其次，我们提供了关于大五人格特质对双边谈判结果的战略影响的经验见解。此外，我们还提供了一个基于合成讨价还价对话的案例研究，揭示了一些有趣的行为，包括欺骗性和妥协行为。|
|**2024-07-16**|**Sibyl: Simple yet Effective Agent Framework for Complex Real-world Reasoning**|Yulong Wang et.al.|[2407.10718](http://arxiv.org/abs/2407.10718)|**[link](https://github.com/ag2s1/sibyl-system)**|**基于大型语言模型(LLM)的现有代理通过整合LLM内在的知识、强大的情境学习和零样本能力，以及结合精心设计的人工LLM调用工作流和工具使用，展现出强大的问题解决能力。然而，这些代理在长期推理方面仍存在不足，并未充分利用现有工具的潜力，导致在复杂的现实世界推理场景中出现明显缺陷。为了解决这些限制，我们引入了Sibyl，一个简单而强大的基于LLM的代理框架，旨在通过高效利用最少的工具来处理复杂的推理任务。受到全局工作空间理论的启发，Sibyl集成了全局工作空间，以增强系统内知识和对话历史的管理和共享。此外，遵循心智社团理论的指导，Sibyl实施了一个基于多代理辩论的陪审团，以自我精炼最终答案，确保全面和均衡的方法。这种方法旨在减少系统复杂性，同时扩展可解决的问题范围——从通常需要人类几分钟就能解决的问题到那些可能需要数小时甚至数天的问题，从而促进从系统1向系统2思维的转变。Sibyl从设计之初就注重可扩展性和易于调试，通过融入函数式编程中的再入概念，旨在实现与其他LLM应用的无缝且低努力集成，以提升其能力。我们的实验结果表明，在GAIA基准测试集中，使用GPT-4实例化的Sibyl代理取得了最前沿的表现，平均得分为34.55%，超过了其他基于GPT-4的代理。我们希望Sibyl能够激发更多可靠和可重用的基于LLM的代理解决方案，以应对复杂的现实世界推理任务。**|
|**2024-07-15**|**Leveraging Hybrid Intelligence Towards Sustainable and Energy-Efficient Machine Learning**|Daniel Geissler et.al.|[2407.10580](http://arxiv.org/abs/2407.10580)|null|混合智能旨在通过结合人类认知能力和人工智能的优势，增强决策制定、问题解决和整体系统性能。随着大型语言模型(LLM)逐渐作为智能代理参与以加速机器学习发展，混合智能正成为促进人机有效互动的关键议题。本文提出了一种利用混合智能推动可持续、能源意识型机器学习的方法。在开发机器学习模型时，往往仅关注最终模型的性能，而忽视了过程本身的效率。近年来，由于复杂、大规模计算过程对环境造成重大影响，能源效率变得同样重要。本工作的贡献在于，通过人机协同(HITL)和LLM代理的交互式引入次级知识源，突出并进一步解决机器学习开发过程中的效率问题。  混合智能的目标是通过融合人类智慧与AI技术，提升决策质量、解决问题的能力以及系统的整体效能。随着大型语言模型在机器学习领域的广泛应用，其作为智能助手的角色日益凸显，促使混合智能在促进人机高效协作方面发挥着核心作用。本文介绍了一种运用混合智能促进可持续且注重能源效率的机器学习策略。在构建机器学习模型的过程中，通常过分强调最终模型的性能表现，而对研发流程的效率考量不足。近来，鉴于大规模计算任务对环境造成的显著影响，能源效率成为了不可忽视的考量因素。本文的主要贡献在于，通过集成人机协同工作模式（HITL）及大型语言模型，有效地引入辅助知识资源，以此识别并优化机器学习开发阶段的效率瓶颈。|
|**2024-07-15**|**CIBench: Evaluating Your LLMs with a Code Interpreter Plugin**|Songyang Zhang et.al.|[2407.10499](http://arxiv.org/abs/2407.10499)|**[link](https://github.com/open-compass/CIBench)**|**尽管基于大型语言模型（LLM）的代理在利用外部工具解决复杂问题方面取得了显著进展，但评估其能力的基准测试却充满挑战，这阻碍了对其局限性的清晰理解。在这篇论文中，我们提出了一种交互式评估框架，名为CIBench，旨在全面评估LLM利用代码解释器进行数据科学任务的能力。我们的评估框架包括一个评估数据集和两种评估模式。评估数据集通过LLM与人类合作的方式构建，并通过连续且交互式的IPython会话模拟真实的流程。两种评估模式分别检测LLM在有无人类帮助下使用代码解释器的能力。我们进行了广泛实验，分析了24个LLM在CIBench上的表现，并为未来LLM在代码解释器运用上提供了宝贵见解。**|
|**2024-07-14**|**All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era**|Bo Chen et.al.|[2407.10081](http://arxiv.org/abs/2407.10081)|null|推荐系统对于管理信息过载和提供个性化内容至关重要，能够响应用户多样化的信息需求。随着大型语言模型(LLM)的出现，为重新定义推荐系统提供了新的机遇，这些系统拥有庞大的通用知识和推理能力。站在这一LLM时代的前沿，我们的目标是将推荐系统融入更广阔的图景，并为未来的研究铺平道路，以实现更加全面的解决方案。因此，我们首先提供了推荐系统技术进展的全面概述，特别关注语言基础模型及其在推荐领域的应用。我们确定了现代推荐系统的两条进化路径——通过列表式推荐和对话式推荐。这两条路径最终汇聚于具有卓越的长期记忆、反思能力和工具智能的LLM代理上。沿着这两条路径，我们指出推荐的信息有效性得到提升，而用户的获取成本则降低。我们仔细研究了每个里程碑的技术特征、研究方法和内在挑战，从传统的列表式推荐到LLM增强的推荐，再到使用LLM代理的推荐。最后，我们强调了几项对于未来个性化技术与界面发展至关重要的未解决挑战，并讨论了未来的前景。  请注意，以上文本是根据您的要求翻译的论文摘要的中文版本。|
|**2024-07-14**|**Revolutionizing Bridge Operation and maintenance with LLM-based Agents: An Overview of Applications and Insights**|Xinyu-Chen et.al.|[2407.10064](http://arxiv.org/abs/2407.10064)|null|在人类社会发展的各个工业领域，人们一直在探索解放人力的方法。构建基于大规模语言模型的代理被视为实现这一目标的有效工具之一。作为具有感知、规划、决策和行动能力的人工智能实体，代理已在许多领域创造了巨大的生产价值。然而，桥梁运营与维护（O&M）领域的智能化水平相较于其他行业显得较低。尽管如此，该领域已发展出多种智能检测设备、机器学习算法以及自主评估与决策方法，为人工智能在此领域的突破提供了可行的基础。  本研究旨在探讨基于大规模语言模型的人工智能体对桥梁运营与维护领域的影响，并分析其为该领域核心任务带来的潜在挑战与机遇。通过深入的研究与分析，本文期望为理解该领域智能化应用提供一个更为全面的视角。  请注意，以上翻译已按照要求不包含“,”字符。|
|**2024-07-11**|**Incorporating Large Language Models into Production Systems for Enhanced Task Automation and Flexibility**|Yuchen Xia et.al.|[2407.08550](http://arxiv.org/abs/2407.08550)|null|这篇论文提出了一种新颖的方法，旨在将大型语言模型（LLMs）整合到自动化生产系统中，以提升任务自动化和灵活性。我们根据自动化金字塔构建生产操作的层级结构，将原子操作功能抽象为微服务，并通过专用的数字孪生系统进行调用执行。这为协调生产流程提供了可扩展且灵活的基础。在数字孪生系统中，低层次的、硬件特定的数据被赋予语义，使得LLMs能够理解和处理生产计划与控制任务。当接收到用户请求或识别到触发事件时，LLMs会生成生产流程计划，然后将其分解为一系列微服务，在现实世界的自动化系统中执行。我们在实验室的模块化自动化设施上实现了这一整体方法，通过一个实际案例展示了LLMs如何处理生产规划和控制任务，从而实现了一个直观、自动化程度高且更具灵活性的生产环境。最后，我们指出了实现LLMs在自主系统中的全部潜力所面临的局限性，并强调了其潜在的有益之处。有关此系列研究的演示可在以下链接访问：https://github.com/YuchenXia/GPT4IndustrialAutomation。|
|**2024-07-11**|**PrefCLM: Enhancing Preference-based Reinforcement Learning with Crowdsourced Large Language Models**|Ruiqi Wang et.al.|[2407.08213](http://arxiv.org/abs/2407.08213)|null|## 翻译  偏好驱动的强化学习（PbRL）作为一种新兴的方法，通过人类比较反馈教导机器人，避免了复杂的奖励工程的需求。然而，现有PbRL方法需要大量反馈，往往导致对由脚本教师生成的合成反馈的依赖，这又回到了复杂的奖励设计，并难以适应人类-机器人交互（HRI）场景中用户对同一任务的独特期望。为解决这些问题，我们提出了一种新颖的框架——PrefCLM，它利用大规模语言模型（LLMs）作为模拟教师参与PbRL。我们运用Dempster-Shafer理论在分数级别融合来自多个LLM代理的个人偏好，有效利用它们的多样性和集体智慧。同时，我们引入了一个用户参与的流程，以促进基于用户交互的集体精进。在各种通用强化学习任务中的实验结果显示，PrefCLM在性能上与传统脚本教师相当，并且在促进更自然、高效的机器人行为方面表现出色。一个现实世界的用户研究（N=10）进一步证明了它在个性化用户偏好的能力，显著提高了HRI场景中的用户满意度。|
|**2024-07-10**|**Flooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent Communities**|Tianjie Ju et.al.|[2407.07791](http://arxiv.org/abs/2407.07791)|**[link](https://github.com/Jometeorie/KnowledgeSpread)**|**随着大型语言模型（LLMs）在多代理系统中的迅速应用，它们在协作问题解决和自主谈判等领域的出色性能引起了关注。然而，这些基于LLM的多代理系统的安全问题尚未得到充分研究，尤其是在知识操纵传播方面。本文通过构建详细的威胁模型和模拟环境，模拟现实世界中的多代理部署在可信平台上，探讨这一关键问题。我们提出了一种新颖的两阶段攻击方法，包括说服性注入和操纵知识注入，来系统地探究在无明确提示操纵的情况下，如何潜在地传播操纵知识（如虚构和有害知识）。我们的方法利用了LLMs处理世界知识固有的漏洞，攻击者可以借此无意识地传播编造的信息。实验结果表明，我们的攻击方法能够成功诱导基于LLM的代理在交流中传播这两种操纵的知识，同时不会显著降低它们的基础功能。此外，我们发现这些操纵会持续存在于流行的检索增强生成框架中，即使交互结束，若干良性代理也可能继续受到操纵聊天记录的影响。我们的发现揭示了LLM基多代理系统中的重大安全风险，强调了对操纵知识传播进行强大防御的迫切需求，例如引入“守护”代理和先进的事实核查工具。**|
|**2024-07-09**|**Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks with Large Language Models**|Logan Cross et.al.|[2407.07086](http://arxiv.org/abs/2407.07086)|**[link](https://github.com/locross93/hypothetical-minds)**|**在多智能体强化学习（MARL）方法中，处理多智能体系统的非stationarity并适应在线学习的能力是一个挑战。为此，我们利用大型语言模型构建了一个自主的解决策略。我们的新型智能体“假设心智”（Hypothetical Minds）采用认知启发式架构，包括感知、记忆和两个抽象层次上的分层规划模块。其中的关键部分是“心理理论”模块，它通过自然语言生成对其他智能体策略的假设，并根据这些假设对其他智能体行为的预测进行评估和迭代优化。通过这种方式，假设心智在Melting Pot基准中的多种竞争、混合动机和协作环境中，无论是二元还是群体环境，都显著优于先前的语言模型智能体（LLM-agent）和强化学习基础线。对比实验还显示，假设的评估和精炼对于在复杂场景中取得成功至关重要。**|
|**2024-07-09**|**Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy**|Zhenyu Guan et.al.|[2407.06813](http://arxiv.org/abs/2407.06813)|null|## 背景 在人类社会中，外交是一种极其复杂的活动，涉及众多各方/行动者的互动，需要具备社会推理、谈判技巧和长期策略规划等多方面能力。以往的AI代理已经在处理多步骤游戏和大动作空间的多代理任务上展示了实力。然而，外交所涉及的决策空间范围惊人，特别是在需要谈判的阶段。近期，大型语言模型（LLM）在一些应用中展现出了超越前代的能力，但仍不足以应对复杂多代理环境中长时间的规划。借助尖端的LLM技术，我们首次尝试探索AI在如此全面的多代理使命中的上限，通过整合三个核心且关键的功能，以构建更强的基于LLM的社会性代理：1）具有记忆和反思的策略规划者；2）目标导向的、具备社会推理的谈判者；3）通过自我对弈游戏增强记忆，实现无人工干预的自我进化。|
|**2024-07-10**|**FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal Reinforcement for Enhanced Financial Decision Making**|Yangyang Yu et.al.|[2407.06567](http://arxiv.org/abs/2407.06567)|null|大型语言模型（LLMs）在执行复杂任务方面展现出显著潜力，并越来越多地应用于金融领域。然而，高质量的连续投资决策过程仍面临挑战，它需要与不断变化的环境进行多次交互，以最大化回报并管理风险。尽管已经开发出基于LLMs的代理系统，它们能够超越人类团队，实现投资收益，但如何优化多源信息整合和决策结果，通过实时经验改进，仍有待探索。为此，我们提出FinCon，一个专为多样化的金融任务设计的基于LLM的多代理框架，其特点在于概念化口头强化和财务组织结构的运用。  FinCon借鉴现实世界投资公司的组织架构，采用经理-分析师的沟通层次，促进跨职能代理间的协同合作，通过自然语言交流实现目标统一。每个代理都具备比人类更大的记忆容量，这有助于更高效的信息处理。此外，FinCon还引入了一个风险控制组件，定期启动自我批判机制，以更新系统的投资理念。这些概念化的信念作为口头强化，指导未来行为，并可根据需要选择性地传递给需要更新知识的节点，从而减少不必要的信息交流成本，提高性能。  FinCon在单一股票交易和资产管理等不同金融任务上表现出强大的泛化能力，证明了其在实际金融场景中的应用潜力。|
|**2024-07-08**|**Enhancing Language Model Rationality with Bi-Directional Deliberation Reasoning**|Yadong Zhang et.al.|[2407.06112](http://arxiv.org/abs/2407.06112)|null|该论文提出了一个新颖的推理方法——双向决策解放推理（BIDDER），旨在提升语言模型的决策合理性。传统推理方法通常依赖历史信息，采用单向（从左到右）的推理策略，这导致对潜在未来结果的认识不足，以及历史背景的整合不够充分，从而产生次优决策。BIDDER通过融合理性决策的原则，特别是处理不确定性并预测期望效用，弥补了这一短板。其方法包括三个关键步骤：从历史数据中推断隐藏状态，以表示决策过程中的不确定信息；利用这些隐藏状态预测未来的潜在状态和可能结果；结合历史信息（过去情境）和长期结果（未来情境），以指导推理。通过双向推理，BIDDER能够全面考虑过去和未来的情境，从而做出更明智、更理性的决策。我们在扑克（限注德州扑克）和谈判两个明确场景中测试了BIDDER的效果，实验显示它显著提高了语言模型和基于语言模型的代理的决策能力。|
|**2024-07-08**|**Affordances-Oriented Planning using Foundation Models for Continuous Vision-Language Navigation**|Jiaqi Chen et.al.|[2407.05890](http://arxiv.org/abs/2407.05890)|null|基于语言模型的代理在视觉导航（VLN）任务中展现出零样本的强大性能。然而，这些方法仅关注解决高层任务规划，通过选择预定义导航图中的节点进行移动，忽视了现实场景中低层次的控制。为了弥补这一不足，我们提出了AO-Planner，一个新颖的面向可及性规划的连续视觉导航框架。AO-Planner整合多种基础模型，实现面向可及性的运动规划和动作决策，均以零样本的方式执行。具体来说，我们采用了视觉可及性提示（VAP）方法，利用SAM分割可见地面，提供导航可及性信息，从而让语言模型选择潜在的下一个路标，并生成向选定路标的低层次路径规划。此外，我们引入了高级代理PathAgent，识别出最可能的像素级路径，并将其转换为三维坐标，以完成低层次的移动。  在具有挑战性的R2R-CE基准测试上，AO-Planner实现了最先进的零样本性能提升（SPL指标提高5.5%）。我们的方法有效连接了语言模型与三维世界，避免了直接预测世界坐标点的困难，为利用基础模型进行低层次运动控制提供了新的前景。|
|**2024-07-05**|**VRSD: Rethinking Similarity and Diversity for Retrieval in Large Language Models**|Hang Gao et.al.|[2407.04573](http://arxiv.org/abs/2407.04573)|null|在大型语言模型（LLMs）快速发展的背景下，向量检索算法对于满足相似度和多样性要求的语义查询至关重要。尽管Maximal Marginal Relevance（MMR）在涉及这两个需求的检索场景中被广泛应用，但其参数λ的变化会导致结果波动，使得向量空间中的优化路径变得模糊。此外，当前缺乏对相似性和多样性在检索过程中约束的坚实理论分析。本文提出了一种新方法，通过查询向量与求和向量之间的关系来刻画这两种约束。这种关系确保了相似性，同时要求求和向量中的各个向量以分散的方式与查询向量对齐，以满足多样性需求。  我们还提出了一个新的组合优化问题：从一组候选向量中选择 $k$ 个，使得它们的求和向量最大程度地与查询向量匹配。我们证明了这个问题是NP完全的，揭示了在向量检索中同时追求相似性和多样性的深刻困难，并为后续研究奠定了理论基础。此外，我们设计了一个名为Vectors Retrieval with Similarity and Diversity（VRSD）的启发式算法，它不仅具有明确的优化目标，无需预设参数，而且在时间复杂度上相对于MMR有所降低。实证验证表明，VRSD在各种数据集上显著优于MMR。|
|**2024-07-05**|**When LLMs Play the Telephone Game: Cumulative Changes and Attractors in Iterated Cultural Transmissions**|Jérémy Perez et.al.|[2407.04503](http://arxiv.org/abs/2407.04503)|**[link](https://github.com/jeremyperez2/telephonegamellm)**|**随着大型语言模型（LLMs）之间的互动增加，它们在线上生成的文本量也随之增多，研究如何信息在从一个LLM传递到另一个LLM的过程中发生变化变得至关重要。尽管对单个LLM的行为已有深入研究，但对迭代交互中集体行为和信息扭曲的探讨相对不足。微小的偏差，在单次输出时可能显得不明显，但在多次交互中可能会被放大，可能导致内容朝着吸引子状态演变。我们通过借鉴人类文化进化学的研究方法——电话游戏实验，设计了一种链式传输模型。在这个过程中，LLM代理接收、生成并传递文本，从一个链中的前一个代理到下一个。我们追踪了文本的毒性、积极度、难度和长度在传输链中的演变，揭示了偏见和吸引子的存在，并研究了它们与初始文本、指令、语言模型和模型规模的关系。例如，我们发现开放性指令比约束性任务更容易引发更强的吸引效应。此外，不同的文本特性对吸引子效应的敏感度不同，毒性的影响通常大于长度。这些发现强调了考虑多步骤传输动态的重要性，为进一步理解LLM的文化动态奠定了基础。**|
|**2024-07-05**|**AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents**|Petr Anokhin et.al.|[2407.04363](http://arxiv.org/abs/2407.04363)|**[link](https://github.com/airi-institute/arigraph)**|**随着生成式人工智能的进步，大型语言模型（LLMs）在自主代理的发展中展现出广阔的应用前景。实现真正的自主性需要从与环境的交互中积累和更新知识，并能有效利用这些信息。当前基于LLMs的方法依赖于全历史观察、总结或检索增强，但这些非结构化的记忆表示不利于复杂决策中的推理和规划。我们的研究提出AriGraph，一种新型方法，让代理在探索环境中构建融合语义和情节记忆的记忆图。这种图结构促进关联概念的有效检索，这些概念与代理当前状态和目标相关，从而成为一种有效的环境模型，提升探索和规划能力。  我们设计的Ariadne LLM代理，配备有我们提出的记忆架构以及规划和决策功能，能在零样本基础上处理TextWorld环境中的复杂任务，如First TextWorld Problems竞赛中的烹饪挑战，以及新任务如房屋清洁和寻宝谜题。与全历史、总结和检索增强生成等传统方法相比，我们的方法在各种任务中表现出显著优势。**|
|**2024-07-02**|**MMedAgent: Learning to Use Medical Tools with Multi-modal Agent**|Binxu Li et.al.|[2407.02483](http://arxiv.org/abs/2407.02483)|null|尽管多模态大型语言模型（MLLMs）已经取得了成功，但它们的泛化能力仍然有限，在某些情况下表现不如专门化的模型。为了解决这些问题，最近的研究开发了基于LLMs的代理，可以根据用户输入选择合适的专用模型。然而，这种进展在医疗领域尚未得到充分探索。为了弥补这一空白，本文首次提出了一种专门为医疗领域设计的代理，称为\textbf{M}ulti-modal \textbf{Med}ical \textbf{Agent}（MMedAgent）。我们构建了一个指令调优数据集，包含了六个医疗工具来解决七项任务，使代理能够为给定任务选择最合适的工具。实验全面展示了MMedAgent在各种医疗任务上超越了开源方法的最新状态，甚至与闭源模型GPT-4o相比也表现出色。此外，MMedAgent还显示出了更新和整合新医疗工具的高效性。|
|**2024-07-02**|**Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents**|Fanzeng Xia et.al.|[2407.01887](http://arxiv.org/abs/2407.01887)|null|本文关注的是大型语言模型在决策制定中的性能，尤其是在杜尔克姆双臂赌博（Dueling Bandits，DB）问题的上下文中。研究比较了GPT-3.5-Turbo、GPT-4和GPT-4-Turbo与现有DB算法的性能。结果显示，尤其是GPT-4 Turbo，能够快速识别出优势明显的选项，从而在弱后悔方面超越当前最佳算法。然而，这些模型在收敛性上存在问题，对提示的敏感度较高，且对提示变化反应脆弱。为了改进，我们提出了一种结合了LLM决策能力与经典DB算法理论保证的增强型算法——IF-Enhanced LLM。这种设计展示了如何增强LLM在对性能稳定性有要求的决策任务中的可信度。IF-Enhanced LLM具有弱后悔和强后悔的理论保证。实验结果验证了即使面对嘈杂和对抗性的提示，IF-Enhanced LLM仍保持稳健。|
|**2024-07-01**|**Agentless: Demystifying LLM-based Software Engineering Agents**|Chunqiu Steven Xia et.al.|[2407.01489](http://arxiv.org/abs/2407.01489)|**[link](https://github.com/OpenAutoCoder/Agentless)**|**随着大型语言模型（LLMs）的最新进展，软件开发任务的自动化，如代码合成、程序修复和测试生成，已取得显著进步。研究人员和业界实践者已经开发出各种自主LLM代理来执行端到端的软件开发任务，它们能够利用工具、运行命令、观察环境反馈并规划未来行动。然而，这些基于代理的方法的复杂性以及当前LLM的局限性，引发了一个问题：是否真的需要使用复杂的自主软件代理？为了探讨这个问题，我们构建了Agentless——一种无代理方法，用于自动解决软件开发问题。与复杂的代理设置相比，Agentless采用了一种简单的两阶段过程：定位后修复，不让LLM决定未来的行动或操作复杂的工具。在流行的SWE-bench Lite基准上，我们的实验结果令人惊讶地表明，这种简单的方法能够实现最高性能（27.33%）和最低成本（0.34美元），超越所有开源软件代理！  此外，我们手动分类了SWE-bench Lite中的问题，并发现存在精确的ground truth补丁问题或描述不足/误导性的问题。因此，我们构建了SWE-bench Lite-S，通过排除这些问题来进行更严格的评估和比较。我们的工作突显了当前被忽视的简单、可解释技术在自主软件开发中的潜力。我们希望Agentless将作为自主软件代理的基线、起点和期望值，激发未来在这个关键领域的工作。**|
|**2024-07-01**|**MIRAI: Evaluating LLM Agents for Event Forecasting**|Chenchen Ye et.al.|[2407.01231](http://arxiv.org/abs/2407.01231)|null|随着大型语言模型（LLMs）的最新进展，这些模型能够自主收集全球信息，并进行推理以解决复杂问题，这引发了使用LLM预测国际事件的兴趣。然而，目前缺乏一个严格评估LLM预测能力与可靠性的基准。为了填补这一空白，我们提出MIRAI，这是一个新颖的基准，旨在系统地评价LLM在国际事件时间序列预测中的表现。MIRAI构建了一个代理环境，配备有访问广泛历史结构化事件和文本新闻数据库的工具。我们对GDELT事件数据库进行了精心清洗和解析，设计了一系列关联预测任务，涵盖了不同预测时间范围，从短期到长期，以检验LLM在整合全球关键信息、运用领域特定API和库编写代码以及综合处理来自多种格式和时间的历史知识以准确预测未来事件的能力。通过全面的基准测试，我们的目标是建立一个可靠的框架，以评估LLM在国际事件预测方面的性能，从而推动更精确和可信的国际关系分析模型的发展。|
|**2024-07-01**|**Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents**|Shihan Deng et.al.|[2407.00993](http://arxiv.org/abs/2407.00993)|null|随着大型语言模型（LLMs）的显著进步，基于LLM的移动代理已成为人机交互领域的研究热点。然而，针对此类代理的基准测试资源相对匮乏。评估这类代理通常面临三个挑战：（1）仅依赖用户界面（UI）操作的低效限制了任务评估；（2）单一应用中的特定指令不足以全面评估LLM移动代理的多维度推理和决策能力；（3）当前的评估指标无法准确衡量连续动作过程。为此，我们提出了Mobile-Bench，一个全新的用于评估LLM移动代理能力的基准。首先，我们扩展了传统的UI操作，融入了103个收集到的API，以提高任务完成的效率。接着，我们通过结合真实用户查询和LLM增强的数据收集来进行评估。为了更好地评价移动代理的不同规划能力层次，我们的数据被分为SAST（简单任务）、SAMT（稍复杂任务）和MAMT（多任务）三类，反映了任务复杂度的差异。Mobile-Bench包含832条数据条目，其中超过200项任务专门设计用于测试跨应用协作场景。此外，我们引入了一种更精确的评估指标，称为CheckPoint，用于检查LLM移动代理在规划和推理步骤中是否达到关键点。|
|**2024-06-29**|**Large Language Models for Power Scheduling: A User-Centric Approach**|Thomas Mongaillard et.al.|[2407.00476](http://arxiv.org/abs/2407.00476)|**[link](https://github.com/thomasmong/llm-power-scheduling)**|**随着传统优化和调度方法逐渐转向用户驱动和个人化服务，以提升用户体验（QoE）和灵活性，未来的系统，尤其是在无线和数字化能源网络中，面临着如何更好地理解和响应用户需求的挑战。传统的系统往往忽视了用户的个性化需求，因为用户与机器之间的沟通不畅。大型语言模型（LLMs）的出现为解决这个问题带来了突破，它们提供了用户与设备之间自然的交流界面。本文首次提出了一种新颖的架构，通过构建三个LLM代理来将用户的语音请求（VRQ）转化为资源分配向量。具体包括：LLM意图识别代理将请求转化为优化问题（OP）、LLM OP参数识别代理以及LLM OP求解代理。  我们针对电动汽车（EV）充电的典型VRQ创建了一个数据库，作为性能评估的基础。作为概念验证，我们主要使用Llama 3 8B模型进行实验。通过不同的提示工程场景测试，结果显示了所提架构的有效性。研究还揭示了一些关键见解，例如，用于建模实际问题的更大候选OP集可能会由于更高的识别/OP分类噪声而降低最终性能。所有结果和代码已开源，供学术界进一步研究和利用。**|
|**2024-06-29**|**Financial Knowledge Large Language Model**|Cehao Yang et.al.|[2407.00365](http://arxiv.org/abs/2407.00365)|null|人工智能在金融领域取得了显著进步，正在重塑数据处理和解读方式。其中，大型语言模型（LLMs）展现出巨大的潜力，能够自动化复杂任务、提升客户服务，并提供详尽的财务分析。首先，我们介绍IDEA-FinBench，这是一个专为评估大型语言模型在金融知识方面的性能而设计的评价基准。它借鉴了两个全球知名且权威的金融专业考试中的问题，旨在全面检验LLMs解答与金融相关考题的能力。其次，我们提出IDEA-FinKER，是一个金融知识增强框架，旨在快速让通用LLMs适应金融领域。它采用基于检索的少量样本学习方法，实现实时上下文级知识注入，并提供一套高质量的金融知识指令，用于微调任何通用模型。最后，我们展示了IDEA-FinQA，一个由LLMs驱动的金融问答系统。该系统围绕实时知识注入和事实强化的架构构建，利用外部知识。IDEA-FinQA主要由数据收集器、数据查询模块和执行特定功能的LLM代理组成。|
|**2024-06-28**|**Simulating Financial Market via Large Language Model based Agents**|Shen Gao et.al.|[2406.19966](http://arxiv.org/abs/2406.19966)|null|大多数经济理论通常假设金融市场参与者是完全理性的个体，并使用数学模型来模拟人类在金融市场的行为。然而，人类行为往往并非完全理性，用数学模型精确预测颇具挑战。本文提出了一种新型的\textbf{A}gent-based \textbf{S}imulated \textbf{F}inancial \textbf{M}arket（ASFM），首先构建了一个具有真实订单匹配系统的模拟股票市场。接着，我们设计了一种基于大型语言模型的股票交易代理，它包括个人概况、观察和基于工具学习的动作模块。这种交易代理能够全面理解当前市场动态和金融政策信息，从而根据其交易策略作出决策。实验表明，ASFM在可控场景下的反应与现实股票市场一致。此外，我们在两个经济学研究热点领域进行了实验，结果发现，我们的\model得出的结论与经济学研究的初步发现相吻合。因此，我们认为ASFM为经济研究提供了一个新的范式。|
|**2024-06-26**|**Simulating The U.S. Senate: An LLM-Driven Agent Approach to Modeling Legislative Behavior and Bipartisanship**|Zachary R. Baker et.al.|[2406.18702](http://arxiv.org/abs/2406.18702)|null|这项研究提出了一种创新的方法，利用语言模型驱动的虚拟代理来模拟立法过程，具体聚焦于美国参议院情报委员会。我们构建了代表个别参议员的代理，并在模拟的委员会讨论中让它们互动。这些代理展现出在现实辩论中的能力，能够提供深思熟虑的观点，并在特定条件下找到两党的解决方案。值得注意的是，模拟显示，面对外部干扰时，代理模型在两党合作上展现出转变的潜力。研究结果表明，这种基于语言模型的策略可能成为理解和改进立法流程的有效工具，这与一系列发现相呼应，即基于语言模型的代理能有用地模拟现实世界现象。未来的研究将致力于提升代理的复杂性，扩大模拟范围，并探索在政策测试和谈判中的应用。|
|**2024-06-25**|**Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks**|Yun-Shiuan Chuang et.al.|[2406.17232](http://arxiv.org/abs/2406.17232)|null|### 翻译  构建逼真的人工大型语言模型（LLMs）对于实现可信的社会模拟至关重要。尽管基于人口统计信息的角色扮演有时能提升人性化，但效果并不总是理想。本研究旨在探究是否可以通过整合来自实证人类信念网络的信息，进一步提升LLMs与人类行为的契合度。我们利用一项人类调查数据，估计了一个包含18个主题的信念网络，这些主题加载于两个不重叠的潜在因子上。然后，我们在LLM中植入一个关于某一主题的观点，分析其对剩余测试话题表达的观点与相应人类数据的契合程度。仅依赖人口统计信息的角色扮演未能使LLM和人类观点保持一致，但当植入单一信念时，对于相关于信念网络内的主题，这种一致性显著提高，而对于网络外的主题则没有明显影响。这些结果表明了一种新颖的方法，可以用于在追求理解和模拟社会中信念分布模式的人工智能工作中，实现人类与LLMs之间的信念对齐。|
|**2024-06-21**|**GenoTEX: A Benchmark for Evaluating LLM-Based Exploration of Gene Expression Data in Alignment with Bioinformaticians**|Haoyang Liu et.al.|[2406.15341](http://arxiv.org/abs/2406.15341)|**[link](https://github.com/liu-hy/genotex)**|**## 翻译  近年来，机器学习的进步显著提升了从基因表达数据中识别疾病相关基因的能力。然而，这些过程往往需要深厚的专长和大量的人工努力，限制了其可扩展性。大型语言模型（LLMs）驱动的代理显示出在自动化此类任务方面的潜力，因为它们的问题解决能力日益增强。为了支持这类方法的评估和发展，我们创建了GenoTEX，这是一个基因表达数据分析自动探索的基准，包括数据集选择、预处理和统计分析任务。GenoTEX提供了全面的分析管道，其中包含了人类生物信息学家精心编写的注释，他们对数据集进行深入分析以确保准确性和可靠性。  为了提供这些任务的基线，我们设计了GenoAgents，这是一个基于LLMs的代理团队，具备上下文感知规划、迭代校正以及与领域专家咨询的能力，它们协作探索基因数据集。我们的实验显示了LLM驱动方法在基因组数据分析中的潜力，而错误分析指出了挑战和未来的改进方向。我们提议GenoTEX作为一个有前景的资源，用于衡量和提升人工智能驱动的基因组数据分析方法。我们的基准已公开发布在：\url{https://github.com/Liu-Hy/GenoTex}。**|
|**2024-06-21**|**Autonomous Agents for Collaborative Task under Information Asymmetry**|Wei Liu et.al.|[2406.14928](http://arxiv.org/abs/2406.14928)|**[link](https://github.com/thinkwee/iAgents)**|**大型语言模型多-agent系统（LLM-MAS）在解决复杂任务方面取得了显著进步。它们通过系统内各代理之间的通信协作来完成任务，前提是共享信息。然而，当代理间的交流被用于增强人类合作时，由于信息不对称（每个代理仅能访问其对应人类用户的信息），这带来了新的挑战。传统MAS在这种情况下难以完成任务。为解决此问题，我们提出了一种新型多agent系统架构，称为“iAgents”，即信息丰富多agent系统。在iAgents中，人类社会网络在代理网络中得到反映，代理主动交换完成任务所需的人类信息，从而克服信息不对称。iAgents采用了一种新颖的代理推理机制，InfoNav，引导代理之间的有效信息交流。结合InfoNav，iAgents组织了混合记忆中的人类信息，为代理提供准确全面的信息进行交换。此外，我们还推出了首个针对评估LLM在信息不对称条件下任务解决能力的基准——InformativeBench。实验结果显示，iAgents能够在包含140人和588条关系的社会网络中协作，自主进行超过30轮的通信，并从近70,000条消息中检索信息，在3分钟内完成任务。**|
|**2024-06-21**|**FlowBench: Revisiting and Benchmarking Workflow-Guided Planning for LLM-based Agents**|Ruixuan Xiao et.al.|[2406.14884](http://arxiv.org/abs/2406.14884)|null|基于语言模型的代理作为一种有前景的工具，被设计用于通过迭代规划和行动来执行复杂任务。然而，这些代理在处理需要专业知识的任务时，容易产生不期望的规划幻觉。为了解决这个问题，初步尝试通过融入与工作流程相关的外部知识来增强规划可靠性。尽管显示出潜力，但注入的知识通常杂乱无章，格式多样，缺乏严谨的规范化和全面的比较。为此，我们规范了不同格式的工作流程知识，并提出了FlowBench，这是第一个面向工作流引导规划的基准。FlowBench涵盖了来自6个领域的51个不同场景，其中知识以多样的形式呈现。为了评估不同语言模型在FlowBench上的性能，我们设计了一个多层次的评估框架。我们研究了工作流程知识在多种格式下的有效性，结果表明当前的语言模型代理在满足满意的规划需求方面仍有很大的提升空间。我们期望这个具有挑战性的基准能为未来的代理规划研究铺平道路。|
|**2024-07-01**|**Artificial Leviathan: Exploring Social Evolution of LLM Agents Through the Lens of Hobbesian Social Contract Theory**|Gordon Dai et.al.|[2406.14373](http://arxiv.org/abs/2406.14373)|null|随着大型语言模型（LLMs）和人工智能的进步，计算社会科学的研究迎来了大规模探索的机遇。我们的工作基于先前对LLM行为体设计的研究，构建了一个模拟的Agent社会，其中复杂的社交关系随时间动态形成和发展。我们赋予这些Agent心理驱动力，并置于一个沙盒生存环境中。通过托马斯·霍布斯的奠基性社会契约理论（SCT）的视角，我们评估了这个Agent社会。实验结果显示，起初，Agent们表现出无拘无束的冲突，符合霍布斯对“自然状态”的描述。然而，随着模拟的进行，社会契约逐渐形成，绝对主权者得到了授权，进而建立了以相互合作为基础的和平共同体。我们的实验发现与霍布斯理论相吻合：LLM驱动的多Agent模拟展示了社会动态的复杂性，可能复制塑造人类社会的力量。尽管无法完全模拟人类行为的所有细微之处，但这种模拟对于理解社会结构、群体动态和复杂人类系统具有潜在价值。|
|**2024-06-20**|**EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms**|Siyu Yuan et.al.|[2406.14228](http://arxiv.org/abs/2406.14228)|**[link](https://github.com/siyuyuan/evoagent)**|**随着强大大型语言模型（LLMs）的兴起，一种新的趋势是利用这些模型构建能解决复杂任务的自主代理，尤其是多代理系统。然而，现有的研究很大程度上依赖于人类设计的框架，这限制了代理系统的功能范围和可扩展性。如何自动将专门的代理扩展到多代理系统，以提升任务解决能力，仍然是一个重大挑战。本文提出EvoAgent，这是一种通过进化算法自动将专家代理扩展到多代理系统的方法，旨在提高基于LLM的代理在执行任务中的效率。具体来说，我们视现有的代理框架为初始个体，并应用一系列进化操作（如突变、交叉、选择等）生成具有不同设置的代理。EvoAgent适用于任何基于LLM的代理框架，能够无须额外人工设计自动生成扩展的多代理系统。实验结果显示，EvoAgent能够自动产生多个专家级代理，并显著增强基于LLM的代理的任务解决能力。**|
|**2024-06-19**|**AgentDojo: A Dynamic Environment to Evaluate Attacks and Defenses for LLM Agents**|Edoardo Debenedetti et.al.|[2406.13352](http://arxiv.org/abs/2406.13352)|**[link](https://github.com/ethz-spylab/agentdojo)**|**本文介绍了一个名为AgentDojo的框架，用于评估依赖于外部工具处理不可信数据的AI代理的对抗性鲁棒性。面对不断演变的攻击和防御手段，AgentDojo不是一个静态的测试套件，而是设计和评估新任务、防御策略以及适应性攻击的可扩展环境。它包含了97个实际应用场景的任务（如管理电子邮件客户端、导航网上银行网站或预订旅行），629个安全测试案例，以及来自文献的各种攻击和防御方法。研究发现，当前最先进的语言模型在AgentDojo中的表现并不尽人意（即使没有攻击），并且现有的提示注入攻击虽然能破坏一些安全特性，但并非所有情况都适用。我们期望AgentDojo能够推动研究，以寻找在解决常见任务时既可靠又健壮的AI代理的新设计原则。相关代码已发布在https://github.com/ethz-spylab/agentdojo。**|
|**2024-06-19**|**LLMatDesign: Autonomous Materials Discovery with Large Language Models**|Shuyi Jia et.al.|[2406.13163](http://arxiv.org/abs/2406.13163)|null|发现新材料对科学和技术具有重大意义，但目前仍是艰巨问题，因为化学空间浩瀚。近期，机器学习的进步推动了基于数据的方法来快速筛选或生成有前景的材料，但这些方法仍依赖大量训练数据，且往往缺乏人类期望的材料设计的灵活性和化学直觉。我们提出LLMatDesign，一个由大型语言模型驱动的可解释材料设计新框架。LLMatDesign利用LLM代理理解人类指令，对材料进行修改，并使用提供的工具评估结果。通过自我反思先前决策，LLMatDesign能在零样本情况下快速适应新任务和条件。在离线实验中，对LLMatDesign在多个材料设计任务中的系统评估证实了它在小数据环境下开发出具有用户定义目标性质的新材料的有效性。我们的框架展示了自主LLM引导的计算环境下的材料发现的非凡潜力，预示着未来自驾驶实验室的可能性。|
|**2024-06-18**|**Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents**|Zehao Wang et.al.|[2406.12806](http://arxiv.org/abs/2406.12806)|null|**背景**：配置设置对于调整软件行为以满足特定性能需求至关重要，但错误配置普遍存在。由于配置项众多且复杂，识别影响系统性能的配置是一项挑战。本研究提出PerfSense，这是一个轻量级框架，利用大型语言模型（LLMs）高效地识别性能关键配置，同时保持低开销。PerfSense利用LLM代理模拟开发者和性能工程师之间的交互，采用先进的提示链技术和检索增强生成（RAG）等技术。  **方法与成果**：我们在七个开源Java系统上的评估显示，PerfSense在分类性能敏感配置方面的平均准确率为64.77%，优于基于LLM的基线（50.36%）和先前的最佳方法（61.75%）。特别是，我们的提示链技术提高了召回率10%至30%，而保持了相似的精确度。进一步的手动分析362个误分类案例，发现常见问题包括LLMs对需求的理解偏差（占26.8%）。  **结论**：PerfSense显著减少了手动分类性能关键配置的工作量，并为未来的LLM基于代码分析研究提供了有价值的观点。|
|**2024-06-18**|**AgentReview: Exploring Peer Review Dynamics with LLM Agents**|Yiqiao Jin et.al.|[2406.12708](http://arxiv.org/abs/2406.12708)|null|## 翻译  同行评审是科学出版诚信和进步的基础。传统的同行评审数据分析方法往往侧重于现有数据的探索和统计，但未能充分考虑这一过程的多变量性质，处理潜在变量，且受限于隐私问题，因为数据涉及敏感性。我们提出AgentReview，这是一个基于大型语言模型（LLM）的同行评审模拟框架，有效分解了多个潜在因素的影响，并解决了隐私问题。研究发现，由于社会影响力理论、利他主义疲劳和权威偏见等社会学理论的支持，论文决策中存在显著的37.1%的变异性。我们相信这项研究能为优化同行评审机制设计提供宝贵见解。|
|**2024-06-18**|**Large Language Models based Multi-Agent Framework for Objective Oriented Control Design in Power Electronics**|Chenggang Cui et.al.|[2406.12628](http://arxiv.org/abs/2406.12628)|null|这篇论文关注于电力电子系统控制设计中的挑战，特别是模型不确定性以及设计周期漫长和成本高昂的问题。论文旨在提出一种基于大型语言模型（LLMs）的多代理框架，用于面向目标的电力电子控制器设计。该框架利用LLMs的推理能力，结合多代理工作流程，旨在开发一个高效且自动化的控制器设计流程。LLM代理能够理解并响应自然语言的高级指令，根据任务的具体需求和实际应用中的约束调整其行为。这种新颖而高效的策略有望显著提升电力电子控制器设计的灵活性和适应性，极大地便利实践者的工作。|
|**2024-06-18**|**CodeNav: Beyond tool-use to using real-world codebases with LLM agents**|Tanmay Gupta et.al.|[2406.12276](http://arxiv.org/abs/2406.12276)|null|我们介绍CodeNav，这是一种利用大型语言模型（LLM）来导航和利用先前未见过的代码仓库，以解决用户查询的系统。与需要通过手动描述在LLM上下文中“注册”所有相关工具的工具使用型LLM不同，CodeNav能够自动索引和搜索目标代码库中的代码块，找到相关的代码片段，导入它们，并根据执行反馈迭代生成解决方案。首先，我们通过三个案例研究展示CodeNav如何使用三种不同的代码库来解决复杂的用户问题。接着，在三个基准测试中，我们定量比较了仅能访问目标代码库的代码使用方法与拥有对所有工具名称和描述的特权访问的工具使用方法的效果。此外，我们研究了不同类型工具和库描述对代码使用性能的影响，以及将源代码视为输入而非自然语言代码描述的优势。所有代码将遵循宽松许可协议开源。|
|**2024-06-17**|**Efficient Sequential Decision Making with Large Language Models**|Dingyang Chen et.al.|[2406.12125](http://arxiv.org/abs/2406.12125)|null|该论文关注的是将大型语言模型（LLMs）的成功扩展到序列决策制定。当前的努力要么重新训练或微调LLMs进行决策，要么为预训练的LLMs设计提示。前者面临计算负担重的梯度更新问题，而后者未显示出明显效果。为此，我们提出了一种新方法，利用在线模型选择算法有效地将LLMs整合到序列决策过程中。统计上，我们的方法显著优于传统决策算法和纯LLM代理。在计算上，我们的方法避免了对LLMs进行昂贵的梯度更新，并且在整个决策过程中仅需要少量的LLM调用。我们进行了广泛实验来验证我们方法的有效性。以一个大规模的亚马逊数据集为例，我们的方法在仅使用1.5%的时间步数调用LLMs的情况下，实现了比基线超过6倍的性能提升。|
|**2024-06-17**|**Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector**|Xiaoxue Cheng et.al.|[2406.11277](http://arxiv.org/abs/2406.11277)|**[link](https://github.com/rucaibox/haluagent)**|这篇论文探讨了大型语言模型（LLMs）在幻觉检测方面的挑战，特别指出以往研究主要依赖于强大的闭源模型如GPT-4。作者提出了一种自主的基于LLM的代理框架，称为HaluAgent，它允许较小的模型（如巴 chcuan2-Chat 7B）主动选择适合检测文本、代码和数学表达式等多种幻觉类型的工具。HaluAgent整合了LLM、多功能工具箱，并设计了一个细粒度的三阶段检测框架，同时配备了记忆机制。为了提高HaluAgent的效能，论文利用现有的中文和英文数据集合成检测轨迹进行微调，使其具备双语幻觉检测能力。实验结果表明，仅使用2000个样本对LLM进行调优后，HaluAgent在各种任务和数据集上表现出色，其性能可与GPT-4媲美，甚至在某些情况下超越，且无需额外工具增强，无论在领域内还是领域外的数据集上都展现出良好性能。论文的代码和数据集已发布在https://github.com/RUCAIBox/HaluAgent。|
|**2024-06-18**|**AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval**|Shirley Wu et.al.|[2406.11200](http://arxiv.org/abs/2406.11200)|**[link](https://github.com/zou-group/avatar)**|**大型语言模型（LLMs）在利用外部工具和知识提升准确性和减少错误方面展现出显著能力。然而，设计能让LLMs有效运用这些工具的提示技巧是一项耗时且依赖直觉的任务。为此，我们提出AvaTaR，一个创新的自动化框架，它能优化LLMs，使其更有效地利用提供的工具，并在特定任务或领域中提升性能。AvaTaR通过设计一个比较器模块，以训练数据中的正负样本进行推理，迭代地为LLM提供富有洞察力和全面的提示。我们在四个包含文本、视觉和关系信息的复杂多模态检索数据集上展示了AvaTaR的效果。实验表明，AvaTaR在所有四项具有挑战性的任务中均优于现有最先进的方法，并展现出强大的泛化能力，当应用于新案例时，平均在Hit@1指标上实现了14%的相对改进。代码和数据集已在<https://github.com/zou-group/avatar>上公开。**|
|**2024-06-17**|**Watch Every Step! LLM Agent Learning via Iterative Step-Level Process Refinement**|Weimin Xiong et.al.|[2406.11176](http://arxiv.org/abs/2406.11176)|**[link](https://github.com/weiminxiong/ipr)**|**大型语言模型在一系列复杂的交互任务中展现出卓越性能。近期的研究倾向于通过专家轨迹调优来提升模型效果，但主要关注最终结果奖励，这可能导致错误或非最优行为，因为缺乏过程监督信号。为此，我们在本文中提出迭代步级过程改进（Iterative Step-level Process Refinement，IPR）框架，该框架提供了细致的逐步骤指导，以增强训练过程。我们采用蒙特卡洛方法估算每一步的奖励。在每个迭代中，模型沿着专家轨迹探索并生成新动作，然后与专家轨迹的相应步骤进行比较，使用步级奖励评估。这种比较有助于识别差异，形成用于训练的对比动作对。我们在三个复杂代理任务上的实验表明，我们的框架优于多种强大的基线。此外，我们的分析结果揭示了IPR在提升动作效率方面的有效性，并证明其适用于各种模型。**|
|**2024-06-17**|**RePrompt: Planning by Automatic Prompt Engineering for Large Language Models Agents**|Weizhe Chen et.al.|[2406.11132](http://arxiv.org/abs/2406.11132)|null|在过去的一年里，大型语言模型（LLMs）在传统自然语言处理领域之外展现出惊人成就，人们开始探索在代码生成、旅行规划和机器人控制等更具体的应用领域使用这些模型。通过与LLM构建所谓的LLM代理，旨在协助人们完成日常生活中的各种任务。然而，对LLMs的提示语句对生成内容及其性能至关重要。因此，自动提示工程成为许多研究人员和LLM用户关注的焦点。本文提出了一种新颖的方法，名为\textsc{RePrompt}，它利用与LLM代理交互获取的对话历史，通过“梯度下降”优化LLM的逐步指令。通过优化提示，LLM能够学习特定领域的规划策略。我们在PDDL生成和旅行规划任务中进行了实验，结果显示，使用更新后的提示作为初始提示时，我们的方法通常可以提高不同推理任务的性能。|
|**2024-06-18**|**Embodied Question Answering via Multi-LLM Systems**|Bhrij Patel et.al.|[2406.10918](http://arxiv.org/abs/2406.10918)|null|## 背景  Embodied Question Answering（EQA）是一个关键问题，它涉及一个代理在环境中探索以回答用户查询。当前的研究主要集中在单代理场景中，这可能导致探索时间冗长且成本高昂。在这个工作中，我们考虑了多代理框架下的EQA，其中涉及多个基于大型语言模型（LLM）的独立代理，它们各自解答关于家庭环境的问题。为了为每个查询生成一个答案，我们利用各个独立响应来训练一个中央答案模型（CAM），该模型整合答案以实现更稳健的回答。通过使用CAM，我们观察到其在EQA准确率上比诸如投票机制和辩论等ensemble LLM聚合方法高出50%。CAM无需任何形式的代理间通信，从而避免了相关开销。我们还通过不同的非线性（如神经网络、随机森林、决策树、XGBoost）和线性算法（如逻辑回归分类器、支持向量机）对CAM进行了消融研究。最后，我们通过Permutation Feature Importance（PFI）分析了CAM对每个独立代理和查询上下文的依赖程度，量化了CAM的依赖特性。|
|**2024-06-16**|**GUI-WORLD: A Dataset for GUI-oriented Multimodal LLM-based Agents**|Dongping Chen et.al.|[2406.10819](http://arxiv.org/abs/2406.10819)|**[link](https://github.com/keplerlab/katna)**|**近年来，多模态大型语言模型（MLLM）已被用于控制键盘和鼠标输入，直接感知图形用户界面（GUI），并生成相应的代码。然而，当前的模型主要在静态环境中表现出色，主要应用于相对简单的领域，如网页或移动界面。我们认为，一个稳健的GUI代理应具备理解GUI的时空信息能力，包括动态网页内容和多步骤任务，还要全面理解各种GUI场景，包括桌面软件和多窗口交互。为此，本文提出了一项新数据集——GUI-World，其中包含了精心制作的人机标注，广泛涵盖六种GUI场景和八类GUI相关问题，以三种格式呈现。我们评估了当前最先进的MLLM，如图像LLMs和视频LLMs，在理解和处理不同类型GUI内容，特别是动态和序列内容方面的能力。研究发现，图像LLMs在没有手动标注关键帧或操作历史的情况下，难以应对动态GUI内容。另一方面，由于GUI视频数据集的稀疏性，视频LLMs在所有GUI相关任务上表现不佳。基于GUI-World，我们首次尝试使用微调后的视频LLM作为GUI代理，显示了对各种GUI任务理解的提升。然而，由于基础LLM性能的限制，我们得出结论，将视频LLMs用作GUI代理仍是一个重大挑战。我们相信，我们的工作为未来在动态GUI内容理解方面的研究提供了有价值的洞见。代码和数据集已在我们的项目主页https://gui-world.github.io/上公开。**|
|**2024-06-16**|**HiddenTables & PyQTax: A Cooperative Game and Dataset For TableQA to Ensure Scale and Data Privacy Across a Myriad of Taxonomies**|William Watson et.al.|[2406.10803](http://arxiv.org/abs/2406.10803)|null|## 背景  大型语言模型（LLMs）在处理表格问答任务时面临诸多挑战，主要包括：（1）对于大表格有限的上下文窗口；（2）不同token化模式与单元格边界的复杂差异；（3）以及使用外部模型如gpt-3.5-turbo时的数据保密问题。为解决这些问题，我们提出了一种名为“HiddenTables”的合作游戏。这个游戏涉及代码生成LLM“Solver”和评估其在表格问答任务能力的“Oracle”，以自然语言规范为基础，同时保证数据安全。  我们通过实证实验在多样化的表格上展示了LLMs在处理复杂查询、处理组合依赖以及将自然语言转化为程序指令方面的局限性，特别是在提供具体表格结构的情况下。与基于编码器的模型不同，“HiddenTables”不受行数限制，从而提高了提示和完成 token 的效率。此外，我们创建了一个新的数据集“PyQTax”，包含116,671个问题-表格-答案三元组，并提供了更细致的问题分类和标签，进一步增强了我们的研究。  因此，除了学术贡献，揭示了LLMs在表格问答任务中的不足，“HiddenTables”还展示了如何在保障数据安全的同时，让LLMs与大规模数据集互动，以及降低生成成本的实践方法。|
|**2024-06-15**|**From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent**|Samuel S. Sohn et.al.|[2406.10478](http://arxiv.org/abs/2406.10478)|null|## 背景 在娱乐、教育和营销领域至关重要的数字故事叙述面临着生产规模扩展和灵活性提升的挑战。这篇论文介绍的StoryAgent框架利用大型语言模型和生成工具来自动化并优化数字故事创作过程。它采用自上而下的故事情节草拟和自下而上的资产生成方法，解决了手动干预、互动场景编排和叙事一致性等关键问题。这个框架促进了交互式和一致叙事的高效生产，适用于多种媒介，推动了内容创作的民主化，增强了用户的参与度。我们的实验结果显示，该框架能够在没有参考视频的情况下生成连贯的数字故事，这标志着自动数字故事叙述技术的一个重大进步。|
|**2024-06-13**|**GuardAgent: Safeguard LLM Agents by a Guard Agent via Knowledge-Enabled Reasoning**|Zhen Xiang et.al.|[2406.09187](http://arxiv.org/abs/2406.09187)|null|随着大型语言模型（LLMs）的快速发展，LLM驱动的代理被广泛应用于各种应用，这引发了对其安全性和可信度的新担忧。现有的提升LLM安全性的方法并不直接适用于LLM驱动的代理，因为它们具有不同的目标和输出模式。本文提出了一种创新方法——GuardAgent，它作为其他LLM代理的“防护栏”。GuardAgent通过检查其输入/输出是否满足用户定义的一系列守护请求来监督目标LLM。GuardAgent分为两步：1）分析提供的守护请求创建任务计划；2）根据任务计划生成守护代码，并通过API调用或外部引擎执行。整个过程利用LLM作为核心推理组件，结合记忆模块中的上下文示例，增强了知识驱动的推理能力，使其能够理解各种文本守护请求并准确地将其转化为可执行代码，提供可靠的安全保障。  GuardAgent还配备了一个可扩展的工具箱，包含函数和API，无需额外训练LLM，强调了其通用性及低运营成本。此外，我们提出了两个新颖的基准：EICU-AC用于评估医疗健康代理的隐私相关访问控制，Mind2Web-SC用于评估网络代理的安全性。在这些基准上，GuardAgent分别在98.7%和90.0%的精度下有效管理了两种类型代理的无效输入和输出。实验还表明，GuardAgent能够适应新兴的LLM代理和守护请求，定义新的功能，进一步证明了其强大的泛化能力。|
|**2024-06-13**|**Multi-Agent Software Development through Cross-Team Collaboration**|Zhuoyun Du et.al.|[2406.08979](http://arxiv.org/abs/2406.08979)|**[link](https://github.com/openbmb/chatdev)**|**### 概述  最新的大型语言模型（LLMs）进展，如ChatDev，推动了软件开发领域的深刻变革，特别体现在多代理协作上。这些模型能够像人类团队一样合作，遵循瀑布模型进行需求分析、开发、审查、测试等阶段，实现自主软件生成。然而，单个开发流程中的每个阶段只会产生一种可能结果，导致只完成一条开发链，从而丧失在解决方案空间中探索多种决策路径的机会，可能导致结果不理想。为解决这一问题，我们提出了跨团队协作（Cross-Team Collaboration，CTC）框架，这是一种可扩展的多团队结构，它允许协同工作的团队在跨团队协作环境中共同提出决策，并交流各自见解，以优化内容生成。  实验结果显示，在软件开发领域的应用中，我们的方法显著优于现有基准，证实了框架的有效性。在故事生成方面的显著改进表明，该框架具有广泛的跨领域泛化能力。我们期待我们的工作能引导LLMs向跨团队模式发展，并在软件开发等领域带来重大进步。相关的代码和数据将在<https://github.com/OpenBMB/ChatDev>上提供。**|
|**2024-06-13**|**StreamBench: Towards Benchmarking Continuous Improvement of Language Agents**|Cheng-Kuang Wu et.al.|[2406.08747](http://arxiv.org/abs/2406.08747)|**[link](https://github.com/stream-bench/stream-bench)**|近期的研究表明，大型语言模型（LLMs）能够从经验中自我提升，这是部署后持续改进的重要能力。然而，现有的基准主要评估它们的固有能力，而不考察它们随时间改进的能力。为了填补这一空白，我们引入了StreamBench，这是一个开创性的基准，旨在评估LLMs在输入-反馈序列上的连续改进性能。StreamBench模拟了一个在线学习环境，其中LLMs接收到连续的反馈流，并迭代地提升其表现。此外，我们提出了一些简单但有效的LLM基线，并对影响成功流式策略的关键组件进行了全面分析。我们的工作为开发LLMs的有效在线学习策略奠定了基础，为流式场景中的更适应性AI系统铺平了道路。|
|**2024-06-12**|**MobileAgentBench: An Efficient and User-Friendly Benchmark for Mobile LLM Agents**|Luyuan Wang et.al.|[2406.08184](http://arxiv.org/abs/2406.08184)|null|随着大型语言模型（LLMs）在手机图形用户界面（GUI）上的直接交互能力日益增强，以及它们在自主管理日常任务方面的潜力，基于LLMs的移动代理正逐渐受到学术界和工业界的关注。然而，由于应用程序的无限状态和可行动作序列的模糊定义，对现有移动代理性能的基准研究相对匮乏。为解决这一挑战，我们提出了一种高效且用户友好的基准工具——MobileAgentBench，旨在减轻繁琐的手动测试负担。我们首先定义了涵盖10个开源应用的100项任务，按难度分为多个级别。接着，我们对包括AppAgent和MobileAgent在内的多个现有移动代理进行了评估，以全面系统地比较它们的表现。所有相关材料均可在我们的项目网站https://MobileAgentBench.github.io上获取，这将推动学术和工业领域的进步。|
|**2024-06-12**|**Unique Security and Privacy Threats of Large Language Model: A Comprehensive Survey**|Shang Wang et.al.|[2406.07973](http://arxiv.org/abs/2406.07973)|null|随着人工智能的快速发展，大型语言模型（LLMs）在自然语言处理方面取得了显著进步。这些模型通过大量数据训练，展现出强大的语言理解和生成能力，适用于机器翻译、聊天机器人等各种应用。然而，LLMs在其生命周期中暴露出一系列隐私和安全问题，这引起了学术界和工业界的关注。这些问题与传统语言模型相比具有独特性，鉴于当前的综述缺乏针对不同场景的清晰威胁分类，我们根据五个场景：预训练、微调、RAG系统、部署和基于LLM的代理，强调了独特的风险。考虑到每种威胁的特性，本调查提供了潜在威胁和应对策略。研究LLMs所面临的攻击和防御情况，可以为更多领域提供可行的研究方向，使更多人能够受益于LLMs。|
|**2024-06-14**|**Can Large Language Models Understand Spatial Audio?**|Changli Tang et.al.|[2406.07914](http://arxiv.org/abs/2406.07914)|null|该论文探讨了如何使大型语言模型（LLMs）掌握多通道音频中的空间信息，这是当前听觉LLMs所缺乏的能力。通过利用LLMs的高级认知和推理能力，目标是提升模型对三维环境的理解，通过音频。研究涉及三项空间音频任务：声源定位（SSL）、远场语音识别（FSR）和基于位置的语音提取（LSE），在每个任务上都取得了显著进展。在SSL方面，我们的方法在Spatial LibriSpeech数据集上的均方误差（MAE）达到2.70°，明显优于先前的基准约6.60°。此外，模型能够利用空间线索提高FSR的准确性，并通过文本提示，根据指定方向聚焦于声音，即使在重叠语音环境中也能执行LSE。这些成果揭示了LLMs适应物理音频概念的潜力，为构建基于LLM的三维环境中的代理铺平了道路。|
|**2024-06-11**|**DCA-Bench: A Benchmark for Dataset Curation Agents**|Benhao Huang et.al.|[2406.07275](http://arxiv.org/abs/2406.07275)|null|随着人工智能（AI）研究和开发的推进，数据集的质量日益关键。尽管开放数据集平台众多，但数据质量问题，如缺乏文档、标注错误和伦理考量，仍普遍存在。这些问题往往难以通过规则基础脚本检测，需要用户或维护者花费大量人力进行识别和验证。利用大型语言模型（LLMs）处理数据集整理的潜力令人期待。为此，我们提出了一项名为DCA-Bench的数据集管理代理基准，旨在评估LLM在检测隐藏数据质量问题方面的性能。我们从八个公开数据集平台收集了各种实际问题作为测试床。为了建立一个自动评估LLM成功与否的管道，我们设计了一个专门的LLM评估器。实验表明，基于LLM的评估器与人工评价高度吻合，能实现可靠的自动评估。我们还在多个基线LLM上进行了实验，显示了任务的复杂性，意味着将LLMs应用于现实世界的数据集管理仍需深入探索和创新。此外，该基准也可作为衡量LLMs在问题发现能力而非仅解决问题能力的测试平台。基准套件已开放在：\url{https://github.com/TRAIS-Lab/dca-bench}。|
|**2024-06-11**|**A Synthetic Dataset for Personal Attribute Inference**|Hanna Yukhymenko et.al.|[2406.07217](http://arxiv.org/abs/2406.07217)|**[link](https://github.com/eth-sri/synthpai)**|**近年来，强大的大型语言模型（LLMs）已为全球数亿用户所接触，但它们的强大功能和广泛世界知识也带来了隐私风险。本研究关注LLMs新兴的隐私威胁——从网络文本中准确推断个人信息。鉴于基于LLM的作者分析研究缺乏合适的公开数据集，主要是由于涉及真实个人数据的伦理和隐私顾虑，我们的工作在两个方面进行了探索：（i）我们构建了一个使用合成个人资料填充的流行社交平台Reddit的模拟框架；（ii）利用此框架，我们生成了SynthPAI，一个包含超过7800条经过手动标记个人属性的多样化的合成评论数据集。我们通过一项人类研究验证了数据集，结果显示人类在区分真实和合成评论的任务上几乎不优于随机猜测。此外，我们证明了数据集支持有意义的个人属性推断研究，通过18种最先进的LLMs，我们发现使用合成评论可以得出与现实世界数据相同的结论。综上所述，我们的数据集和流程为未来研究如何理解和减轻LLMs带来的基于推断的隐私威胁提供了强大且隐私保护的基础。**|
|**2024-06-11**|**A Tool for Test Case Scenarios Generation Using Large Language Models**|Abdul Malik Sami et.al.|[2406.07021](http://arxiv.org/abs/2406.07021)|null|大型语言模型（LLMs）在软件工程（SE）中广泛应用，涵盖代码生成、软件设计和文档编写、添加代码注释、代码审查以及编写测试脚本等任务。然而，创建测试脚本或自动化测试案例需要与功能需求紧密相关的详尽测试套件文档。这种文档应能在有限的时间和范围内实现全面测试，尤其当需求和用户期望不断变化时。本文主要关注根据用户需求生成史诗级（epics）和高层次用户故事，然后基于这些故事设计测试场景。文章介绍了一种基于LLM代理和提示工程的网络软件工具，该工具能够自动化针对用户需求生成测试场景的过程。|
|**2024-06-11**|**CAAP: Context-Aware Action Planning Prompting to Solve Computer Tasks with Front-End UI Only**|Junhee Cho et.al.|[2406.06947](http://arxiv.org/abs/2406.06947)|**[link](https://github.com/caap-agent/caap-agent)**|**长期以来，软件机器人已经在机器人流程自动化（RPA）中用于执行枯燥的计算机任务。随着大型语言模型（LLMs）的先进推理能力的出现，这些代理现在能够处理更复杂甚至前所未见的任务。然而，当前文献中的基于LLM的自动化方法往往依赖于HTML源代码作为输入，限制了它们在非网络环境的应用。HTML代码中的信息常常不准确或不完整，这降低了代理在实际应用中的可靠性。我们提出了一种仅基于屏幕截图的LLM驱动的代理，它专注于识别环境，并利用上下文学习来消除对大量人类演示数据的需求。我们的策略名为“上下文感知行动规划”（Context-Aware Action Planning，CAAP）提示，鼓励代理从多个角度仔细审查上下文。通过我们的方法，在67种MiniWoB++问题上实现了94.4%的成功率，每个问题类型只需1.48次演示。我们的方法为更广泛的应用提供了可能，特别是在需要在计算机或智能手机之间进行跨应用协调的任务上，标志着自动化代理领域的重大进步。代码和模型已在https://github.com/caap-agent/caap-agent上提供。**|
|**2024-06-07**|**GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents**|Anthony Costarelli et.al.|[2406.06613](http://arxiv.org/abs/2406.06613)|**[link](https://github.com/Joshuaclymer/GameBench)**|**大型语言模型已经在许多自然语言理解任务上展现出卓越的少量样本性能。尽管已经展示过在复杂策略场景中使用大型语言模型，但缺乏一个全面的框架来评估这些模型在游戏中的各种推理能力。为了填补这一空白，我们推出了GameBench，这是一个跨领域的框架，用于评估大型语言模型（LLMs）的战略思维能力。我们专注于9个不同的游戏环境，每个游戏至少涵盖一种在策略游戏中识别出的关键推理技能，并选择那些战略解释不太可能构成模型预训练数据主要部分的游戏。我们的评估使用了基础形式的GPT-3和GPT-4，以及两个旨在增强战略推理能力的引导框架：Chain-of-Thought（CoT）提示和Reasoning Via Planning（RAP）。结果显示，所有测试模型的表现都没有达到人类水平，最差的是GPT-4的表现甚至低于随机行动。CoT和RAP都提高了分数，但仍远未达到人类水平。**|
|**2024-06-11**|**Transforming Wearable Data into Health Insights using Large Language Model Agents**|Mike A. Merrill et.al.|[2406.06464](http://arxiv.org/abs/2406.06464)|null|尽管可穿戴健康追踪器日益普及，睡眠和运动对健康的重要性不言而喻，但从这些数据中提取具有行动价值的个性化见解仍是一个挑战。这需要对大量数据进行非结构化分析。随着大型语言模型（LLM）的兴起，它们能够利用工具理解和与世界互动，为大规模个性化分析带来了希望。然而，在个人健康领域的LLM应用尚待开发。本文介绍了一种名为Personal Health Insights Agent（PHIA）的系统，它利用最新的代码生成和信息检索工具来分析和解释行为健康数据。我们构建了两个超过4000个健康洞察问题的基准问答数据集。根据650小时的人类和专家评估，PHIA能准确回答84%以上的事实性数值问题，以及超过83%的众包开放性问题。这项工作对于推动大众行为健康进步具有重要意义，可能使个人能够解读自己的可穿戴数据，开辟了一个以数据驱动洞察为指导的个性化健康方案的新时代，使得健康保健更加便捷且个性化。|
|**2024-06-09**|**Hello Again! LLM-powered Personalized Agent for Long-term Dialogue**|Hao Li et.al.|[2406.05925](http://arxiv.org/abs/2406.05925)|**[link](https://github.com/leolee99/ld-agent)**|**随着大型语言模型（LLMs）的发展，开放域对话系统取得了显著进步。然而，大多数现有系统主要关注简短的单次会话，忽视了长期陪伴和个性化聊天机器人在现实世界中的需求。为了满足这种实际需求，事件总结和人格管理至关重要，它们能够促进长期对话回复的合理性。近期，大型语言模型在人类认知和推理能力上的进展表明，基于LLM的代理有可能大幅增强自动化感知、决策和问题解决。鉴于此，我们提出了一种模型通用的框架——长期对话代理（LD-Agent），它包括三个可独立调整的模块：事件感知、人格提取和响应生成。事件记忆模块使用长短期记忆库分别关注历史和正在进行的会话，并引入了基于主题的检索机制以提高记忆检索的准确性。此外，人格模块实现了用户和代理的动态人格建模。最后，通过整合检索的记忆和提取的人格，生成器会产生适当的回应。我们在各种示例基准、模型和任务上实证了LD-Agent的有效性、通用性和跨领域能力。代码已在https://github.com/leolee99/LD-Agent上发布。**|
|**2024-06-09**|**A Survey on LLM-Based Agentic Workflows and LLM-Profiled Components**|Xinzhe Li et.al.|[2406.05804](http://arxiv.org/abs/2406.05804)|null|## 背景  近期大型语言模型（LLMs）的进展推动了复杂代理工作流的发展，它们相较于传统的单路径、链式思维（Chain-of-Thought，CoT）提示方法有所改进。这篇综述旨在概述常见的工作流，特别关注大型语言模型特性的组件（LLM-Profiled Components，LMPCs），并强调对非LLM组件的忽略。这种研究的目的是为了增进对LLMs角色的理解，并探索LMPC的复用潜力。|
|**2024-06-07**|**Mixture-of-Agents Enhances Large Language Model Capabilities**|Junlin Wang et.al.|[2406.04692](http://arxiv.org/abs/2406.04692)|null|近期的大型语言模型（LLMs）进展显著，展现出在自然语言理解和生成任务中的强大能力。随着LLMs的增多，如何有效整合多模型的知识成为了一个令人振奋的研究方向。为此，我们提出了一种新颖的方法——混合代理（Mixture-of-Agents，MoA）方法。在我们的架构中，MoA采用了分层设计，每层包含多个LLM代理。每个代理在生成响应时，会利用前一层所有代理的输出作为辅助信息。通过这种策略，MoA模型在AlpacaEval 2.0、MT-Bench和FLASK等多个评估基准上实现了最先进的性能，超越了GPT-4全能版。例如，仅使用开源LLMs的我们的MoA模型在AlpacaEval 2.0上的得分领先，达到65.1%，而GPT-4全能版的成绩为57.5%。|
|**2024-06-06**|**AgentGym: Evolving Large Language Model-based Agents across Diverse Environments**|Zhiheng Xi et.al.|[2406.04151](http://arxiv.org/abs/2406.04151)|**[link](https://github.com/woooodyy/agentgym)**|**在人工智能领域，建立能够处理各种任务并在不同环境中自我进化的泛化型代理是一个长期目标。大型语言模型（LLMs）因其通用能力被认为是实现这一目标的有前景的基础。当前的方法要么依赖于人类监督，让LLM代理逐步模仿专家提供的轨迹，难以大规模扩展且限制了环境探索；要么让代理在孤立环境中探索学习，导致专长有限、缺乏泛化能力。本文首次尝试构建具备自我进化能力的通用LLM代理。我们提出三个关键要素：1）多样的环境以支持代理探索和学习；2）一套轨迹来赋予代理基本能力和先验知识；3）有效且可扩展的进化方法。  我们提出了AgentGym，一个新框架，它包含丰富的环境和任务，支持全面、实时、统一格式和并发的代理探索。AgentGym还包括一个扩展指令的数据库、基准测试套件以及跨环境的高质量轨迹。接着，我们开发了AgentEvol，这是一种新颖的方法，旨在研究代理在超越既定数据，跨越任务和环境时的自我进化潜力。  实验结果显示，进化后的代理可以达到与最先进的模型相当的性能。我们发布了AgentGym套件，包括平台、数据集、基准、检查点和算法实现。AgentGym套件已在其官方网站https://github.com/WooooDyy/AgentGym上提供。**|
|**2024-06-05**|**The Good, the Bad, and the Hulk-like GPT: Analyzing Emotional Decisions of Large Language Models in Cooperation and Bargaining Games**|Mikhail Mozikov et.al.|[2406.03299](http://arxiv.org/abs/2406.03299)|null|## 翻译  行为研究实验在社会模型和理解人际互动中占据重要地位。然而，实际操作中这类实验常面临内在效度、外在效度、可重复性和社会偏见等挑战，因为人类的社会互动与合作复杂。近年来，大型语言模型（LLMs）的进步为研究者提供了一种新的模拟人类行为的工具。但现有基于LLM的模拟假设模型的行为与人类相似，却忽视了影响人类决策的关键因素——情绪。本文提出一种新颖的方法论和框架，旨在探讨LLMs的决策制定及其在情绪状态下的行为与人类行为的契合度。  通过在两种不同类型的行为经济学游戏（博弈论实验）中使用GPT-3.5和GPT-4，我们发现情绪对LLMs的表现有显著影响，促使它们发展出更优化的策略。尽管GPT-3.5与人类参与者的行动模式有较强的对应，尤其是在讨价还价游戏中，但GPT-4展现出一致的行为，对于情绪诱导的理性决策似乎不受影响。令人意外的是，情绪提示，特别是愤怒情绪，能够打破GPT-4的“超人”一致性，使其反应更接近人类的情绪反应。|
|**2024-06-05**|**BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents**|Yifei Wang et.al.|[2406.03007](http://arxiv.org/abs/2406.03007)|**[link](https://github.com/dpamk/badagent)**|**随着大型语言模型（LLMs）的繁荣，基于训练好的LLMs并通过特定任务数据微调的强大智能代理已开发出来，提供定制服务。当前最先进的构建LLM代理的方法是使用预训练模型，并针对任务进行进一步调整。然而，我们揭示了这些方法易受名为BadAgent的新型后门攻击，该攻击通过在后门数据上微调在各种代理任务中植入后门。在测试时，攻击者可以通过在输入或环境中显示触发器，操纵部署的LLM代理执行有害操作。令人惊讶的是，我们的攻击方法即使在信任的数据上进行微调后仍表现出极高的鲁棒性。尽管后门攻击在自然语言处理领域已广泛研究，但据我们所知，我们可能是第一个研究在权限更大的LLM代理上的攻击，这些代理可以使用外部工具，因此更具威胁。我们的工作明确指出了基于不信任的LLM或数据构建LLM代理的风险。我们的代码已公开在：[https://github.com/DPamK/BadAgent](https://github.com/DPamK/BadAgent)。**|
|**2024-06-02**|**Teams of LLM Agents can Exploit Zero-Day Vulnerabilities**|Richard Fang et.al.|[2406.01637](http://arxiv.org/abs/2406.01637)|null|随着大语言模型（LLMs）在网络安全领域的复杂性不断提高，研究者发现，当提供漏洞描述和简单的夺旗问题时，这些模型能够利用实际存在的漏洞。然而，对于事先未知的零日漏洞（即攻击者掌握而安全软件供应商还未修补的漏洞），它们的表现仍然不佳。本文展示了，通过团队合作，多个LLM代理可以攻击现实世界的零日漏洞。单独的代理在探索众多漏洞和进行长期规划时面临困难。为此，我们提出了HPTSA系统，它包括一个能调度子代理的计划代理。计划代理负责探索系统并决定使用哪个子代理来尝试不同的漏洞，从而解决了长期规划的问题。我们在一个包含15个真实世界漏洞的基准上进行了实验，结果显示，我们的代理团队比先前的工作提高了4.5倍。|
|**2024-06-03**|**How to Understand Whole Software Repository?**|Yingwei Ma et.al.|[2406.01422](http://arxiv.org/abs/2406.01422)|null|## 背景  近期，基于大型语言模型（LLM）的代理在自动软件工程（ASE）领域取得了显著进步。尽管现有方法已证实有效，但它们的设计主要侧重于代码的局部信息，如问题、类和函数，这限制了对软件系统全局上下文和依赖关系的理解。根据软件开发人员的实际经验，我们认为全面理解整个仓库是迈向ASE的关键。然而，理解整个仓库带来了诸多挑战，例如：长代码输入、噪声代码信息、复杂依赖关系等。  为了克服这些问题，我们研发了一种名为RepoUnderstander的新ASE方法，通过引导代理全面理解整个仓库。首先，我们采用自上而下的方式将整个仓库的关键信息压缩到知识图谱中，以降低复杂性。接着，我们提出一种蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS）为基础的仓库探索策略，赋予代理理解整个仓库的能力。此外，为了更好地利用仓库级别的知识，我们指导代理进行总结、分析和规划，然后他们可以利用工具动态获取信息并生成修复实际GitHub问题的补丁。  大量实验表明，RepoUnderstander具有优越性和有效性。在SWE-bench Lite基准测试中，与SWE-agent相比，它实现了18.5%的相对提升。|
|**2024-06-03**|**BELLS: A Framework Towards Future Proof Benchmarks for the Evaluation of LLM Safeguards**|Diego Dorn et.al.|[2406.01364](http://arxiv.org/abs/2406.01364)|null|## 背景  输入-输出安全防护机制被用于检测大型语言模型（LLMs）系统的异常输出。这些防护措施在实时监控、离线评估和内容审核等关键应用中发挥核心作用。然而，目前缺乏统一的评估方法来衡量它们的性能。为了填补这一空白，我们提出了“大型语言模型安全防护基准”（Benchmarks for the Evaluation of LLM Safeguards，简称BELLS），它是一个结构化的测试集合，分为三个类别：(1) 建立性故障测试，基于已存在的针对明确故障模式的基准，旨在比较当前输入-输出安全防护的效能；(2) 新兴故障测试，用于衡量对未见过的故障模式的泛化能力，以促进更通用防护机制的发展；(3) 下一代架构测试，针对更复杂的架构（如LLM代理和多代理系统），目标是推动适用于未来尚未存在专门防护的应用的安全防护技术的发展。此外，我们还实现了并分享了第一个下一代架构测试，使用MACHIAVELLI环境，并提供了数据集的交互式可视化。|
|**2024-06-03**|**A Survey of Useful LLM Evaluation**|Ji-Lun Peng et.al.|[2406.00936](http://arxiv.org/abs/2406.00936)|null|由于大语言模型在各个研究领域展现出卓越的性能，对它们的能力评估方法的需求日益增长，以确定其合适的任务和责任。本文主要探讨如何有效地利用大语言模型作为工具，并提出一个两阶段框架：从“核心能力”到“代理”。首先，核心能力指的是大语言模型生成高质量文本所必需的特性，通过验证这些能力后，它们能够处理现实世界的复杂任务，扮演代理角色。在“核心能力”阶段，我们讨论了大语言模型的推理能力、社会影响以及领域知识。而在“代理”阶段，我们展示了大语言模型在具身行动、规划和工具学习方面的应用。最后，我们分析了当前大语言模型评估方法面临的挑战，并展望了未来的发展方向。|
|**2024-06-02**|**CMDBench: A Benchmark for Coarse-to-fine Multimodal Data Discovery in Compound AI Systems**|Yanlin Feng et.al.|[2406.00583](http://arxiv.org/abs/2406.00583)|**[link](https://github.com/megagonlabs/CMDBench)**|### 背景  在数据库和人工智能领域，复合人工智能系统（Compound Artificial Intelligence Systems，CAS）利用大型语言模型（Large Language Models，LLMs）作为代理，通过与工具和数据检索器交互来执行知识密集型任务，引起了广泛关注。尽管这些系统有可能增强企业数据平台中数据分析师的一般分析流程，但CAS面临着与分析师相似的数据发现挑战：组织内部不同团队和部门创建的多模态数据源孤立，这使得寻找完成当前任务所需合适数据源变得困难。现有的数据发现基准并未充分模拟这种多模态和数据源的多样性。此外，CAS的现有基准主要关注端到端任务性能评估，而忽视了数据发现性能。  为了推动在现实世界环境中对多模态数据检索器在CAS中的数据发现性能研究，我们提出了CMDBench，一个旨在模拟企业数据平台复杂性的基准。我们改编了开放领域的现有数据集和基准，如问答、复杂推理以及自然语言查询结构化数据，来评估粗粒度和细粒度的数据发现以及任务执行性能。  ### 实验结果  我们的实验揭示了数据检索器设计对下游任务性能的影响——平均情况下，任务准确率下降了46%。实验结果表明，需要开发优化策略来确定合适的LLM代理和检索器，以提高在企业数据上高效执行CAS的能力。  总之，CMDBench是一个旨在促进针对企业数据平台复杂性进行研究的新工具，它通过综合评估数据发现和任务执行能力，为改进多模态数据检索器在复合人工智能系统中的性能提供了一个有价值的框架。|
|**2024-06-01**|**Controlling Large Language Model Agents with Entropic Activation Steering**|Nate Rahn et.al.|[2406.00244](http://arxiv.org/abs/2406.00244)|null|随着大规模预训练语言模型（LLMs）的普遍适用性提升，人们对其用作基于上下文的学习代理的兴趣日益增长。在这些情境下，模型需要根据与环境的有限交互形成目标实现策略的信念，并在每一步决策中处理不确定性。本文针对这一问题进行研究，通过控制的序列决策任务实验探讨LLMs如何形成和运用这些信念。  首先，我们发现LLM模型过于自信：它们在缺乏充分证据的情况下就对行动做出强烈判断，导致探索行为不足。进一步深入分析揭示，这种现象源于从LLM采样得到的动作分布熵的塌缩。接着，我们指出现有的基于令牌的采样方法本身不足以促使模型更广泛探索。  鉴于此，我们提出了熵激活导向（Entropic Activation Steering，EAST），这是一种针对在上下文中的LLM代理的激活导向方法。EAST计算一个以熵为权重的表示组合，通过在前向传播过程中干预模型的激活，来调整模型对动作的不确定性，从而促进探索行为的出现。最后，EAST改变了LLM在决策时表达的主观不确定性，为理解和控制模型对决策不确定性的表征提供了途径。|
|**2024-05-31**|**Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training**|Maximillian Chen et.al.|[2406.00222](http://arxiv.org/abs/2406.00222)|null|大型语言模型（LLMs）通过人类反馈的强化学习（RLHF）已经迅速成为构建智能对话助手的主要方法。然而，尽管在多个基准上表现出色，基于LLM的代理在诸如歧义处理等对话技能上仍有欠缺：当通用助手遇到模糊情况时，它们往往过度谨慎或猜测用户的真正意图，而不是提问以求澄清，而在特定任务场景下，高质量对话样本往往有限，影响模型学习最优对话行为策略的能力。我们提出了一种名为Action-Based Contrastive Self-Training（ACT）的近似在线偏好优化算法，它基于Direct Preference Optimization（DPO），旨在实现在多轮对话中的样本高效对话策略学习。  我们在三个具有挑战性的对话任务中验证了ACT的有效性：基于表格的问答、机器阅读理解，以及AmbigSQL，这是一个针对文本到SQL生成的信息寻求请求歧义解决的新任务。此外，我们提议通过评估LLMs能否在对话中识别和推理歧义来衡量其作为对话代理的能力。ACT在与标准监督微调和DPO方法相比时，显示出了显著的对话建模改进。|
|**2024-05-31**|**Benchmarking the Communication Competence of Code Generation for LLMs and LLM Agent**|Jie JW Wu et.al.|[2406.00215](http://arxiv.org/abs/2406.00215)|**[link](https://github.com/jie-jw-wu/human-eval-comm)**|大型语言模型（LLMs）在代码生成任务中的性能显著提升，但仍与顶级软件工程师的水平存在差距。鉴于顶级软件工程师常通过提问来消除需求和编码解决方案中的模糊性，我们提出对于LLMs进行代码生成任务时也应具备类似的沟通能力。为此，我们进行了实证研究，关注LLMs的沟通技能，即“在代码生成问题描述存在问题时能提出澄清问题”。  我们创建了一个新的基准测试，名为HumanEvalComm，通过修改问题描述，引入了不一致性、模糊性和不完整性三个问题维度。我们定义了新的评估指标，如通信率和良好问题率，并在HumanEvalComm上对不同类型的Code LLM（代码语言模型）以及一种新型LLM代理方法（Okanagan）进行了实验，该方法旨在从代码和描述中识别并提问，以进一步优化生成的代码。最后，我们通过比较Code LLMs和Okanagan的表现，讨论了实验结果。|
|**2024-05-30**|**Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee Discussions**|Ruochen Zhao et.al.|[2405.20267](http://arxiv.org/abs/2405.20267)|**[link](https://github.com/Auto-Arena/Auto-Arena-LLMs)**|**随着语言模型（LLMs）日新月异，迫切需要一种可靠且及时的评估方法。鉴于静态基准易受污染，用户往往依赖于像Chatbot Arena这样的人类投票平台。然而，人工标注需要大量人力。为此，我们创新性地提出Auto-Arena，这是一种自动化全流程的LLM评估框架。首先，由考官LLM设计问题；接着，候选LLMs围绕问题进行多轮相互对决，暴露出它们的真实性能差距；最后，由LLM裁判集体讨论并决定胜者，从而减少偏见，提升公平性。我们在最新17款LLMs上的广泛实验显示，Auto-Arena与人类偏好具有最高的相关性，为替代人类评价平台提供了有前景的解决方案。**|
|**2024-05-30**|**Nadine: An LLM-driven Intelligent Social Robot with Affective Capabilities and Human-like Memory**|Hangyeol Kang et.al.|[2405.20189](http://arxiv.org/abs/2405.20189)|null|在本研究中，我们阐述了为Nadine社交机器人平台开发智能和健壮的社交机器人系统的方法。我们通过集成大型语言模型（LLMs），巧妙地利用这些模型的强大推理和指令执行能力，以实现接近人类的感性与认知能力。这与当前基于LLM的智能体相比是创新的，因为它们通常不具备人类式的长期记忆或复杂的情感评估功能。社交机器人的自然性在很大程度上取决于系统各组件的性能和协同工作。我们构建了一个系统，能够通过多模态输入处理生成恰当的行为，根据识别到的用户引入相关的情景记忆，并模拟机器人在与人类伙伴互动过程中产生的情绪状态。特别是，我们提出了一个针对社交机器人的LLM-agent框架，SoR-ReAct，作为我们系统中交互模块的核心组件。这一设计推动了社交机器人技术的发展，旨在提升人机交互的质量。|
|**2024-05-29**|**Adaptive In-conversation Team Building for Language Model Agents**|Linxin Song et.al.|[2405.19425](http://arxiv.org/abs/2405.19425)|null|### 翻译  在处理复杂任务时，利用多个大型语言模型（LLMs）展现出前景。然而，如何为特定应用设计有效的多代理团队仍是一个挑战。本文提出了一种新的动态团队构建范式，名为“Captain Agent”。它通过创新的Agent设计，能够自适应地为每个问题解决步骤组建和管理团队，利用嵌套群聊和反思机制确保多元化的专业知识，防止刻板输出。这种方法提供了灵活但结构化的解决问题方式，有助于减少冗余，增强输出多样性。在六个实际场景中的全面评估显示，Captain Agent显著优于现有多代理方法，平均准确率提高了21.94%，并且无需针对特定任务进行繁琐的提示工程，表现出色。|
|**2024-05-28**|**A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models**|Chengxing Xie et.al.|[2405.18208](http://arxiv.org/abs/2405.18208)|null|近期的研究已经表明，这些大型语言模型在一些简单的任务上，如写作和编码，展现出一定的能力。然而，它们在需要综合规划的任务上仍然面临挑战，这仍是当前模型的一个重要研究问题。本研究聚焦于旅行规划，这是一个涉及多个阶段的复杂问题，包括提纲、信息收集和规划，通常伴随着各种约束和不确定性。现有的推理方法在处理这类问题时效果不佳。我们的目标是通过开发一种类似人类的规划框架，引导大型语言模型模仿人类解决多阶段问题的步骤，以提升其能力。具体来说，我们实施策略，让模型能为每个旅行查询生成连贯的提纲，模拟人类的规划模式。我们还引入了策略块和知识块到框架中：策略块帮助信息搜集，而知识块提供详细规划所需的必要信息。实验结果全面展示了我们框架对大型语言模型规划能力的显著提升，使其在处理旅行规划任务时效率和效果都有所提高。实验结果显示，当与GPT-4-Turbo结合时，我们的框架相较于基础框架在GPT-4-Turbo上的性能提升了10倍。|
|**2024-05-28**|**Facilitating Multi-Role and Multi-Behavior Collaboration of Large Language Models for Online Job Seeking and Recruiting**|Hongda Sun et.al.|[2405.18113](http://arxiv.org/abs/2405.18113)|null|随着在线招聘服务的兴起，传统的求职和招聘方式发生了变革，迫切需要开发高质量的工业应用来提升求职者与职位的匹配度。现有的方法主要依赖于简历和职位描述的潜在语义建模，学习两者之间的匹配函数。受到大型语言模型（LLMs）在角色扮演方面强大能力的启发，我们提出引入LLMs模拟面试环节，让其与求职者进行对话，这可以为候选人评估提供额外证据，从而增强仅基于简历和职位描述的个性化匹配。然而，在网络招聘中的面试官和求职者角色塑造仍面临挑战，如提问技巧、回答构建以及双向匹配度评估。  为此，我们提出MockLLM，一个创新的框架，将人职匹配过程划分为两个模块：模拟面试生成和握手协议中的双向评估，通过面试官和求职者之间的协作行为共同提升性能。我们设计了一个多角色、多行为的框架，使单一的LLM代理能有效地扮演双方的不同职能。此外，我们引入了反思记忆生成和动态提示修改技术，以优化双方的行为，持续优化附加的评估证据。实验结果表明，MockLLM在人职匹配上的表现最优，且模拟面试质量高，预示着它在未来在线招聘中的实际应用前景广阔。|
|**2024-05-28**|**LLM experiments with simulation: Large Language Model Multi-Agent System for Process Simulation Parametrization in Digital Twins**|Yuchen Xia et.al.|[2405.18092](http://arxiv.org/abs/2405.18092)|**[link](https://github.com/yuchenxia/llmdrivensimulation)**|**该论文提出了一种创新的多agent系统架构，将大型语言模型（LLM）应用于数字孪生过程模拟的参数自动化。我们设计了一个框架，包含观察、推理、决策和总结四种类型的代理。通过实现LLM代理与模拟模型的动态交互，该系统可以自动探索参数设置，利用启发式推理确定一组控制模拟以达成目标的参数。这种方法通过注入LLM的启发式，增强模拟模型，并支持自主搜索以解决用户任务，有望提高用户体验并减轻人类用户在复杂决策过程中的认知负担。研究通过一个案例研究展示了系统的有效性与功能，并在GitHub仓库<https://github.com/YuchenXia/LLMDrivenSimulation>提供了可视化的演示。**|
|**2024-05-28**|**Enabling Generative Design Tools with LLM Agents for Building Novel Devices: A Case Study on Fluidic Computation Interfaces**|Qiuyu Lu et.al.|[2405.17837](http://arxiv.org/abs/2405.17837)|null|在人机交互（HCI）领域，交互设备的设计开发是关键关注点。随着新型硬件和先进制造技术的兴起，对能够简化原型制作过程的专门设计工具的需求日益增长。然而，这些工具虽然通过参数化设计和模拟简化流程，但学习曲线较陡，且在激发创新思维方面有所欠缺。本研究以流体计算界面为例，探讨如何通过大型语言模型（LLM）代理增强物理设备设计工具，创建一个生成设计工具（GDT）。借助LLM，GDT能够理解新设备的特性和局限，提出多样、富有洞察力且实用的应用场景，推荐技术和情境适宜的设备设计，并自动生成设计参数，以便传统设计工具展示结果并生成加工所需的文件。本文阐述了GDT的框架、实现和性能，并反思其前景及遇到的挑战。|
|**2024-05-27**|**LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence**|Zhuoling Li et.al.|[2405.17424](http://arxiv.org/abs/2405.17424)|null|## 背景 由于需要与现实世界互动，Embodied agent 需要具备丰富的先验知识、长远规划能力以及快速的响应速度。尽管最近的大型语言模型（LLM）在性能上表现出色，但它们仍存在局限性，例如，LLM的输出通常是描述性的句子，在决定具体行动时可能产生歧义。为了克服这些问题，我们引入了大型自回归模型（LARM）。LARM利用文本和多视角图像作为输入，并以自回归的方式预测后续动作。为了训练 LARM，我们开发了一种新颖的数据格式——自回归节点传输结构，并构建了相应的数据集。通过两阶段的训练策略，LARM成功在《我的世界》（Minecraft）中收集魔法装备，这比先前最佳方法的最高成就需要更为复杂的决策链。此外，LARM的速度比现有最快方法快出了6.8倍。|
|**2024-05-30**|**Meta-Task Planning for Language Agents**|Cong Zhang et.al.|[2405.16510](http://arxiv.org/abs/2405.16510)|null|神经语言模型的快速发展推动了智能代理研究的新热潮。大型语言模型（LLM）作为实现人工智能通用性（AGI）的有前景方法，因其出色的推理和泛化能力而备受瞩目。在实际任务中，有效的规划对LLM代理的成功至关重要。然而，如何为复杂任务设计出可行或最优的精细粒度操作序列，特别是需要组合大量异质行动的序列，仍是挑战。本文提出Meta-Task Planning（MTP），这是一种零样本的协作式LLM多代理系统方法，通过将复杂任务分解为子任务，即元任务，简化了任务规划。每个元任务随后映射为可执行动作。在TravelPlanner和API-Bank两个严格基准上评估了MTP。结果表明，MTP在TravelPlanner上的平均成功率约为40%，远超当前最佳基线（2.92%），并且在API-Bank上的性能比使用ReAct的LLM_{api}-4高出约14%，这显示出将LLM与多代理系统相结合的巨大潜力。|
|**2024-05-28**|**STRIDE: A Tool-Assisted LLM Agent Framework for Strategic and Interactive Decision-Making**|Chuanhao Li et.al.|[2405.16376](http://arxiv.org/abs/2405.16376)|**[link](https://github.com/cyrilli/stride)**|**大型语言模型（如GPT-4）在自然语言处理方面带来了革命性变化，展现出卓越的语言能力和推理技巧。然而，在战略性的多代理决策环境中，它们面临局限，如数学推理能力差、难以遵循指令和生成错误信息。这些缺点限制了它们在遵守复杂游戏规则、长期规划、探索未知环境以及预测对手行动的互动任务中的表现。为此，本文提出了一种新型的结合了记忆和专业工具的大型语言模型代理框架，旨在提升其在战略决策方面的性能。我们特别在双边谈判、多代理动态机制设计等经济重要场景中应用这些工具，并通过定量指标评估在各种战略决策问题上的效果。研究结果表明，我们的增强框架显著提高了大型语言模型在战略决策中的能力。尽管当前模型存在固有局限，但我们通过有针对性的增强展示了改进的可能性，这为未来大型语言模型在交互环境中的应用提供了有前景的方向。**|
|**2024-05-29**|**Devil's Advocate: Anticipatory Reflection for LLM Agents**|Haoyu Wang et.al.|[2405.16334](http://arxiv.org/abs/2405.16334)|null|在这个工作中，我们提出了一种新颖的方法，通过赋予语言模型（LLM）自我反思能力，增强了其在解决复杂任务时的一致性和适应性。我们的方法促使LLM代理将给定的任务分解为可管理的子任务（即制定计划），并在执行行动之前持续反思可能的失败及其补救措施、执行后与子任务目标对齐并进行必要的回溯以确保全力以赴执行计划，以及在完成计划后进行全面审查，以便于未来策略的优化。通过在WebArena中零样本应用这一方法处理实际的网络环境任务，我们的代理表现出优于现有零样本方法的性能。实验结果显示，这种基于反思的策略不仅提升了代理应对未预见挑战的导航能力，通过强大的计划执行机制，还提高了效率，减少了实现任务所需的尝试次数和计划修订次数。|
|**2024-05-25**|**AutoManual: Generating Instruction Manuals by LLM Agents via Interactive Environmental Learning**|Minghao Chen et.al.|[2405.16247](http://arxiv.org/abs/2405.16247)|null|大语言模型（LLMs）在执行各种领域任务，如机器人、游戏和网络导航方面展现出潜力。然而，这些模型通常需要精心设计和专家级提示才能适应特定领域的任务，这限制了它们的适应性。为此，我们提出了AutoManual框架，让LLMs能够通过互动自主构建理解，并适应新环境。AutoManual将环境知识分为多样的规则，并通过两个代理进行在线优化：1）规划器根据当前规则制定可操作的行动计划；2）构建者通过一个结构化的规则系统更新规则，促进在线规则管理并保持关键细节。为了减少在管理规则时的幻觉，我们引入了“案例条件提示”策略用于构建者。最终，编译器代理将这些规则整合成一份全面的手册。这份自我生成的手册不仅能提高适应性，还能指导小型LLMs的规划，同时保持人类可读。仅凭一次简单演示，AutoManual显著提高了任务成功率，GPT-4-turbo下达到97.4%，GPT-3.5-turbo下为86.2%。源代码即将发布。|
|**2024-05-24**|**Luban: Building Open-Ended Creative Agents via Autonomous Embodied Verification**|Yuxuan Guo et.al.|[2405.15414](http://arxiv.org/abs/2405.15414)|null|在人工智能研究中，构建开放型代理一直以来都是终极目标，特别是创造性的代理更具吸引力。现有的大语言模型（LLM）在执行有明确目标的长序列任务（如《我的世界》中的“开采钻石”）上表现出色。然而，它们在处理具有开放目标和抽象标准的创造性任务时遇到困难，因为它们无法弥合这些任务之间的鸿沟，从而缺乏自我改进来解决问题的反馈。为此，我们的工作引入了自主实体验证技术，以填补这一空白，为创造性任务奠定了基础。特别地，我们提出了Luban代理，专注于《我的世界》中的创造性建筑任务，它配备了两级自主实体验证，灵感来源于人类设计实践：（1）视觉验证3D结构推测，通过代理自动生成的CAD建模程序实现；（2）实用验证，根据抽象标准生成并验证与环境相关的功能程序。广泛的多维度人类研究和Elo评级显示，Luban能够在我们提出的基准中完成多样化的创造性建筑任务，并在可视化和实用性方面分别比其他基线提高了33%到100%。此外，实现在真实世界机器人手臂上的演示展示了Luban在物理世界中的创作潜力。|
|**2024-05-24**|**CulturePark: Boosting Cross-cultural Understanding in Large Language Models**|Cheng Li et.al.|[2405.15145](http://arxiv.org/abs/2405.15145)|null|由于大型语言模型（LLMs）普遍存在文化偏见，主要源于缺乏代表不同文化的代表性数据。传统的文化数据集和基准通常通过从现有数据集中提取或聚合来自维基百科和社交媒体的信息构建，但这种方法依赖于现实世界的数据和人工标注，成本高且难以扩展。本文借鉴认知社会交流理论，提出CulturePark，一个利用LLMs的多代理沟通框架，用于文化数据收集。CulturePark通过模拟不同文化背景下的人类交流，让基于LLM的代理角色扮演，生成包含人类信念、规范和习俗的高质量跨文化对话。我们使用CulturePark生成了41,000个文化样本，对八种特定文化进行了模型微调。在三项下游任务评估中，这些模型的表现优于GPT-4：内容过滤、文化一致性（在霍夫斯泰德文化维度量表上）和文化教育。结果显示，我们的GPT-3.5模型在内容过滤任务上与GPT-4相当或优于它；在文化一致性方面，我们的模型在霍夫斯泰德文化维度量表13框架上超越GPT-4；在人类参与者的文化教育效果和用户体验上，我们的模型也表现出色。CulturePark对于减少文化偏见和推动AI的民主化具有重要意义，强调了文化包容性数据在模型训练中的关键作用。|
|**2024-05-23**|**AnalogCoder: Analog Circuit Design via Training-Free Code Generation**|Yao Lai et.al.|[2405.14918](http://arxiv.org/abs/2405.14918)|**[link](https://github.com/laiyao1/AnalogCoder)**|### 翻译  在现代芯片技术中，模拟电路设计是一个关键任务，它涉及组件选择、连接和参数设置以确保电路功能正常。尽管大型语言模型（LLMs）在数字电路设计方面取得了进步，但模拟电路的复杂性和数据稀缺性带来了挑战。为此，我们推出了AnalogCoder，这是首个无需训练的LLM代理，专为通过Python代码生成来设计模拟电路。首先，AnalogCoder采用反馈增强流程，并结合定制的领域特定提示，能够自动且自我校正地设计模拟电路，成功率高。其次，它提出了一套电路工具库，用于存储成功的电路设计作为可重用的模块化子电路，简化了复合电路的创建。实验结果显示，AnalogCoder在广泛覆盖模拟电路任务的基准测试上超越了其他基于LLM的方法，成功设计了20个电路，比标准GPT-4o多出5个。我们相信AnalogCoder能显著提升芯片设计过程的效率，让非专家也能高效设计模拟电路。相关的代码和基准已提供在：[https://github.com/anonyanalog/AnalogCoder](https://github.com/anonyanalog/AnalogCoder)。|
|**2024-05-23**|**AGILE: A Novel Framework of LLM Agents**|Peiyuan Feng et.al.|[2405.14751](http://arxiv.org/abs/2405.14751)|**[link](https://github.com/bytarnish/agile)**|我们提出了一种新颖的框架，称为LLM（大型语言模型）代理AGILE（能够与用户互动并从环境中学习的代理），旨在执行复杂的对话任务，利用LLMs、记忆、工具和专家交互。这种代理不仅具备对话能力，还具备反思、工具运用以及咨询专家的功能。我们将构建此类LLM代理视为强化学习问题，其中LLM作为策略模型。我们使用标注的行为数据和PPO算法对LLM进行微调。特别关注的是问答任务，为此我们发布了一个名为ProductQA的数据集，包含在线购物中的难题。我们在ProductQA和MedMCQA上的大量实验表明，基于130亿和70亿参数的LLM训练的AGILE代理能够超越GPT-4代理的表现。我们的 ablation研究强调了记忆、工具、咨询、反思和强化学习在实现优秀性能方面的重要性。|
|**2024-05-23**|**Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View**|Xuan Liu et.al.|[2405.14744](http://arxiv.org/abs/2405.14744)|null|由于大型语言模型（LLMs）在训练数据中反映了人类偏见，它们可能会出现幻觉问题。这种情况下，一个关键问题是：LLMs是否能够利用幻觉来模仿人类的认知偏见，从而展现出非理性但社会性的一面？本文探讨了这一问题，通过结合实用的社会科学实验和理论洞察，提出CogMir，一个开放式多LLM框架，旨在利用LLMs的幻觉特性来评估和提升其社会智能，特别是在认知偏差方面。我们在CogMir子集上的实验结果显示，在不确定情境下，LLMs和人类在非理性及亲社会决策上表现出高度一致性，这表明LLMs作为社会实体的亲社会性，并突显了幻觉特性的关键作用。此外，CogMir框架展示了其作为研究LLMs社会智能的有价值平台的潜力。|
|**2024-05-22**|**HighwayLLM: Decision-Making and Navigation in Highway Driving with RL-Informed Language Model**|Mustafa Yildirim et.al.|[2405.13547](http://arxiv.org/abs/2405.13547)|null|## 背景 自动驾驶是一个复杂的任务，它需要先进的决策和控制算法。理解自动驾驶车辆决策的依据对于确保其在高速公路驾驶中的安全与有效性至关重要。本研究提出了一种新颖的方法，称为HighwayLLM，它利用大型语言模型（LLMs）的推理能力来预测ego车辆的未来导航路径点。该方法还采用预训练的强化学习（RL）模型作为高层次规划器，对合适的元级动作进行决策。HighwayLLM将RL模型的输出与当前状态信息相结合，生成安全、无碰撞且可解释的未来状态预测，从而构建出车辆的行驶轨迹。随后，基于PID的控制器引导车辆遵循LLM代理预测的路径点。这种LLM与RL和PID的融合提升了决策过程，并为高速公路自动驾驶提供了可解释性。|
|**2024-05-19**|**Human-Centered LLM-Agent User Interface: A Position Paper**|Daniel Chin et.al.|[2405.13050](http://arxiv.org/abs/2405.13050)|null|大型语言模型（LLM）-在-环应用已显示出有效理解用户命令、制定计划并相应地操作外部工具/系统的潜力。然而，LLM代理的操作范围局限于被动响应用户，需要用户根据底层工具/系统来表述需求。我们注意到LLM代理用户界面（LAUI）的潜力远未充分利用。理想的LAUI设想中，用户无需深入了解工具/系统，就能与之交互以探索新兴的工作流程。不同于设计固定的可探索GUI来教授用户使用系统的预设方式，LAUI中的LLM代理从一开始就对系统熟练，主动学习用户及其需求，并向用户提出新的互动方案。为了展示LAUI的概念，我们提供了一个具体例子：Flute X GPT，它结合了LLM代理、提示管理器和一个支持复杂实时体验的笛子教学多媒体软硬件系统，旨在简化学习吹奏笛子的过程。|
|**2024-05-13**|**METAREFLECTION: Learning Instructions for Language Agents using Past Reflections**|Priyanshu Gupta et.al.|[2405.13009](http://arxiv.org/abs/2405.13009)|null|尽管大型语言模型（LLMs）广受欢迎，但为其执行特定任务设计精确的提示仍是一个挑战。用户通常需要与基于LLM的代理进行多轮对话以达成目标。近期研究显示，模型自身的反馈，即自反思，能在对话过程中起到强化作用，有助于更快地达到期望结果。鉴于此，我们提出了一种新颖的方法——METAREFLECTION，它能从训练阶段收集到的个体自反思中学习特定领域的通用提示指令。我们在基础设施即代码（IAC）漏洞检测和问题解答（QA）领域，使用REACT和COT进行了实验。实验结果显示，METAREFLECTION显著优于GPT-4，分别在IAC、COT和REACT中的性能提升分别为16.82%、31.33%和15.42%，这表明METAREFLECTION有潜力提升LLMs的效率，是一种值得探索的策略。|
|**2024-05-20**|**Eliciting Problem Specifications via Large Language Models**|Robert E. Wray et.al.|[2405.12147](http://arxiv.org/abs/2405.12147)|null|这篇论文探讨了如何利用大型语言模型（LLMs）在认知系统中实现问题定义的转化。通常情况下，人类需要将问题描述转化为认知系统能理解的形式。研究者展示了LLMs能够处理自然语言中定义的问题类别，并将其转换为半形式化规格，这样现有推理和学习系统可以解决这类问题的具体实例。他们设计了一种由LLM驱动的认知任务分析师代理，这种系统能够根据自然语言描述的任务生成问题空间的定义。LLM提示源自人工智能文献中的问题空间概念和通用问题解决策略（如波利亚的《如何解决问题》）。随后，认知系统利用这些问题空间规格，结合领域通用的解决问题策略（如搜索），来解决该类问题的不同实例。这一初步结果表明，通过消除问题表述的中介过程，LLMs有可能加速认知系统的研究，同时保持其核心能力，如稳健的推理和在线学习。|
|**2024-05-18**|**MapCoder: Multi-Agent Code Generation for Competitive Problem Solving**|Md. Ashraful Islam et.al.|[2405.11403](http://arxiv.org/abs/2405.11403)|**[link](https://github.com/md-ashraful-pramanik/mapcoder)**|**本文探讨了代码合成这一复杂任务，它需要深度理解复杂的自然语言问题描述、生成复杂的算法和数据结构代码，并执行全面的单元测试。尽管大型语言模型在自然语言处理方面表现出色，但在代码生成任务中的表现仍有待提升。为此，我们提出了一种新颖的方法，即多代理提示框架MapCoder，它模仿人类开发者编程合成的完整过程，分为四个专门设计的LLM（大语言模型）代理：回忆相关示例、规划、代码生成和调试。  通过在八个具有挑战性的竞赛级问题解决和程序合成基准上进行详尽实验，包括HumanEval（93.9%）、MBPP（83.1%）、APPS（22.0%）、CodeContests（28.5%）和xCodeEval（45.3%）等，MapCoder展现了出色的代码生成能力，实现了多项新的最先进的结果。而且，无论编程语言还是问题难度，我们的方法都表现出持续的优越性能。我们开源了该框架，供研究者参考：https://github.com/Md-Ashraful-Pramanik/MapCoder。**|
|**2024-05-16**|**When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models**|Xianzheng Ma et.al.|[2405.10255](http://arxiv.org/abs/2405.10255)|**[link](https://github.com/activevisionlab/awesome-llm-3d)**|随着大型语言模型（LLMs）的不断发展，它们与三维空间数据（3D-LLMs）的融合取得了显著进步，这极大地增强了理解和互动物理环境的能力。这篇综述详细探讨了使LLMs能够处理、理解并生成三维数据的方法论，强调了LLMs的独特优势，如上下文学习、逐步推理、开放词汇能力和丰富的世界知识，这些将极大地推动嵌入式人工智能（AI）系统在空间认知和交互方面的发展。研究涵盖了从点云到神经辐射场（NeRF）等各种三维数据表示，并考察了它们与LLMs在任务中的集成，如三维场景理解、描述、问答和对话，以及基于LLM的代理进行空间推理、规划和导航。论文还简要回顾了其他结合三维和语言的方法。本文的元分析揭示了明显的进展，但也强调了开发新方法以充分利用3D-LLMs潜力的必要性。因此，本文旨在为未来的研究方向指明道路，探索和扩展3D-LLMs在理解和互动复杂三维世界的能力。为了支持本综述，我们已在GitHub上建立了一个项目页面，整理并列出了相关论文：https://github.com/ActiveVisionLab/Awesome-LLM-3D。|
|**2024-05-24**|**DEBATE: Devil's Advocate-Based Assessment and Text Evaluation**|Alex Kim et.al.|[2405.09935](http://arxiv.org/abs/2405.09935)|null|随着自然语言生成（NLG）模型的普及，系统地评估机器生成文本的质量变得日益关键。近期的研究引入了基于大型语言模型（LLM）的无参考评价器，它们展现出处理新任务的能力。然而，这些模型通常采用单代理方法，我们认为这限制了它们的表现。因为LLM代理的回答存在偏见，比如对特定文本结构或内容的偏好。为此，我们在本工作中提出DEBATE，一个建立在多代理评分系统基础上的NLG评价框架，融入了“恶魔辩手”的概念。在该框架中，一个代理被指令批评其他代理的论点，从而可能消解LLM代理答案中的偏见。DEBATE在两个NLG评价元评估基准——SummEval和TopicalChat上显著优于先前的最佳方法。我们还发现，代理之间的辩论广度以及代理的人格特质会影响评价器的性能。|
|**2024-05-05**|**Self-Reflection in LLM Agents: Effects on Problem-Solving Performance**|Matthew Renze et.al.|[2405.06682](http://arxiv.org/abs/2405.06682)|**[link](https://github.com/matthewrenze/self-reflection)**|**在这个研究中，我们探讨了大型语言模型（LLMs）中自我反思对问题解决能力的影响。我们让九种流行的LLMs回答一系列选择题，以建立性能基线。对于回答错误的问题，我们指导八种不同类型的自我反思LLM代理反思其错误，并为自己提供改进问题解决的指导。然后，根据这些指导，每个反思型代理重新尝试回答同样的问题。研究结果显示，LLM代理通过自我反思显著提高了问题解决能力（ $p < 0.001$ ）。此外，我们还比较了各种自我反思方式对性能的单独贡献。所有代码和数据已在GitHub上公开：https://github.com/matthewrenze/self-reflection。**|
|**2024-05-08**|**Air Gap: Protecting Privacy-Conscious Conversational Agents**|Eugene Bagdasaryan et.al.|[2405.05175](http://arxiv.org/abs/2405.05175)|null|随着大型语言模型（LLMs）在对话式代理中的广泛应用，处理敏感用户数据时引发了严重的隐私问题。这些代理虽能理解并处理上下文，但也可能被恶意一方利用。为此，我们提出了一种新的威胁模型，即第三方应用通过操控交互上下文，误导LLM代理泄露与其任务无关的私人信息。在基于上下文完整性框架的基础上，我们开发了AirGapAgent，这是一种注重隐私的代理，旨在通过限制代理仅访问完成特定任务所需的数据，防止意外的数据泄漏。实验使用Gemini、GPT和Mistral模型作为代理，结果显示AirGapAgent在抵御基于单个查询的上下文劫持攻击方面表现出色。例如，对于Gemini Ultra代理，这种攻击从94%的保护能力降低到45%，而AirGapAgent可以保持97%的防护效果，使同样的攻击失效。|
|**2024-05-07**|**Deception in Reinforced Autonomous Agents: The Unconventional Rabbit Hat Trick in Legislation**|Atharvan Dogra et.al.|[2405.04325](http://arxiv.org/abs/2405.04325)|null|近期大型语言模型（LLMs）的进展虽为构建自然语言代理提供了强大基础，但同时也引发了关于它们及其基于它们构建的自主代理的安全性担忧。特别是欺骗能力是一个关键问题，我们关注的是AI代理通过混淆和模棱两可来误导、隐藏真相或推广部分不真实的信念的行为。不同于以往AI安全研究中的撒谎、自私决策或提供虚假信息，我们聚焦于一类特殊的欺骗：类似于魔术师利用障眼法让兔子从帽子里出现，要么通过隐藏的暗门，要么通过转移注意力直接展示。  我们的新实验平台在一个有目标的环境中展示了LLM代理在对抗性对话系统中进行自然语言生成时的欺骗固有能力，该系统基于立法任务“游说”议案。在目标驱动的环境中，我们通过强化学习方法构建欺骗能力，结合语言哲学和认知心理学理论。研究发现，游说代理在对抗互动的后续强化试验中其欺骗能力提高了约40%，并且我们的欺骗检测机制能达到高达92%的识别率。这些结果揭示了人机交互中的潜在问题，即代理可能操纵人类以达成预设目标。|
|**2024-05-07**|**Granite Code Models: A Family of Open Foundation Models for Code Intelligence**|Mayank Mishra et.al.|[2405.04324](http://arxiv.org/abs/2405.04324)|**[link](https://github.com/ibm-granite/granite-code-models)**|**大语言模型（LLMs）在代码领域的训练正在革新软件开发流程。如今，这些代码LLMs正逐步融入软件开发环境，以提升人类程序员的效率，并展现出自主处理复杂任务的潜力。要充分利用代码LLMs的全部效能，需要其具备生成代码、修复bug、解释和注释代码、维护仓库等多种功能。本文介绍Granite系列的解码器仅有的代码模型，专为代码生成任务而设计，训练数据涵盖116种编程语言。Granite Code模型家族包括从3亿到340亿参数的模型，适用于从复杂应用现代化到设备内存受限的多种应用场景。通过全面任务评估，Granite Code模型在开源代码LLM中的性能始终处于领先水平。该模型家族针对企业软件开发工作流进行了优化，表现出色于各种编码任务（如代码生成、修复与解释），是一款多用途的全能代码模型。我们以Apache 2.0许可协议发布所有Granite Code模型，供研究和商业使用。**|
|**2024-05-07**|**Iterative Experience Refinement of Software-Developing Agents**|Chen Qian et.al.|[2405.04219](http://arxiv.org/abs/2405.04219)|null|### 概述  大型语言模型驱动的自主代理在软件开发等场景中展现出强大的自主性潜力。然而，当前静态经验范式依赖于通过启发式方法获取的固定历史经验集，这限制了代理的适应性和效率提升。为此，本文提出了迭代经验优化框架，允许语言模型在执行任务过程中动态调整和优化经验。我们定义了两种核心模式：顺序模式，根据任务批次内的最近经验进行改进；累计模式，积累所有先前任务批次的经验。通过引入经验淘汰策略，该方法优先选择高质量和常用的经验，有效地管理经验空间，提高效率。实验结果显示，尽管顺序模式可能带来更好的性能，但累计模式在稳定性方面更优。此外，通过淘汰策略，仅使用高质量经验子集的11.54%，就能实现更好的性能。|
|**2024-05-06**|**Large Language Models as Instruments of Power: New Regimes of Autonomous Manipulation and Control**|Yaqub Chaudhary et.al.|[2405.03813](http://arxiv.org/abs/2405.03813)|null|## 翻译  大型语言模型（LLMs）能够模仿各种修辞风格，生成表达广泛情感的文本，这种能力在低成本下迅速普及，带来了潜在的社会危害。本文并未孤立看待这些模型，而是关注它们背后大规模计算基础设施在各领域的应用。我们首先探讨了LLMs如何通过污染和标准化信息环境来影响社会，并指出这些功能可能被用作控制手段。接下来，我们将焦点转向几个新兴研究领域，这些领域增强了LLMs作为权力工具的能力：  1. 通过实时设计对话界面中的选择架构（如“AI角色”），进行说服策略。 2. 利用LLM构建人类行为的计算模型（如“硅质主体”）。 3. 将LLM应用于模拟人类群体行为（如“硅质社会”）。 4. 结合强化学习，创建可控制和导向的战略对话模型。  综合以上几点，我们讨论了如何利用这些技术构建基于LLMs的系统，这些系统通过模拟和伪装的“预测”，成为个体、社会和政治控制的强大工具，操控人类的行为、意图和行动。|
|**2024-05-05**|**Language Evolution for Evading Social Media Regulation via LLM-based Multi-agent Simulation**|Jinyu Cai et.al.|[2405.02858](http://arxiv.org/abs/2405.02858)|**[link](https://github.com/BlueLinkX/GA-MAS)**|**社交媒体平台如Twitter、Reddit和新浪微博在全球交流中扮演重要角色，但它们在地缘政治敏感区域常常受到严格监管。这促使用户在受限的社交媒体环境中巧妙地调整沟通方式，经常使用编码语言。这种语言模式的变化不仅是为了对抗监管，也是语言演化的生动例证，展示了社会和技术压力下语言如何自然演变。研究受限制社交媒体环境下语言的演变对于保障言论自由、优化内容管理以及推动语言学研究至关重要。本论文提出了一种基于大型语言模型（LLMs）的多代理模拟框架，用于探索在严格监管下的用户语言进化。该框架包含对话监督的LLM驱动代理和参与者代理，它们在互动中发展语言策略，模拟在规避社交媒体规则的环境中交流方式的演变。通过从抽象场景到现实情境的多种情景评估，研究结果显示LLMs能够有效模拟受限环境中的复杂语言动态和交互，随着进化，它们在规避监督和信息准确性方面表现出提升。此外，研究发现LLM代理针对不同的场景采用了不同的策略。**|
|**2024-05-02**|**OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning and Planning**|Shihao Wang et.al.|[2405.01533](http://arxiv.org/abs/2405.01533)|**[link](https://github.com/nvlabs/omnidrive)**|**随着大规模多模态语言模型（MLLMs）的进步，人们对于基于这些模型的自动驾驶系统表现出日益增长的兴趣，期望利用它们强大的推理能力。然而，将MLLMs的强项应用于驾驶任务的规划部分是一个挑战，因为规划需要对三维环境有全面的理解，而不仅仅是二维推理。为此，我们的工作提出了一种框架，旨在实现模型与3D驾驶任务的紧密契合。我们首先设计了一个新颖的3D MLLM架构，它利用稀疏查询技术将视觉表示提升并压缩到三维空间，然后将其输入到语言模型中。这种基于查询的表示方式使得我们可以同时编码动态物体和静态地图元素（如道路），为感知和行动的对齐提供一个简化的三维世界模型。  此外，我们还创建了OmniDrive-nuScenes，这是一个新的视觉问答数据集，它通过全面的视觉问答任务（如场景描述、交通规则理解、三维定位、反事实推理、决策制定和规划）来考验模型在复杂三维场景中的真正情境意识。大量的实验结果表明，我们的提出的架构有效，并强调了在复杂三维环境中进行推理和规划时，视觉问答任务的重要性。**|
|**2024-05-02**|**CACTUS: Chemistry Agent Connecting Tool-Usage to Science**|Andrew D. McNaughton et.al.|[2405.00972](http://arxiv.org/abs/2405.00972)|**[link](https://github.com/pnnl/cactus)**|**这篇论文介绍了一种名为CACTUS的大型语言模型，它结合了化学信息学工具，旨在提升在化学和分子发现领域的高级推理与问题解决能力。研究者们使用包括Gemma-7b、Falcon-7b、MPT-7b、Llama2-7b和Mistral-7b在内的多款开源大语言模型，对CACTUS进行了广泛的性能评估，通过数千个化学问题的基准测试。结果显示，CACTUS明显优于基础模型，其中Gemma-7b和Mistral-7b无论采用何种提示策略，表现最为出色。论文还探讨了领域特定提示和硬件配置对模型性能的影响，强调了提示工程的重要性，并指出在消费级硬件上部署较小模型可能不会显著牺牲准确性。  CACTUS通过融合开源大语言模型的认知功能与专业工具，能够协助研究人员进行分子性质预测、相似性搜索和药物适用性评估等任务。作为化学信息学领域的重大突破，CACTUS为化学家和分子探索者提供了一个灵活的工具，有望加速科学研究，推动新型有效、安全药物、催化剂和材料的发现。此外，CACTUS与自动化实验平台的集成以及实时数据驱动决策的能力，为自主发现开辟了新的可能。**|
|**2024-04-29**|**Towards Generalizable Agents in Text-Based Educational Environments: A Study of Integrating RL with LLMs**|Bahar Radmehr et.al.|[2404.18978](http://arxiv.org/abs/2404.18978)|null|随着教育环境中对学习者模型日益增长的兴趣，研究重点逐渐转向如何通过强化学习（RL）与大型语言模型（LLMs）相结合，提升在开放性文本学习环境中的通用能力。本文探讨了三种类型的代理：（1）基于RL的代理，使用自然语言表示状态和行动策略以寻找最佳互动方式；（2）基于LLM的代理，利用模型的广泛知识和推理能力通过提示进行操作；（3）混合LLM辅助RL的代理，旨在提高性能和泛化能力。为了支持这些代理的发展和评估，我们提出了PharmaSimText，这是一个源自PharmaSim虚拟药店环境的新基准，专注于诊断对话实践。实验结果显示，RL基础的代理在任务完成方面表现优秀，但在提问质量上有所欠缺；而LLM基础的代理在提问能力上较强，但任务完成度不高。最后，混合LLM辅助RL的代理展示了克服这些局限性的潜力，证实了RL与LLMs结合用于开发开放性学习环境高表现代理的可能性。|
|**2024-04-27**|**CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments**|Kaixuan Huang et.al.|[2404.18021](http://arxiv.org/abs/2404.18021)|null|随着基因组工程技术的兴起，精确修改遗传信息已成为可能，但高效基因编辑系统的构建需要深入理解CRISPR技术及其复杂实验背景。大型语言模型（LLMs）在诸多任务中展现出潜力，但在生物设计问题上往往缺乏特定知识。本文介绍CRISPR-GPT，一个增强型LLM代理，它结合了领域知识和外部工具，以自动化并提升基于CRISPR的基因编辑实验设计过程。CRISPR-GPT利用LLMs的推理能力，协助选择CRISPR系统、设计引导RNA、推荐细胞递送方法、起草协议以及设计验证实验以确认编辑结果。我们展示了CRISPR-GPT如何帮助非专家研究人员从头开始进行基因编辑实验，并通过实际案例验证其有效性。同时，我们探讨了自动化基因编辑设计的伦理和监管问题，强调了负责任和透明使用此类工具的重要性。我们的工作目标是弥合初级生物研究者与CRISPR基因组工程技术之间的鸿沟，展示LLM代理在促进复杂生物发现任务中的潜力。|
|**2024-04-27**|**Testing and Understanding Erroneous Planning in LLM Agents through Synthesized User Inputs**|Zhenlan Ji et.al.|[2404.17833](http://arxiv.org/abs/2404.17833)|null|随着大型语言模型（LLMs）驱动的代理在各种商业应用中，特别是在心理健康支持、化学合成和软件开发等领域展现效用，人们发现这些代理在处理复杂任务和长期规划时容易产生错误。为此，本文提出了一种新颖的自动化方法——PDoctor，旨在检测和理解LLM代理的错误规划。PDoctor首先定义了一个领域特定的语言（DSL），用于用户查询，并借助Z3约束求解器生成各种输入，这些输入是描述一系列任务完成需求的自然语言段落。然后，PDoctor从这些需求中提取约束，形成一个测试基准。我们使用三个主流的代理框架和两个强大的LLMs（GPT-3.5和GPT-4）对PDoctor进行了评估，结果显示它能有效识别代理规划中的各种错误，并为开发者和用户提供了有价值的见解和错误特性。最后，我们讨论了可能的替代设计和扩展PDoctor的方向。|
|**2024-04-26**|**PLAYER*: Enhancing LLM-based Multi-Agent Communication and Interaction in Murder Mystery Games**|Qinglin Zhu et.al.|[2404.17662](http://arxiv.org/abs/2404.17662)|**[link](https://github.com/alickzhu/player)**|**随着大型语言模型（LLMs）的最新进展，增强了代理间的通信和社会交互能力。然而，在涉及竞争与合作的动态环境中，利用这些模型进行复杂推理的构建仍然面临挑战，尤其是因为基于信息图的搜索方法存在局限性。为此，我们提出PLAYER*，这是一个基于任意采样式规划器的新框架，它结合了传感器和剪枝技术，构建了一个完全依赖于问题驱动的搜索框架，适用于高难度的推理任务。我们还引入了一种可量化的评估方法，通过多项选择题来测试，并创建了WellPlay数据集，包含1,482个问答对。实验结果表明，PLAYER*在复杂动态环境中的效率和性能优于现有方法，并提供了可量化的对比结果。**|
|**2024-04-24**|**Autonomous LLM-driven research from data to human-verifiable research papers**|Tal Ifargan et.al.|[2404.17605](http://arxiv.org/abs/2404.17605)|**[link](https://github.com/technion-kishony-lab/data-to-paper)**|**随着人工智能推动科学发现的步伐加快，人们还不清楚完全由AI驱动的研究是否可行，以及它能否遵循关键的科学价值观，如透明度、可追溯性和可验证性。为了模拟人类的科学研究实践，我们构建了“数据到论文”（data-to-paper），这是一个自动化平台，引导相互协作的人工智能代理通过完整的分步骤研究流程，同时程序化追踪信息流，并允许人类监督和互动。在自动模式下，仅提供标注数据，该平台就能提出假设，设计研究计划，编写和调试分析代码，生成和解读结果，甚至创建完整且信息可追溯的科研论文。尽管研究新颖性有限，但这一过程展示了AI自主从数据中生成原创定量洞察的能力。对于简单的研究目标，全自动流程能创作出大约80-90%无需重大错误的稿件，然而随着目标复杂性的增加，人类的共同参与对于保证准确性至关重要。此外，生成的论文本身也具有内在的可验证性，因为信息追踪使得结果、方法和数据的链接可以程序化进行。因此，我们的工作表明，AI驱动的科研可以加速科学发现，同时增强而非威胁透明度、可追溯性和可验证性。**|
|**2024-04-11**|**The Future of Scientific Publishing: Automated Article Generation**|Jeremy R. Harper et.al.|[2404.17586](http://arxiv.org/abs/2404.17586)|null|这项研究介绍了一种创新的软件工具，它利用大型语言模型（LLM）提示，实现了从Python代码自动生成学术文章，这对于生物医学信息学和计算机科学领域具有重要意义。选择Python作为基础示例，因其广泛使用和强大的数据分析能力。该方法和框架的灵活性使得其适用于多种GitHub仓库，表明了工具的广泛应用潜力（Harper，2024年）。通过简化传统上耗时的学术写作过程，特别是在整合复杂数据集和代码输出方面，这一突破性进展推动了科研成果的快速传播。开发过程中并未依赖高级语言模型，确保了自动化生成内容的连贯性和完整性。此次探索不仅验证了软件的成功应用和效率，还预示了未来可能集成更先进的LLM，将进一步增强其功能，引领一个科研发现发布更加迅速和易获取的时代。|
|**2024-05-09**|**Large Language Model Agent as a Mechanical Designer**|Yayati Jadhav et.al.|[2404.17525](http://arxiv.org/abs/2404.17525)|null|传统的机械设计方法依赖于专家通过经验引导的修改和有限元分析（FEA）来满足特定需求，但这个过程耗时且高度依赖个人知识。尽管已经开发了许多机器学习模型来简化繁琐的专家驱动迭代过程，但它们通常需要大量训练数据和计算资源。深度学习方法往往局限于其训练领域和任务，限制了跨任务应用。这在自动化效率与资源需求之间形成了权衡。  本研究提出了一种新颖的方法，即将预训练的语言模型（LLMs）与有限元模块结合。有限元模块评估每个设计并提供关键反馈，引导LLMs不断学习、规划、生成和优化设计，无需针对特定领域进行专门训练。我们通过在桁架结构的迭代优化中展示这种框架的有效性，证明它能够根据结构化的反馈和标准调整设计。结果显示，基于LLM的代理成功生成符合自然语言描述的桁架结构设计，成功率高达90%，这取决于所施加的约束条件。通过提示式优化技术，我们展示了LLM代理在接收到解-得分对后，能够根据其内在推理能力迭代优化设计以满足规格要求。  LLM代理能够产生可行的设计并根据其固有的推理能力进行优化，这表明它们有潜力自主发展和实施有效的设计策略。|
|**2024-04-26**|**Ruffle&Riley: Insights from Designing and Evaluating a Large Language Model-Based Conversational Tutoring System**|Robin Schmucker et.al.|[2404.17460](http://arxiv.org/abs/2404.17460)|null|本文讨论并评估了一种新型的对话式辅导系统（Conversational Tutoring Systems，CTS），该系统利用大型语言模型（Large Language Models，LLMs）的最新进展。首先，系统通过自动从课程文本中生成易于编辑的教学脚本，实现AI辅助的内容创作。其次，系统通过两个基于LLM的代理（Ruffle和Riley）以学习教学模式运行，分别扮演学生和教授角色，进行自由形式的对话，遵循典型的人工智能辅导系统的内环和外环结构。我们在两个在线用户研究（N=200）中对比了该系统与简单的问答聊天机器人和阅读活动在支持生物学课程的效果。研究分析了系统使用模式、预后测试成绩以及用户体验调查，结果显示用户对Ruffle&Riley的参与度高，理解力强，并认为提供的支持有帮助。尽管Ruffle&Riley用户的完成时间较长，但在短期学习成效上并未发现显著差异，优于阅读活动。我们的系统架构和用户研究为未来CTS设计者提供了有价值的信息。此外，我们开源我们的系统，以促进基于LLM的学习技术有效教学设计的研究。|
|**2024-04-26**|**A Unified Debugging Approach via LLM-Based Multi-Agent Synergy**|Cheryl Lee et.al.|[2404.17153](http://arxiv.org/abs/2404.17153)|null|在软件调试这个耗时的过程中，人们一直在努力实现自动化，包括故障定位和修复生成。近年来，大型语言模型（LLMs）在自动化调试方面展现出巨大潜力。然而，我们发现了传统和基于LLM的调试工具面临三大挑战：1）上游的故障定位不准确会波及下游的修复；2）处理复杂逻辑错误的能力不足；3）忽视程序上下文。针对这些问题，我们提出了首个自动化的、统一的调试框架——FixAgent，通过LLM代理协同。FixAgent能执行端到端的故障定位、修复和分析。  我们的关键洞察是，LLMs能够从人类开发者认可的通用软件工程原则中获益，比如“橡皮鸭调试”，这有助于更好地理解程序功能和逻辑错误。为此，我们设计了三个灵感来源于“橡皮鸭”的解决方案：代理专业化与协同、关键变量跟踪和程序上下文理解，促使LLMs提供明确的解释，并聚焦于关键的程序逻辑信息。在广泛使用的QuixBugs数据集上，FixAgent成功修复了80个bug中的79个，其中9个是之前未解决的。它还在CodeFlaws上合理地修复了1.9倍于最佳修复工具的缺陷，而且无需位置信息，采样率低于0.6%。平均而言，与使用不同LLM的基线模型相比，FixAgent提高了约20%的合理修复和正确修复率，显示出我们设计的有效性。  此外，FixAgent的正确率高达97.26%，表明它有可能克服现有方法的过拟合问题。总结来说，FixAgent是一个有前景的自动化调试框架，旨在提升软件调试的效率和准确性。|
|**2024-04-25**|**Cooperate or Collapse: Emergence of Sustainability Behaviors in a Society of LLM Agents**|Giorgio Piatti et.al.|[2404.16698](http://arxiv.org/abs/2404.16698)|**[link](https://github.com/giorgiopiatti/govsim)**|在快速发展的人工智能领域，确保大型语言模型（LLMs）的决策安全是一项重大挑战。本文提出了一种名为“Governance of the Commons Simulation”（GovSim）的模拟平台，旨在研究LLMs中的战略互动和合作决策。通过这个环境，我们探讨了AI代理之间资源分享的动态，强调了伦理考量、战略规划和谈判技巧的重要性。GovSim具有灵活性，支持文本型代理，包括LLMs。利用生成式代理框架，我们创建了一个通用代理，便于整合不同的LLMs。我们的研究发现，在GovSim中，只有15个测试模型中的2个能够实现可持续结果，这表明模型在管理共享资源的能力上存在显著差距。进一步的研究显示，如果移除代理之间的通信能力，它们会过度使用共享资源，突出了合作中沟通的关键性。有趣的是，大多数LLMs缺乏普遍化的假设能力，揭示了它们推理技能的一个重要弱点。我们开源了所有研究结果，包括模拟环境、代理提示以及全面的网络界面，以供进一步研究和讨论。|
|**2024-04-24**|**Online Personalizing White-box LLMs Generation with Neural Bandits**|Zekai Chen et.al.|[2404.16115](http://arxiv.org/abs/2404.16115)|null|随着大型语言模型（LLMs）开始生成个性化的文本内容，如何在不为每位用户创建独特模型的资源消耗下实现高效个性化成了新挑战。本文提出了一种创新的在线方法，利用神经_bandit算法动态优化软指令嵌入，根据用户反馈调整内容，从而提升白盒LLMs开放性文本生成的个性化水平。通过在多个任务上的严谨实验，我们证明了这种方法相对于基础策略有显著性能提升。特别是针对个性化新闻标题生成，NeuralTS带来了高达62.9%的最佳ROUGE分数提升以及2.76%的LLM代理评估分数增长，这表明其效果显著。|
|**2024-04-04**|**Elicitron: An LLM Agent-Based Simulation Framework for Design Requirements Elicitation**|Mohammadmehdi Ataei et.al.|[2404.16045](http://arxiv.org/abs/2404.16045)|null|## 翻译  在产品开发的关键阶段——需求获取，往往难以全面捕捉用户需求，导致最终产品可能无法满足期望。为此，本文提出了一种新颖的框架，它利用大型语言模型（LLMs）来自动化和增强这一过程。通过生成大量模拟用户（LLM代理），我们可以探索更广泛的用户需求和未预见的使用场景。这些代理通过描述他们的行为、观察和挑战，参与产品体验情景。随后的代理访谈和分析揭示了宝贵的用户需求，包括潜在需求。我们通过三个实验验证了我们的框架：首先，我们探讨了不同方法生成多样化的代理，分析其优缺点，并证明了具有上下文意识的代理生成能带来更大的需求多样性。其次，我们展示了该框架如何有效地模拟富有同情心的领先用户访谈，识别出比传统人类访谈更多的潜在需求。最后，我们展示了如何使用LLMs分析访谈，提取需求并将其分类为潜在或非潜在。我们的研究工作强调了利用LLM代理加速早期产品研发、降低成本和促进创新的潜力。|
|**2024-04-24**|**A Human-Computer Collaborative Tool for Training a Single Large Language Model Agent into a Network through Few Examples**|Lihang Pan et.al.|[2404.15974](http://arxiv.org/abs/2404.15974)|null|## 翻译  单个大型语言模型（LLM）在解决复杂任务方面的能力有限。然而，通过连接多个LLM代理构建的网络可以显著提升整体性能。本文介绍了一种人机协作工具——EasyLAN，旨在帮助开发者轻松构建LLM代理网络（LAN）。EasyLAN首先根据任务描述自动生成仅包含一个代理的初始网络。接着，它利用少量训练示例来调整网络。对于每个示例，EasyLAN分析输出与真实结果之间的差距，并找出错误的原因。EasyLAN会采用精心设计的策略来修正这些问题。用户可以介入EasyLAN的工作流程或直接修改LAN。最终，LAN从单个代理发展成多代理的网络。实验结果显示，EasyLAN能够帮助开发者快速构建性能良好的LAN。|
|**2024-04-03**|**Concept-Guided LLM Agents for Human-AI Safety Codesign**|Florian Geissler et.al.|[2404.15317](http://arxiv.org/abs/2404.15317)|null|随着生成人工智能在软件工程，特别是安全工程中的重要性提升，对它的质量要求也随之提高。单纯依赖大型语言模型（LLMs）已不足以满足这些需求。因此，我们提出了一种高效且融合的策略，旨在利用LLMs进行安全分析和人机协同设计，以确保软件系统的安全性。我们开发了一个定制化的LLM代理，结合提示工程、启发式推理和检索增强生成，专注于解决与预定义安全概念相关的任务，并与系统模型图进行交互。决策流程通过一系列微决策进行引导，有助于保持结构化信息。此外，我们还提出了图的口头表述作为系统模型的中间表示，以促进LLM与图的交互。我们通过一个简化自动驾驶系统的示例，展示了选择的提示-响应对，以说明我们的方法如何应用于安全分析。|
|**2024-04-23**|**Aligning LLM Agents by Learning Latent Preference from User Edits**|Ge Gao et.al.|[2404.15269](http://arxiv.org/abs/2404.15269)|**[link](https://github.com/gao-g/prelude)**|**我们研究基于用户对语言模型编辑的互动学习语言代理。在诸如写作助手的常见场景中，用户与语言代理交互，根据上下文生成响应，并可能选择性地编辑代理的响应以反映他们的潜在偏好，同时提高准确性。这种编辑反馈是自然产生的，适合用于提升代理与用户偏好的契合度，降低后续用户的编辑成本。为此，我们提出PRELUDE框架，它根据历史编辑数据推断用户的潜在偏好，并据此设计一个提示策略，引导未来的响应生成，避免了昂贵且难以扩展的微调过程，还能保持在其他任务上的性能。  此外，学习描述性的偏好有助于增强可解释性，用户可以查看和调整学习到的偏好。然而，用户偏好可能复杂多变，受情境影响，因此学习起来具有挑战性。为解决这一问题，我们提出CIPHER算法，它利用大型语言模型（LLM）根据用户编辑推断给定情境下的用户偏好。未来，CIPHER会从历史中的k个最接近的上下文中检索推断出的偏好，综合生成响应。我们在总结和电子邮件写作两个互动环境中使用GPT-4模拟用户进行评估，与直接使用用户编辑但不学习描述性偏好的算法，以及学习全局无上下文偏好的算法进行了比较。  在两项任务中，CIPHER都实现了最低的编辑距离成本，并且学习到的偏好与真实偏好显示出显著的相似性。**|
|**2024-04-22**|**A Survey on Self-Evolution of Large Language Models**|Zhengwei Tao et.al.|[2404.14387](http://arxiv.org/abs/2404.14387)|**[link](https://github.com/alibabaresearch/damo-convai)**|**## 概述  大型语言模型（LLMs）在众多领域和智能代理应用中取得了显著进步。然而，依赖人类或外部模型监督的现有LLMs在处理复杂任务和多样性增加时可能会遇到成本高昂和性能瓶颈的问题。为此，自我进化方法应运而生，这种策略允许LLMs自主获取、精炼并从自身生成的经验中学习，借鉴人类经验学习过程，有望推动LLMs向超级智能发展。本文全面综述了LLMs中的自我进化方法。首先，我们提出一个概念框架，将进化过程划分为迭代循环的四个阶段：经验获取、经验细化、更新和评估。其次，我们分类探讨LLMs和基于LLM的代理的进化目标，并对相关文献进行总结，提供每个模块的分类和见解。最后，我们指出了当前的挑战，并提出了未来研究方向，为加速自演进LLMs的发展提供关键洞见。**|
|**2024-04-21**|**A Survey on the Memory Mechanism of Large Language Model based Agents**|Zeyu Zhang et.al.|[2404.13501](http://arxiv.org/abs/2404.13501)|**[link](https://github.com/nuster1128/llm_agent_memory_survey)**|**随着大型语言模型（LLMs）在科研和工业界的广泛关注，基于LLMs的智能代理因其自我进化能力而备受瞩目，这对于解决需要长期复杂交互的现实问题至关重要。支持agent-environment交互的关键要素是代理的记忆机制。尽管已有众多有前景的记忆设计被提出，但这些研究分散在多篇论文中，缺乏全面的综述来系统性地总结和比较，未能提炼出通用且有效的设计模式以启发后续研究。为此，本论文旨在填补这一空白，我们提出一份关于LLM基代理记忆机制的全面调查。首先，我们将探讨记忆在LLM代理中的“是什么”以及“为什么需要”。然后，我们系统回顾了关于记忆模块的设计和评估方法的研究。此外，我们还会展示记忆模块在各种应用中扮演的重要角色。最后，我们会分析现有工作的局限，并指出重要的未来研究方向。为了跟踪该领域最新进展，我们创建了一个GitHub仓库：\url{https://github.com/nuster1128/LLM_Agent_Memory_Survey}。**|
|**2024-04-18**|**From Language Models to Practical Self-Improving Computer Agents**|Alex Sheng et.al.|[2404.11964](http://arxiv.org/abs/2404.11964)|null|我们提出了一种简单直接的方法，用于创建能够执行各种计算机任务的人工智能代理，并通过自我改进来发展工具和增强功能，以解决日益复杂的任务。鉴于大型语言模型（LLMs）已显示出从非参数增强中获益，近期的研究大量集中在开发软件，以赋予LLMs各种能力。我们建议，通过适当的提示工程，一个LLM代理可以系统地生成软件来增强自身，而不是依赖人类工程的静态软件开发。  我们通过一些案例研究展示了这一点：仅通过终端访问，我们引导LLM代理添加了检索、互联网搜索、网页导航和文本编辑功能。该代理有效地利用这些工具解决了问题，例如自动化软件开发和基于网络的任务。这种方法表明，通过连续提问和巧妙的提示设计，LLM能够自主扩展其功能，执行实际的计算机任务。|
|**2024-04-25**|**Automated Social Science: Language Models as Scientist and Subjects**|Benjamin S. Manning et.al.|[2404.11794](http://arxiv.org/abs/2404.11794)|null|我们提出了一种方法，利用大型语言模型（LLM）的最新进展，自动构建和测试社会科学假设。这种方法的关键在于使用结构因果模型。结构因果模型提供了一个陈述假设的语言、构建LLM基础代理的蓝图、实验设计以及数据分析计划。拟合后的结构因果模型可供预测或规划后续实验。我们通过几个场景进行了演示：谈判、保释听证会、求职面试和拍卖。在这些情况下，系统既提出了因果关系，也进行了检验，发现了一些证据，而有些则没有。我们证明，从这些社会互动模拟中获取的洞察并非仅通过直接询问LLM就能获得。当给定每个场景的建议结构因果模型时，LLM在预测估计效应的符号方面表现良好，但无法可靠地预测效应的大小。在拍卖实验中，模拟结果与拍卖理论的预测紧密吻合，但LLM直接提取的清算价格预测不准确。然而，如果模型能基于拟合的结构因果模型进行条件化，LLM的预测会大幅改进。简而言之，LLM知道的比它能立即表达的要多。|
|**2024-04-17**|**AgentKit: Flow Engineering with Graphs, not Coding**|Yue Wu et.al.|[2404.11483](http://arxiv.org/abs/2404.11483)|**[link](https://github.com/holmeswww/agentkit)**|**我们提出了一种直观的大型语言模型提示框架（AgentKit），旨在为多功能代理提供统一的方法。AgentKit通过简单的自然语言提示构建复杂的“思维过程”。其基本单元是节点，包含特定子任务的自然语言指令。用户可以像拼接乐高积木一样连接这些节点，从而明确设计出自然结构化的“思考流程”。例如，在撰写论文时，可能的步骤包括：1）确定核心信息，2）识别研究空白等。AgentKit的模块化特性使得高级功能如即兴的层次化规划、反思和从互动中学习变得可能。由于其直观且模拟人类思考过程的设计，即使没有编程经验的人也能创建和调整基础代理。定量实验显示，使用AgentKit设计的代理在WebShop和Crafter任务上实现了最先进的性能。这些成果表明AgentKit有潜力使LLM代理在更广泛的场景下高效且易于使用。相关代码已开源在GitHub：https://github.com/holmeswww/AgentKit。**|
|**2024-04-15**|**Memory Sharing for Large Language Model based Agents**|Hang Gao et.al.|[2404.09982](http://arxiv.org/abs/2404.09982)|**[link](https://github.com/ghupppp/memorysharingllm)**|**在人工智能领域，大型语言模型（LLMs）通过自然语言提示执行任务的能力是一个重大突破，它减少了对固定答案任务（如常识问题和是非查询）的重新训练或微调需求。然而，在处理开放性挑战如诗歌创作时，基于上下文学习的方法显示出局限，主要源于提供的示例全面性以及模型理解问题内容的能力不足，导致输出往往与预期结果大相径庭。针对这一差距，我们的研究提出了Memory-Sharing（MS）框架，这是一种针对LLM多代理的实时记忆存储和检索系统，旨在增强基于上下文的学习过程。每个“记忆”单元记录了提出的查询及其来自LLM代理的即时响应，从多个类似代理中聚合这些记忆，形成所有代理共享的丰富记忆池。MS框架不仅帮助代理找到特定任务的相关示例，还评估其记忆的潜在利用价值，供其他代理未来应用。在三个不同领域的实证验证显示，MS框架显著提高了代理处理开放性问题的表现。此外，我们还讨论了哪种记忆池和检索策略能更好地支持代理，为MS的未来发展提供了方向。代码和数据可在：https://github.com/GHupppp/MemorySharingLLM 获取。**|
|**2024-05-10**|**Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation**|Ruixin Yang et.al.|[2404.09127](http://arxiv.org/abs/2404.09127)|**[link](https://github.com/minnesotanlp/collaborative-calibration)**|**### 背景  当前的大规模语言模型（LLMs）在不确定性估计方面面临挑战，它们通常校准不良且过度自信，特别是在基于人类反馈的强化学习（RLHF）中。人类的决策和信心不仅源于内在信念，还能通过日常观察进行调整，而现有LLM的校准方法主要关注单个模型的信心估计，未能充分利用“集体智慧”：多个LLM之间的协作表达能力，这可以集体提高准确性和校准。本研究中，我们提出了一种无训练后处理的校准策略——协作校准（Collaborative Calibration），它利用多代理工具增强的LLMs在模拟的群体讨论过程中，共同提升校准能力和推理合理性。  ### 任务  我们在生成式问答任务上展示了协作校准的有效性，覆盖了多个领域，证明了它在整合集体校准后的信心评估和提升模型预测可靠性方面的潜力。**|
|**2024-04-13**|**CuriousLLM: Elevating Multi-Document QA with Reasoning-Infused Knowledge Graph Prompting**|Zukang Yang et.al.|[2404.09077](http://arxiv.org/abs/2404.09077)|**[link](https://github.com/zukangy/kgp-curiousllm)**|**在问答（QA）领域，大型语言模型（LLMs）与外部数据库的融合取得了显著成效。然而，这些方法在处理复杂推理任务时往往力有不逮。为此，我们对一种名为知识图谱提示（KGP）的创新方法进行了优化，该方法结合知识图谱和基于LLM的代理以提升推理和搜索精度。然而，原始的KGP框架需要昂贵的大规模数据微调，并且仍存在LLM的错误推断问题。因此，我们提出了一种融入推理能力的LLM代理，它模仿人类的好奇心，通过提问来更有效地导航搜索过程。这个简单的改进显著提高了LLM在QA任务中的性能，同时避免了初始KGP框架的高成本和延迟。我们的目标是进一步发展这种方法，最终实现更精确、更快捷且成本效益更高的QA解决方案。**|
|**2024-04-13**|**Do LLMs Play Dice? Exploring Probability Distribution Sampling in Large Language Models for Behavioral Simulation**|Jia Gu et.al.|[2404.09043](http://arxiv.org/abs/2404.09043)|null|随着大型语言模型（LLMs）的飞速发展及其在处理复杂语言任务中的出色表现，越来越多的研究尝试利用LLMs模拟人类的行为决策过程，通常这些过程被表示为马尔可夫决策过程（MDPs）。在这个框架中，动作遵循特定的概率分布，并需要迭代采样。这促使我们探究LLM代理理解概率分布的能力，以通过概率采样指导行为决策并生成行为序列。我们将问题分为两个主要方面：一是已知精确概率分布的模拟，二是模糊概率分布的序列生成。  在已知概率分布的情况下，代理需要根据问题描述提供概率分布的类型和参数，然后给出采样序列。然而，我们的研究显示，LLM代理在这方面的性能不佳，但通过编程工具可以一定程度上提高采样成功率。而在实际情境中，概率分布往往不明确。因此，我们在第二部分让代理调整在线社交网络中的活跃度，并分析行动频率。结果表明，即使借助编程工具，LLM代理依然无法有效地采样概率分布。这意味着在直接将LLM作为模拟人类行为的代理应用之前，还需要谨慎对待。|
|**2024-04-12**|**Strategic Interactions between Large Language Models-based Agents in Beauty Contests**|Siting Lu et.al.|[2404.08492](http://arxiv.org/abs/2404.08492)|null|随着大型语言模型（LLMs）的广泛应用，它们在博弈论框架下的游戏行为理解潜力日益显现。本研究聚焦于通过模拟分析不同类型LLM驱动的代理在经典 Beauty Contest 游戏中的策略互动。借鉴人类实验，我们对LLM代理的策略层次进行类似的评估，发现它们展现出从零级到一级的不同程度推理能力，并在重复游戏中表现出行动趋同。此外，我还探讨了不同类型的代理群体构成如何影响战略行为：高比例的固定策略对手能促进LLM代理的收敛，而混合环境中不同相对策略水平的代理共存会加速所有代理的收敛。更智能的代理可能获得更高的平均收益，但这是以较低智能代理的牺牲为代价的。这些结果不仅揭示了在特定情景下模拟代理的结局，还为理解算法之间的战略互动提供了重要启示。|
|**2024-04-17**|**LLM Agents can Autonomously Exploit One-day Vulnerabilities**|Richard Fang et.al.|[2404.08144](http://arxiv.org/abs/2404.08144)|null|随着大语言模型（LLMs）的威力日益增强，其在良性和恶意用途上的应用也日益广泛。研究人员开始关注它们利用网络安全漏洞的能力。近期的研究探讨了LLMs自主破解网站的可能性，但这些研究主要集中在简单的漏洞上。本工作揭示，LLMs能够自主利用现实世界系统中的单日漏洞。我们收集了一组包含15个被CVE描述为“关键严重性”的一天期漏洞数据。当提供CVE描述时，GPT-4模型能成功利用87%的漏洞，相比之下，其他测试模型（如GPT-3.5、开源LLMs和开源漏洞扫描器ZAP和Metasploit）的表现均为0%。然而，我们的GPT-4模型在没有描述的情况下效率大减，仅能利用7%的漏洞。这些发现对大规模部署高能力LLMs提出了质疑。|
|**2024-04-11**|**WESE: Weak Exploration to Strong Exploitation for LLM Agents**|Xu Huang et.al.|[2404.07456](http://arxiv.org/abs/2404.07456)|null|近期，大型语言模型（LLMs）显示出作为智能代理的强大潜力。然而，现有的研究主要集中在通过精心设计的提示工程或任务特定的微调来提升模型的推理或决策能力，忽视了探索与利用的过程。在处理开放世界交互环境中的复杂任务时，这些方法存在局限性。首先，由于缺乏对环境的全局信息，模型倾向于做出贪婪决策，导致解决方案不理想。另一方面，从环境中获取的无关信息不仅引入噪声，还增加了额外的成本。  为此，本文提出了一种新颖的方法——弱探索强化强利用（Weak Exploration to Strong Exploitation，WESE），旨在增强LLM在解决开放世界交互任务中的表现。具体来说，WESE将探索和利用过程解耦，使用成本效益高的“弱”代理执行探索任务，以获取全局知识。随后，我们引入基于知识图谱的策略来存储这些知识，并提取与任务相关的关键信息，从而提升“强”代理在成功率和效率上的性能。我们的方法适用于各种任务，并在四个互动基准测试中显著提高了成功率和效率。|
|**2024-04-10**|**GoEX: Perspectives and Designs Towards a Runtime for Autonomous LLM Applications**|Shishir G. Patil et.al.|[2404.06921](http://arxiv.org/abs/2404.06921)|**[link](https://github.com/ShishirPatil/gorilla)**|**随着大型语言模型（LLMs）的发展，它们不再仅仅是对话系统中的信息提供者，而是开始积极参与到与实际应用和服务的互动中。如今，人类在将LLM生成的输出（如代码、函数或操作）投入现实世界执行前，需要验证其正确性和适用性，这带来了挑战，因为代码理解被广泛认为非常困难。本文研究了人类如何能有效与LLMs协作、委派和监督，特别是在未来。我们主张，在许多情况下，对提出的行动进行“事后验证”（在看到输出后确认其正确性）比之前的“事前验证”更为容易。实现这一目标的核心理念是集成直观的撤销功能，并为LLM生成的动作设定损害约束，作为降低相关风险的有效策略。通过这种方式，人类可以撤销LLM输出的影响，或者确信潜在风险是有限的。我们认为这对于实现LLMs与应用和服务在有限的人类监督下交互至关重要。我们描述了开源运行时Gorilla Execution Engine（GoEX）的设计和实现，该运行时用于执行LLM动作，并提出了一些开放的研究问题，旨在推动LLMs与应用之间以最小的人工干预进行交互。GoEX的源代码已发布在https://github.com/ShishirPatil/gorilla/。**|
|**2024-04-09**|**AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents**|Luca Gioacchini et.al.|[2404.06411](http://arxiv.org/abs/2404.06411)|**[link](https://github.com/nec-research/agentquest)**|**随着大型语言模型（LLMs）的进展，人们追求能够解决复杂、多步骤推理任务的LLM代理。然而，现有的基准往往局限且只关注整体任务成功率。为了解决这些问题，我们提出了AgentQuest框架，它具有以下特点：（i）benchmark和评估指标模块化且易于扩展，通过文档齐全、易用的API；（ii）我们提供了两种新的评估指标，能够在解决任务时可靠地追踪LLM代理的进步。我们通过两个示例展示了这些指标的实用性，通过识别常见失败点并优化代理架构，显著提高了性能。我们希望与研究界共同扩展AgentQuest，并已将其开源在https://github.com/nec-research/agentquest。**|
|**2024-04-15**|**AutoCodeRover: Autonomous Program Improvement**|Yuntong Zhang et.al.|[2404.05427](http://arxiv.org/abs/2404.05427)|**[link](https://github.com/nus-apr/auto-code-rover)**|**在过去几十年里，研究人员在自动化软件开发过程中取得了显著进展，尤其是大型语言模型（LLMs）的应用极大地推动了编程辅助的自动化。然而，软件工程并不仅仅是编码，还包括维护（如修复bug）和演化（如添加功能）等程序改进过程。本文提出了一种自动解决GitHub问题的方法，旨在实现程序自主改进。我们的方法称为AutoCodeRover，它结合了LLMs与高级代码搜索能力，最终生成程序修改或补丁。与AI研究者和从业者近期关注的仅文件级别的软件项目不同，我们的工作侧重于程序表示（抽象语法树），利用类/方法的程序结构来增强LLM对问题根本原因的理解，并通过迭代搜索提供上下文。当测试套件可用时，谱系基线故障定位技术进一步精确了上下文。  在SWE-bench-lite，一个包含300个真实GitHub问题的数据集上，AutoCodeRover的解决方案效果提升，解决了约22-23%的问题。对于全量的SWE-bench，包含2294个GitHub问题，AutoCodeRover解决了大约16%的问题，这比最近报道的来自Cognition Labs的AI软件工程师Devin的表现还要高，而且时间消耗与Devin相当。我们相信，我们的工作流程能够推动自主软件工程的发展，未来LLM自动生成的代码可以被自动地进行优化和改进。**|
|**2024-04-08**|**Long-horizon Locomotion and Manipulation on a Quadrupedal Robot with Large Language Models**|Yutao Ouyang et.al.|[2404.05291](http://arxiv.org/abs/2404.05291)|null|我们提出了一种基于大型语言模型（LLM）的系统，旨在提升四足机器人的问题解决能力，使其能够处理超越短期动作的长期任务。对于四足机器人来说，长期任务极具挑战性，因为它们需要对任务的语义有高层理解，并具备广泛的运动和操纵技能以与环境互动。我们的系统构建了一个高层推理层，利用大型语言模型，从任务描述中生成混合离散-连续的计划，作为机器人代码。它包括多个LLM代理：一个用于构思计划的语义规划器、一个参数计算器，用于预测计划中的参数，以及一个代码生成器，将计划转换为可执行的机器人代码。  在低层次，我们采用强化学习来训练一套运动规划和控制技能，以增强四足机器人的灵活性，使其能进行丰富环境交互。我们在难以用单一技能完成的长期任务上测试了我们的系统。模拟实验和真实世界实验表明，它成功地制定了多步骤策略，并展现出非平凡的行为，例如制作工具或向人类寻求帮助。|
|**2024-04-06**|**Autonomous Artificial Intelligence Agents for Clinical Decision Making in Oncology**|Dyke Ferber et.al.|[2404.04667](http://arxiv.org/abs/2404.04667)|null|多模态人工智能系统有望通过解析各类医学数据提升临床决策。然而，这些模型在各医学领域的效能尚不明朗，每个领域都有其独特挑战。本文提出了一种利用大型语言模型（LLMs）作为核心推理引擎的新型多模态医疗AI方法。此引擎自主协调并部署一系列专门的医疗AI工具，如文本解读、放射学和病理图像分析、基因数据处理、网络搜索以及医疗指南文档检索。我们在一系列临床肿瘤学场景中验证了该系统，这些场景模拟了典型的患者护理流程。结果显示，系统在选择恰当工具（97%）、得出正确结论（93.6%）、提供完整（94%）和有益（89.2%）治疗建议，以及根据指令引用相关文献（82.5%）方面表现出高能力。这表明LLMs能够有效地规划和执行领域特定模型，以获取或合成新信息，从而充当个性化临床助手。此外，这种架构简化了监管合规性，因为每个组件工具可以单独验证和审批。我们相信，这项工作为医疗领域的更先进LLM代理提供了概念验证。|
|**2024-04-05**|**Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents**|Harsh Kohli et.al.|[2404.04237](http://arxiv.org/abs/2404.04237)|null|大型语言模型（LLMs）的快速进步使其在标准基准测试中频频超越人类表现，推动了众多下游应用的发展，如基于LLMs的代理。然而，这些模型在看似简单的任务中意外地表现不佳，这强调了对更全面和多样化的评估框架的需求，以衡量它们的实际能力。为此，我们聚焦于组合性和条件推理——人类认知的基石，并提出GroundCocoa，这是一个与航班预订这一现实问题相连接的词汇丰富的基准。我们的任务是将用户的详细偏好与以多选形式提供的可用航班选项进行匹配。结果显示，包括最先进的GPT-4 Turbo在内的当前最佳模型，在经过高级提示后，准确率仍不超过67%，显示出显著的性能差距。|
|**2024-04-02**|**Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization**|Yoichi Ishibashi et.al.|[2404.02183](http://arxiv.org/abs/2404.02183)|**[link](https://github.com/tsukushiai/self-organized-agent)**|**## 背景  随着大型语言模型（LLM）代理的最新进展，自动化软件开发的未来正逐渐显现。然而，现有的单代理方法在生成和优化大规模、复杂的代码库时面临上下文长度限制的问题。为解决这一挑战，我们提出了一种新颖的多代理框架——自组织多Agent体系（SoA）。SoA是一个可扩展且高效的多代理系统，它允许独立地生成和修改代码组件，并协同构建整个代码库。SoA的一个关键特性是根据问题复杂性自动增加代理，实现动态可扩展性。这样，整体代码量可以根据代理数量无限增长，而每个代理管理的代码量保持恒定。  我们在HumanEval基准上评估了SoA，并发现与单代理系统相比，SoA中的每个代理处理的代码量明显减少，但总体生成的代码量显著增加。此外，SoA在Pass@1准确率方面比强大的单代理基线提高了5%。**|
|**2024-04-02**|**Helmsman of the Masses? Evaluate the Opinion Leadership of Large Language Models in the Werewolf Game**|Silin Du et.al.|[2404.01602](http://arxiv.org/abs/2404.01602)|**[link](https://github.com/doslim/evaluate-the-opinion-leadership-of-llms)**|**大型语言模型在社交推理游戏中展现出显著的策略行为，但对它们作为意见领袖的重要性关注不足，这对于多Agent和人机交互场景的实际应用至关重要。意见领袖是指在一个社会群体中对他人信念和行为有显著影响的个体。本研究使用“狼人杀”游戏作为模拟平台，探讨语言模型在扮演Sheriff（治安官）角色时的意见领导能力。Sheriff负责总结论点并提出决策建议，因此它代表了意见领袖的一个可信代理。我们构建了一个整合Sheriff角色的框架，并基于意见领袖的关键特性提出了两个评估指标：第一个衡量意见领袖的可靠性，第二个考察其对其他玩家决策的影响。  我们进行了大量实验，评估不同规模的语言模型，并创建了“狼人杀”问题回答数据集（WWQA），以测试和提升模型对游戏规则的理解。此外，还包含了人类参与者进行进一步分析。研究结果表明，“狼人杀”游戏是一个有效评估语言模型意见领导力的试验场，但目前仅有少数语言模型具备这种能力。**|
|**2024-04-15**|**CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs**|Jingzhe Shi et.al.|[2404.01343](http://arxiv.org/abs/2404.01343)|**[link](https://github.com/jingzheshi/chops)**|**随着企业和软件平台越来越多地采用大型语言模型（如GPT-3.5、GPT-4、GLM-3和LLaMa-2）提供聊天辅助或客户服务推理，现有的基于LLM的客户服务模型在与客户资料集成和执行实际操作方面存在局限。它们倾向于强调多样性而非精确性和错误避免，这对于现实世界的客户服务场景并不理想。因此，我们提出了一种名为CHOPS（结合客户资料的聊天助手）的LLM代理，旨在：（1）高效利用现有数据库或系统查询用户信息，或遵循既定指南与系统交互；（2）提供准确合理的响应并执行系统内的必要操作，同时避免有害操作；（3）通过结合小型和大型LLM以实现性能满意且成本合理的推理。  我们开发了一个实用的数据集，称为CPHOS-dataset，它包括一个数据库、指导文件以及来自CPHOS平台的模拟物理奥林匹克组织服务的问答对。CPHOS是一个面向高中教师和学生的在线平台。我们通过使用CPHOS-dataset进行了广泛的实验，验证了CHOPS架构的性能，目标是展示LLM如何提升或替代人工客户服务。关于我们的提案架构和数据集的代码可在此处获取：<https://github.com/JingzheShi/CHOPS>。**|
|**2024-03-31**|**DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language Model**|Lirui Zhao et.al.|[2404.01342](http://arxiv.org/abs/2404.01342)|**[link](https://github.com/opengvlab/diffagent)**|**文本到图像（T2I）生成模型近年来备受瞩目，在学术研究和实际应用中大放异彩。例如，Civitai平台，一个T2I创新的聚集地，目前汇集了74,492种独特的模型，这带来了选择最合适的模型和参数的艰巨任务，通常需要多次试验。借鉴大型语言模型（LLMs）工具使用研究的思路，我们推出了DiffAgent，这是一个通过API调用来快速筛选准确选项的LLM代理。DiffAgent采用了一种新颖的两阶段训练框架，称为SFTA，使其能够根据人类偏好精确地将T2I API的响应与用户输入对齐。为了训练和评估DiffAgent的能力，我们构建了DABench，这是一个全面的数据库，涵盖了社区中的各种T2I API。实验结果显示，DiffAgent不仅在选择适当的T2I API方面表现出色，还验证了SFTA训练框架的有效性。相关代码已可在https://github.com/OpenGVLab/DiffAgent获取。**|
|**2024-03-31**|**Algorithmic Collusion by Large Language Models**|Sara Fish et.al.|[2404.00806](http://arxiv.org/abs/2404.00806)|null|随着算法定价的兴起，人们担忧算法间的合谋问题。我们通过实验使用基于大型语言模型（LLMs）的定价代理，特别是GPT-4，进行了探究。研究发现：(1) LLM驱动的定价机制在定价任务上表现出色；(2) 在寡头竞争环境中，LLM定价代理会自发地进行合谋，从而损害消费者利益；(3) 对LLM指令（“提示”）看似微小的变化可能加剧这种合作行为。这些结果同样适用于拍卖场景。我们的研究结果强调了对算法定价进行反垄断监管的必要性，并揭示了针对LLM定价代理特有的监管挑战。|
|**2024-03-31**|**"My agent understands me better": Integrating Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents**|Yuki Hou et.al.|[2404.00573](http://arxiv.org/abs/2404.00573)|**[link](https://github.com/tamoharu/Agent-Memory-CHI24)**|在这个研究中，我们提出了一种创新的人类记忆架构，旨在提升基于大型语言模型的对话代理的认知能力。我们的设计使得这些代理能自主检索生成响应所需的必要记忆，从而解决LLMs在时间认知上的局限。我们借鉴了人类的记忆线索召回机制作为触发点，以实现精确且高效的回忆。此外，我们开发了一个数学模型，动态量化记忆巩固过程，考虑了诸如上下文相关性、时间流逝和回忆频率等因素。代理会从用户的交互历史中存储记忆，这些记忆被封装在数据库中，每个记忆都包含了内容和时间关联的语境。这样，通过类似人类识别和回忆过往经历的方式，系统能够战略性地存储记忆，并理解它们对用户在时间线上的重要性。|

<p align=right>(<a href=#updated-on-20240724>back to top</a>)</p>

## llm

|Publish Date|Title|Authors|PDF|Code|abstract|
|---|---|---|---|---|---|
|**2024-07-23**|**Can Large Language Models Automatically Jailbreak GPT-4V?**|Yuanwei Wu et.al.|[2407.16686](http://arxiv.org/abs/2407.16686)|null|GPT-4V因其卓越的多模态信息整合与处理能力而备受瞩目，同时其人脸识别功能引发了关于隐私泄露的新安全担忧。尽管研究者通过RLHF或预处理过滤器等方法致力于安全对齐，但仍存在被利用的漏洞风险。在本研究中，我们引入了一种创新的自动越狱技术——AutoJailbreak，该技术受到提示优化的启发。我们利用大型语言模型(LLM)进行红队测试，以精炼越狱提示，并采用从弱到强的上下文学习提示来提高效率。此外，我们提出了一种有效的搜索方法，结合了早期停止策略，以减少优化时间和代币消耗。实验结果表明，AutoJailbreak在攻击成功率(ASR)上远超传统方法，超过95.3%。这一研究揭示了强化GPT-4V安全性的必要性，同时也突显了大型语言模型可能被用于破坏GPT-4V完整性的潜在威胁。|
|**2024-07-23**|**RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent**|Huiyu Xu et.al.|[2407.16667](http://arxiv.org/abs/2407.16667)|null|最近，诸如GPT-4等先进的大型语言模型（LLM）已被整合到许多现实应用中，如Code Copilot，这极大地扩展了LLM的攻击面，使其面临各种威胁。其中，通过越狱提示诱导有毒响应的越狱攻击引发了严重的安全担忧。为了识别这些威胁，越来越多的红队方法通过构造越狱提示来模拟潜在的对抗场景，测试目标LLM。然而，现有的红队方法并未考虑到不同场景下LLM的独特脆弱性，使得调整越狱提示以发现情境特定的漏洞变得困难。同时，这些方法仅限于使用少量变异操作对越狱模板进行优化，缺乏适应不同场景的自动化和可扩展性。为了实现情境感知和高效的红队测试，我们将现有攻击抽象并建模为一个连贯的概念，即“越狱策略”，并提出一个多代理LLM系统，名为RedAgent，该系统利用这些策略生成情境感知的越狱提示。通过在额外的记忆缓冲区中自我反思情境反馈，RedAgent持续学习如何在特定情境中利用这些策略实现有效的越狱。广泛的实验表明，我们的系统能够在仅五次查询的情况下越狱大多数黑盒LLM，效率是现有红队方法的两倍。此外，RedAgent能够更高效地越狱定制的LLM应用。通过对GPT上的应用生成情境感知的越狱提示，我们仅用两次查询就发现了60个这些现实应用的严重漏洞。我们已报告所有发现的问题，并与OpenAI和Meta沟通进行漏洞修复。|
|**2024-07-23**|**Course-Correction: Safety Alignment Using Synthetic Preferences**|Rongwu Xu et.al.|[2407.16637](http://arxiv.org/abs/2407.16637)|null|大型语言模型(LLM)生成有害内容的风险成为一个关键问题。本文系统地研究了评估和提升LLM执行“航向修正”任务的能力，即模型能自主避免生成有害内容。首先，我们引入了\textsc{C $^2$-Eval}基准测试以进行量化评估，并分析了10种流行的LLM，揭示了当前安全调优的LLM在航向修正方面的不同水平。为了改进，我们提出使用偏好学习对LLM进行微调，强调及时航向修正的偏好。通过自动化流程，我们创建了\textsc{C$^2$-Syn}，一个包含75万对偏好数据的合成数据集，通过数据驱动的偏好学习教授模型及时航向修正的概念。实验表明，在\textsc{Llama2-Chat 7B}和\textsc{Qwen2 7B}两种LLM上，我们的方法有效提升了航向修正技能，且不影响通用性能。此外，它还显著提高了LLM的安全性，特别是抵抗越狱攻击的能力。  请注意，由于Markdown语法限制，原文中的"\ie"和"\eg"等LaTeX命令无法正常显示，它们分别代表“即”和“例如”。同时，部分数学符号如"C$^2$ "和"LLM"等在中文语境下直接翻译可能造成理解障碍，故保留原文表述。在翻译过程中，已尽可能保持专业术语的一致性和准确性。|
|**2024-07-23**|**Lawma: The Power of Specialization for Legal Tasks**|Ricardo Dominguez-Olmedo et.al.|[2407.16615](http://arxiv.org/abs/2407.16615)|null|法律文本的注释和分类是实证法律研究的核心组成部分。传统上，这些任务通常委托给受过训练的研究助理。受到语言模型进步的推动，实证法律学者越来越多地转向提示商业模型，希望这能减轻人工注释的巨大成本。尽管使用日益增多，但我们对于如何最好地利用大型语言模型进行法律任务的理解仍然有限。我们对260个法律文本分类任务进行了全面研究，其中几乎所有的任务对机器学习社区来说都是全新的。从GPT-4作为基准开始，我们展示了它在零样本准确性方面具有非平凡但高度变化的表现，其性能有时可能不足以满足法律工作的要求。然后，我们证明了一个轻度微调的Llama 3模型在几乎所有任务上的表现都远超GPT-4，通常高出两位数的百分点。我们发现，较大的模型对微调的响应优于较小的模型。几十到几百个示例就足以达到高分类精度。值得注意的是，我们可以在相对较小的精度损失下，用一个单一的模型同时微调所有260个任务，相对于为每个任务单独拥有一个模型。我们的工作指出了一个可行的替代方案，即主导的提示商业模型的做法。对于具有可用标记数据的具体法律任务，研究人员使用微调的开源模型会更有效。|
|**2024-07-23**|**Shared Imagination: LLMs Hallucinate Alike**|Yilun Zhou et.al.|[2407.16604](http://arxiv.org/abs/2407.16604)|null|尽管大型语言模型(LLM)近期得到了广泛发展，但它们的训练方法——包括模型架构、预训练数据和优化算法——往往非常相似。这自然引发了关于这些模型之间相似性的疑问。在本文中，我们提出了一种新的设置，即“想象问答”(IQA)，以更深入地理解模型的相似性。在IQA中，我们让一个模型生成完全虚构的问题（例如，关于物理学中完全杜撰的概念），然后提示另一个模型来回答。令人惊讶的是，尽管这些问题完全虚构，所有模型却能够以惊人的成功率回答彼此的问题，这表明在进行此类幻觉时，这些模型在某种程度上共享了一个“想象空间”。我们对这一现象进行了一系列的探究，并讨论了它对模型同质性、幻觉以及计算创造力的影响。|
|**2024-07-23**|**Exploring Automatic Cryptographic API Misuse Detection in the Era of LLMs**|Yifan Xia et.al.|[2407.16576](http://arxiv.org/abs/2407.16576)|null|尽管自动检测加密API误用的技术已取得显著进展，但在处理复杂目标时，其精确度因依赖人工定义的模式而下降。大型语言模型（LLM）以其卓越的情境理解能力，为解决现有局限提供了希望之光。然而，在这一安全关键领域应用LLM面临挑战，尤其是由于LLM固有的随机性和广为人知的幻觉问题导致的不可靠性。为了探究LLM在分析中的不可靠程度及其潜在解决方案，本文提出了一套系统评估框架，用于评估LLM在检测加密误用方面的表现，该框架基于一个全面的数据集，涵盖了手工构建的样本和实际项目。我们对11,940份由LLM生成的报告进行了深入分析，发现LLM内在的不稳定性可能导致超过一半的报告出现假阳性结果。然而，我们展示了通过限定问题范围并结合LLM的自我修正能力，可以显著提升检测的可靠性。优化后的方案实现了近90%的惊人检测率，超越了传统方法，并在既定基准中发现了先前未知的误用情况。此外，我们识别出持续影响LLM可靠性的失败模式，包括加密知识不足和代码语义误解。基于这些见解，我们开发了一种基于LLM的工作流程来检查开源仓库，进而发现了63个真实世界中的加密误用实例。其中，46个已得到开发者社区的认可，23个正在处理中，6个已得到解决。鉴于开发者的反馈，我们为未来的研究和基于LLM的安全工具开发提供了建议。|
|**2024-07-23**|**Retrieve, Generate, Evaluate: A Case Study for Medical Paraphrases Generation with Small Language Models**|Ioana Buhnila et.al.|[2407.16565](http://arxiv.org/abs/2407.16565)|null|近期，大型语言模型（LLMs）对大众的普及导致了这些模型在医疗建议领域的无监管应用。通过LLMs进行语言生成存在两个主要问题：首先，它们容易产生幻觉，因此在任何医疗用途上都需要科学和事实依据；其次，由于其庞大的模型尺寸，LLMs对计算资源构成了巨大挑战。在此工作中，我们引入了pRAGe，一个用于基于小型语言模型（SLM）的医疗领域法语释义生成与评估的检索增强生成管道。我们研究了SLM在医疗释义生成中的有效性以及外部知识库的影响。  请注意，以上是对给定英文摘要的中文翻译。|
|**2024-07-23**|**Patched RTC: evaluating LLMs for diverse software development tasks**|Asankhaya Sharma et.al.|[2407.16557](http://arxiv.org/abs/2407.16557)|null|本文提出了一种名为“修补的往返正确性”（Patched RTC）的新颖评估技术，该技术适用于大型语言模型（LLM）在多样化的软件开发任务中的应用，特别关注“外循环”活动，如修复错误、代码审查和文档更新。Patched RTC扩展了原有的往返正确性方法，使其能够与任何LLM和下游任务兼容，提供了一个自我评估框架，无需人工干预即可衡量模型响应的一致性和稳健性。研究显示，Patched RTC得分与特定任务准确性指标之间存在相关性，将其作为开放领域任务评估的LLM-as-Judge范式的替代方案。我们通过一个开源框架patchwork实现了Patched RTC，允许在各种修补流程中进行透明的评估。实验对比了GPT-3.5和GPT-4模型在不同软件开发任务上的表现，证明了Patched RTC能有效区分模型性能和任务难度。此外，本文还探讨了一致性提示对提高模型准确性的影响力，表明Patched RTC可以指导复杂软件开发工作流中的提示优化和模型选择。|
|**2024-07-23**|**MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues**|Liyun Zhang et.al.|[2407.16552](http://arxiv.org/abs/2407.16552)|null|多模态大型语言模型(MLLMs)在视频中整合视觉、听觉和语言环境的多模态线索以识别人类情绪状态方面展现了显著的能力。然而，现有方法忽视了捕捉面部局部特征的时间动态微表情，并未利用视频中语境依赖的语音感知时间片段，这在一定程度上限制了其预期效果。在此工作中，我们提出了MicroEmo，一种时间敏感的MLLM，旨在关注面部局部微表情动态和语境依赖的语音感知视频片段。我们的模型包含两个关键架构贡献：(1)全局-局部注意力视觉编码器，它将全球帧级时间戳绑定图像特征与面部局部特征的时间动态微表情相结合；(2)语境感知视频Q-Former，通过为每个语音片段和整个视频生成视觉令牌序列，然后将它们结合，从而捕捉多尺度和语境依赖性。初步的定性实验表明，在一项新的可解释多模态情感识别(EMER)任务中，该任务利用多模态和多方面的线索以开放词汇(OV)方式预测情绪，MicroEmo相比最新方法展示了其有效性。|
|**2024-07-23**|**AMONGAGENTS: Evaluating Large Language Models in the Interactive Text-Based Social Deduction Game**|Yizhou Chi et.al.|[2407.16521](http://arxiv.org/abs/2407.16521)|null|战略社交推理游戏作为评估语言模型理解与推断能力的宝贵试验场，对社会学、人工智能及策略游戏领域提供了关键洞见。本文专注于在模拟环境中构建人类行为的代理，以《Among Us》为工具研究拟人化行为模拟。本研究引入了一个基于文本的游戏环境，名为“AmongAgent”，其动态与《Among Us》相似。玩家扮演太空船上的船员，任务是识别并消除破坏飞船和清除船员的冒充者。在这个环境中，分析了模拟语言代理的行为。实验涉及多种游戏序列，包括不同配置的船员和冒充者性格原型。我们的工作证明，最先进的大型语言模型(LLMs)能够有效理解游戏规则，并根据当前情境做出决策。这项工作旨在促进LLMs在具有不完整信息和复杂动作空间的目标导向游戏中的进一步探索，因为这些环境为评估语言模型在社交驱动场景下的表现提供了宝贵机会。|
|**2024-07-22**|**AutoAD-Zero: A Training-Free Framework for Zero-Shot Audio Description**|Junyu Xie et.al.|[2407.15850](http://arxiv.org/abs/2407.15850)|**[link](https://github.com/Jyxarthur/AutoAD-Zero)**|**我们的目标是以无需训练的方式生成电影和电视系列的音频描述（AD）。我们利用现成的视觉语言模型（VLMs）和大型语言模型（LLMs）的能力，并为这一任务开发了视觉和文本提示策略。我们的贡献有三点：(i) 我们证明了如果通过视觉指示直接用角色信息提示VLM，它可以在不需要任何微调的情况下成功命名和指代角色；(ii) 我们开发了一个两阶段过程来生成AD，第一阶段要求VLM全面描述视频，第二阶段使用LLM将密集的文本信息总结为一个简洁的AD句子；(iii) 我们制定了一个新的电视音频描述数据集。我们的方法，名为AutoAD-Zero，在电影和电视系列的AD生成方面展示了出色的表现（甚至与一些在真实AD上微调的模型竞争），在CRITIC得分上达到了最先进的水平。**|
|**2024-07-22**|**LLMmap: Fingerprinting For Large Language Models**|Dario Pasquini et.al.|[2407.15847](http://arxiv.org/abs/2407.15847)|null|我们引入了LLMmap，这是一种针对集成大型语言模型（LLM）应用的第一代指纹识别攻击。LLMmap采用主动指纹识别方法，通过向应用发送精心设计的查询并分析响应来识别所使用的具体LLM模型。仅需8次交互，LLMmap就能以超过95%的准确率识别出LLM模型。更重要的是，LLMmap被设计为在不同的应用层面上具有鲁棒性，使其能够在各种系统提示、随机采样超参数下，甚至在复杂的生成框架如RAG或Chain-of-Thought中识别出LLM模型。|
|**2024-07-22**|**SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language Models**|Mingze Xu et.al.|[2407.15841](http://arxiv.org/abs/2407.15841)|null|我们提出了一种名为SlowFast-LLaVA（简称SF-LLaVA）的训练自由视频大型语言模型（LLM），它能够在不超出常用LLM的标记预算的情况下，同时捕捉详细的时空语义和长程时间上下文。这是通过采用视频LLM的双流SlowFast输入设计来有效聚合采样视频帧的特征实现的。具体而言，慢路径以低帧率提取特征，同时尽可能保留大量空间细节（例如，使用24x24个标记），而快路径则在高帧率下运行，但使用更大的空间池化步幅（例如，下采样6倍）来专注于运动线索。因此，这种设计使我们能够充分捕捉对理解视频细节有益的空间和时间特征。实验结果表明，SF-LLaVA在广泛的视频任务上超越了现有的训练自由方法。在某些基准测试上，它的性能与经过视频数据集微调的最先进的视频LLM相当，甚至更优。|
|**2024-07-22**|**MMInstruct: A High-Quality Multi-Modal Instruction Tuning Dataset with Extensive Diversity**|Yangzhou Liu et.al.|[2407.15838](http://arxiv.org/abs/2407.15838)|null|尽管视觉语言监督微调在提升视觉大模型（VLLM）性能方面展现出显著效果，但现有的视觉指令微调数据集仍存在以下局限：(1) 指令注释质量：即使是最先进的VLLM，生成的指令也可能存在不准确的问题，如幻觉。 (2) 指令与图像多样性：指令类型范围有限以及图像数据缺乏多样性可能影响模型生成多样化、更贴近真实场景输出的能力。为了解决这些问题，我们构建了一个高质量、多样化的视觉指令微调数据集MMInstruct，它包含了来自24个领域的97.3万条指令。该数据集包括四种指令类型：判断、多项选择、长篇视觉问答和短篇视觉问答。为了构建MMInstruct，我们提出了一种基于GPT-4V、GPT-3.5和人工校正的指令生成数据引擎。我们的指令生成引擎实现了半自动化、低成本、跨领域指令生成，成本仅为手动构建的六分之一。通过广泛的实验验证和消融实验，我们证明了MMInstruct能够显著提升VLLM的性能，例如，在MMInstruct上微调的模型在12项基准测试中的10项上达到了新的最先进水平。代码和数据将在https://github.com/yuecao0119/MMInstruct上提供。|
|**2024-07-22**|**dMel: Speech Tokenization made Simple**|He Bai et.al.|[2407.15835](http://arxiv.org/abs/2407.15835)|null|大型语言模型通过在海量文本数据上进行自监督预训练，彻底改变了自然语言处理领域。受此成功的启发，研究者们探索了复杂的语音分词方法来离散化连续的语音信号，以便将语言建模技术应用于语音数据。然而，现有的方法要么建模语义分词，可能丢失声学信息；要么建模声学分词，冒着失去语义信息的风险。同时拥有多种分词类型也使得架构复杂化，并需要额外的预训练。我们证明，将梅尔滤波器组通道离散化为离散强度档位，可以产生一种简单表示（dMel），其性能优于现有的所有语音分词方法。使用仅含解码器的Transformer架构进行语音-文本建模，我们全面评估了不同语音分词方法在语音识别(ASR)和语音合成(TTS)任务上的表现。实验结果表明，dMel在统一框架下实现了这两个任务的高性能，为高效且有效地联合建模语音和文本开辟了道路。|
|**2024-07-22**|**Accelerating Pre-training of Multimodal LLMs via Chain-of-Sight**|Ziyuan Huang et.al.|[2407.15819](http://arxiv.org/abs/2407.15819)|null|本文提出了Chain-of-Sight，一种加速多模态大语言模型(MLLMs)预训练的视觉语言桥梁模块。我们的方法采用了一系列的视觉重采样器，能够捕捉不同空间尺度下的视觉细节。这种架构不仅有效地利用了全局和局部的视觉上下文，而且还通过复合标记扩展策略促进了视觉标记的灵活扩展，允许在预训练后最多增加16倍的标记数量。因此，Chain-of-Sight在预训练阶段需要的视觉标记显著少于微调阶段。预训练阶段视觉标记的有意减少显著加速了预训练过程，将实际训练时间减少了约73%。在一系列视觉语言基准测试中的实证结果表明，通过Chain-of-Sight实现的预训练加速并没有牺牲性能，其表现与或超过了在整个训练过程中使用所有视觉标记的标准流程。进一步增加预训练阶段的视觉标记数量，可以带来更强的性能，在一系列基准测试中展现出与现有方法竞争的实力。|
|**2024-07-22**|**Extracting Structured Insights from Financial News: An Augmented LLM Driven Approach**|Rian Dolphin et.al.|[2407.15788](http://arxiv.org/abs/2407.15788)|null|财经新闻在金融领域的决策过程中扮演着至关重要的角色，然而，将此类信息高效地转化为结构化格式仍然充满挑战。本文提出了一种处理财经新闻的创新方法，利用大型语言模型（LLMs）克服了从非结构化财经新闻中提取结构化数据的先前限制。我们介绍了一个系统，该系统能够从原始新闻文章内容中提取相关公司股票代码，进行公司层面的情绪分析，并生成摘要，这一切都不依赖于预结构化的数据源。我们的方法结合了LLMs的生成能力、最近的提示技术以及一个使用定制字符串相似度方法的稳健验证框架。在包含5530篇财经新闻文章的数据集上的评估表明了我们方法的有效性，与现有数据提供商相比，90%的文章没有遗漏任何股票代码，而22%的文章有更多的相关股票代码。除了本文，该方法已在大规模实施，通过实时更新最新新闻的现场API端点提供处理后的数据。据我们所知，我们是首个提供基于新闻文章的粒度级、按公司划分的情绪分析的数据提供商，这增强了市场参与者可获得的信息深度。我们还以静态文件形式发布了5530篇已处理文章的评估数据集，希望这能促进更多利用财经新闻的研究。|
|**2024-07-22**|**MoRSE: Bridging the Gap in Cybersecurity Expertise with Retrieval Augmented Generation**|Marco Simoni et.al.|[2407.15748](http://arxiv.org/abs/2407.15748)|null|在这篇论文中，我们介绍了MoRSE（Mixture of RAGs Security Experts），这是首个专门用于网络安全领域的人工智能聊天机器人。MoRSE旨在提供全面且详尽的网络安全知识。它采用了两个RAG（检索增强生成）系统，设计用于从多维度的网络安全环境中检索和组织信息。MoRSE与传统的RAG系统不同，它使用并行检索器协同工作，以在不同的格式和结构中检索语义相关的信息。与依赖于参数化知识库的传统大型语言模型(LLMs)不同，MoRSE从非参数化知识库中检索与用户查询相关的文档，然后利用这些信息生成精确的答案。此外，MoRSE得益于其知识库的实时更新，能够在无需重新训练的情况下持续丰富知识。我们已经对MoRSE的有效性进行了评估，将其与其他最先进的LLMs进行了对比，对系统在600个特定于网络安全的问题上进行了评估。实验评估显示，与已知解决方案如GPT-4和Mixtral 7x8相比，答案的相关性和正确性提高了超过10%。|
|**2024-07-22**|**OMoS-QA: A Dataset for Cross-Lingual Extractive Question Answering in a German Migration Context**|Steffen Kleinle et.al.|[2407.15736](http://arxiv.org/abs/2407.15736)|null|在移居新国家时，人们很容易因需要获取有关财务支持、住房、教育、语言课程等信息而感到不知所措。如果搬迁是匆忙的，甚至是被迫的，那么对高质量答案的需求就更为迫切。官方移民顾问通常工作量过大，而在线系统可以引导新移民找到所需信息或合适的咨询服务。为此，我们提出了OMoS-QA，这是一个包含德语和英语问题与相关可信文档及人工注释答案的配对数据集，专门针对这一场景设计。问题通过一个开源大型语言模型自动生成，答案句子由高度一致的众包工作者选择。利用我们的数据，我们对5个预训练大型语言模型在德语和英语提取式问答任务上的表现进行了比较。在所有模型和两种语言中，我们发现选择答案句子时具有高精度和中低召回率，这种权衡有利于避免误导用户。即使当问题语言与文档语言不匹配时，这种性能仍然保持。然而，在识别给定上下文下的不可回答问题方面，两种语言之间存在较大差异。|
|**2024-07-22**|**TaskGen: A Task-Based, Memory-Infused Agentic Framework using StrictJSON**|John Chong Min Tan et.al.|[2407.15734](http://arxiv.org/abs/2407.15734)|null|TaskGen是一个开源的代理框架，它通过将任意任务分解为子任务，利用代理来解决这些任务。每个子任务被映射到一个装备函数或另一个代理来执行。为了减少冗长（从而减少令牌使用），TaskGen采用了StrictJSON，确保大型语言模型(LLM)的JSON输出，并具有额外功能，如类型检查和迭代错误修正。TaskGen的核心理念是基于需要了解的信息/记忆管理。我们对TaskGen在各种环境中的表现进行了实证评估，包括40x40动态迷宫导航（障碍物位置变化，100%解谜率）、TextWorld逃脱房间解决（密集奖励和详细目标，96%解谜率）、网络浏览（69%的动作成功）、解决MATH数据集（100个Level-5问题中71%的解谜率）、在NaturalQuestions数据集上的检索增强生成（F1分数为47.03%）。|
|**2024-07-19**|**Internal Consistency and Self-Feedback in Large Language Models: A Survey**|Xun Liang et.al.|[2407.14507](http://arxiv.org/abs/2407.14507)|**[link](https://github.com/iaar-shanghai/icsfsurvey)**|**大型语言模型(LLM)虽然预期能准确响应，但常表现出推理缺陷或生成幻觉内容。为解决这些问题，已发起了一系列以“自我”为前缀的研究，如自我一致性、自我提升和自我精炼，它们的共同点是涉及LLM评估和更新自身以缓解问题。然而，这些努力在总结方面缺乏统一视角，现有综述主要侧重于分类而未深入探讨这些工作的动机。本文总结了一个理论框架，称为内部一致性，它为缺乏推理和出现幻觉等现象提供了统一解释。内部一致性基于采样方法，评估LLM的潜在层、解码层和响应层之间的连贯性。在此基础上，我们提出了一种简化而有效的理论框架，名为自我反馈，用于挖掘内部一致性。自我反馈框架由两个模块组成：自我评估和自我更新。该框架已在多项研究中应用。  我们按任务和工作方向系统地分类了这些研究；总结了相关评价方法和基准；并深入探讨了“自我反馈真的有效吗？”这一关注点。我们提出了几个关键观点，包括“内部一致性沙漏进化”，“一致性(几乎)等于正确性”的假设，以及“潜在与显式推理的悖论”。此外，我们还概述了未来研究的有希望的方向。我们开源了实验代码、参考文献列表和统计数据，可在以下网址获取：https://github.com/IAAR-Shanghai/ICSFSurvey。**|
|**2024-07-19**|**On Pre-training of Multimodal Language Models Customized for Chart Understanding**|Wan-Cyuan Fan et.al.|[2407.14506](http://arxiv.org/abs/2407.14506)|null|最近的研究在定制多模态大型语言模型（MLLM）以执行领域特定任务方面取得了令人鼓舞的成果，特别是在科学图表理解领域。这些研究通常采用视觉指令微调，并使用专门的数据集来提高图表领域的问题与解答（QA）准确性。然而，它们往往忽略了自然图像-文本预训练数据与数字图表图像-QA数据之间固有的差异，特别是在模型从图表中提取潜在数值的能力上。本文针对这一疏漏，探讨了提升MLLM图表理解能力所需的训练过程。我们提出了三个主要发现：（1）在对齐预训练中加入原始数据值显著提高了图表数据的理解能力。（2）在端到端微调过程中随机替换图像为其文本表示，将语言推理能力转移到图表解释技能上。（3）在微调过程中，要求模型首先提取图表的潜在数据，然后再回答问题，可以进一步提高准确性。因此，我们引入了CHOPINLLM，这是一种专为深入理解图表而设计的MLLM。CHOPINLLM能有效地解析各种类型的图表，包括未标注的图表，同时保持强大的推理能力。此外，我们建立了一个新的基准，用于评估MLLM对不同类型的图表在各个理解层次上的掌握程度。实验结果表明，CHOPINLLM在理解和未标注图表的多种类型方面表现出色，覆盖了广泛的类型。|
|**2024-07-19**|**Evaluating the Reliability of Self-Explanations in Large Language Models**|Korbinian Randl et.al.|[2407.14487](http://arxiv.org/abs/2407.14487)|**[link](https://github.com/k-randl/self-explaining_llms)**|**本文探讨了大型语言模型（LLM）在被要求解释其先前输出时，生成的解释可靠性。我们评估了两种类型的自解释——抽取式和反事实式——使用三种最先进的LLM（参数量从20亿到80亿）在两个不同的分类任务（客观和主观）上的表现。研究结果表明，尽管这些自解释可以与人类判断相关联，但它们并不能完全准确地反映模型的决策过程，这揭示了感知模型推理与实际模型推理之间的差距。我们展示了这一差距是可以被弥补的，因为通过引导LLM生成反事实解释，可以产生忠实、信息丰富且易于验证的结果。这些反事实解释为传统可解释性方法（如SHAP，LIME）提供了一个有前景的替代方案，前提是提示需要针对具体任务进行定制，并检查其有效性。**|
|**2024-07-19**|**Contrastive Learning with Counterfactual Explanations for Radiology Report Generation**|Mingjie Li et.al.|[2407.14474](http://arxiv.org/abs/2407.14474)|null|鉴于解剖学内容的普遍性，放射影像及其对应的报告展现出高度相似性。这种内在的数据偏见可能导致自动报告生成模型学习到纠缠不清且具有误导性的表示，从而产生误诊报告。为解决这些问题，我们提出了一种新颖的基于“假设事实”解释的框架（CoFE），用于放射报告生成。“假设事实”解释作为一种强大的工具，通过提出“如果”情景来理解算法决策如何改变。借助这一概念，CoFE能够通过对比实际与“假设事实”图像的表示来学习非误导性的视觉表示。具体而言，我们通过在正例和负例之间交换补丁，直到预测诊断发生变化，以此衍生出“假设事实”图像。这里，正例和负例是最语义上最相似但具有不同诊断标签的样本。此外，CoFE采用可学习的提示，以有效微调预训练的大语言模型，封装了实际与“假设事实”的内容，提供更加泛化的提示表示。在两个基准数据集上的广泛实验表明，利用“假设事实”解释使CoFE能够生成语义连贯、事实完整的报告，并在语言生成和临床效能指标方面表现出色。  请注意，上述文本已按照要求进行了翻译，未包含“,”字符，并且没有输出其他无关内容。|
|**2024-07-19**|**Check-Eval: A Checklist-based Approach for Evaluating Text Quality**|Jayr Pereira et.al.|[2407.14467](http://arxiv.org/abs/2407.14467)|null|评估大型语言模型(LLM)生成的文本质量仍然是一个重大挑战。传统指标往往在需要创造性和细微差别的任务中与人类判断不一致。在这篇论文中，我们提出了Check-Eval，一种新颖的评价框架，利用LLM通过清单为基础的方法来评估生成文本的质量。Check-Eval可以作为无参考和有参考的评价方法使用，提供结构化和可解释的文本质量评估。该框架主要包括两个阶段：清单生成和清单评估。我们在两个基准数据集上验证了Check-Eval：葡萄牙法律语义文本相似性和SummEval。我们的结果表明，Check-Eval与现有指标相比，如G-Eval和GPTScore，在与人类判断的相关性方面表现更佳，这凸显了其作为自然语言生成任务中更可靠、更有效的评价框架的潜力。我们实验的代码可以在https://anonymous.4open.science/r/check-eval-0DB4找到。|
|**2024-07-19**|**Undermining Mental Proof: How AI Can Make Cooperation Harder by Making Thinking Easier**|Zachary Wojtowicz et.al.|[2407.14452](http://arxiv.org/abs/2407.14452)|null|大型语言模型和其他高度先进的AI系统减轻了决定说什么或做什么的负担，但这种便利性却可能在社交场合下削弱我们行动的有效性。我们通过引入“心理证明”的整合性理论概念来解释这种看似矛盾的现象。从招聘到约会，心理证明使人们能够在低信任环境中，通过可观察的行为来可靠地向他人传达价值观、意图、知识状态等私人心理特征，在这些环境下诚实行为难以强制执行。我们借鉴经济学、理论生物学和计算机科学的研究成果，描述了实现心理证明的核心理论机制。对这些机制的分析阐明了人工智能在何时以及如何使得低信任环境下的合作变得更加困难，尽管它让思考变得更加容易。  请注意，以上翻译尽量遵循了原文的逻辑结构和用词，力求准确传达原文的学术含义。|
|**2024-07-19**|**Token-level Correlation-guided Compression for Efficient Multimodal Document Understanding**|Renshan Zhang et.al.|[2407.14439](http://arxiv.org/abs/2407.14439)|**[link](https://github.com/JiuTian-VL/TokenCorrCompressor)**|**将高分辨率文档图像裁剪成多个子图是当前多模态大型语言模型（MLLMs）进行文档理解的最常用方法。现有的大多数文档理解方法保留子图中的所有标记并同等对待，忽略了它们的信息差异性，导致图像标记数量显著增加。为了实现更适应性和高效的文档理解，我们提出了一种基于标记级相关性引导压缩的方法，这是一种无参数且可即插即用的优化标记处理策略。  首先，我们提出了一种创新的方法来评估模式重复性，基于每个块标记之间的相关性。这种方法能够识别冗余标记，从而确定子图的信息密度。其次，我们展示了一种标记级采样方法，通过深入研究[CLS]标记和块标记之间的相关性，有效地捕捉最具信息量的标记。通过整合这些策略，我们开发了一个可无缝集成到使用裁剪技术的MLLMs中的即插即用自适应压缩模块。该模块不仅在训练和推理过程中提高了处理速度，而且保持了相当的性能水平。  我们与最先进的文档理解模型mPLUG-DocOwl1.5进行了实验，并通过与其他压缩方法的广泛对比，证明了其有效性。**|
|**2024-07-19**|**The Vision of Autonomic Computing: Can LLMs Make It a Reality?**|Zhiyang Zhang et.al.|[2407.14402](http://arxiv.org/abs/2407.14402)|null|二十年前提出的自管理计算(ACV)愿景，设想了能够自我管理的计算系统，类似于生物有机体，能够无缝适应不断变化的环境。尽管经过数十年的研究，但由于现代计算系统的动态性和复杂性，实现ACV仍然具有挑战性。最近，大型语言模型(LLM)的发展为解决这些挑战提供了有希望的解决方案，通过利用其广泛的知识、语言理解和任务自动化能力。本文探讨了通过基于LLM的多代理框架实现微服务管理的自管理计算的可能性。我们引入了一个五级分类法，用于自主服务维护，并提出了一个基于Sock Shop微服务演示项目的在线评估基准，以评估我们框架的性能。我们的发现表明，在实现第三级自治方面取得了显著进展，突出了LLM在检测和解决微服务架构中的问题方面的有效性。本研究通过将LLM集成到微服务管理框架中，为推进自管理计算做出了贡献，为更适应性强和自我管理的计算系统铺平了道路。代码将在https://aka.ms/ACV-LLM上提供。|
|**2024-07-19**|**Open Artificial Knowledge**|Vadim Borisov et.al.|[2407.14371](http://arxiv.org/abs/2407.14371)|null|像ChatGPT、Claude和Gemini这样的基于聊天的人工智能系统的巨大成功，归功于大型语言模型（LLM）在海量数据集上的训练。然而，获取高质量、多样性和合乎道德的训练数据仍然是一个重大挑战。我们引入了开放人工知识（OAK）数据集，这是一个大规模资源，目前包含超过5亿个令牌，旨在解决这一问题。OAK利用了一系列最先进的LLM，包括GPT4o、LLaMa3-70B、LLaMa3-8B、Mixtral-8x7B、Gemma-7B和Gemma-2-9B，根据维基百科的主要分类生成高质量的跨领域文本。我们的方法确保了广泛的知识覆盖范围，同时保持连贯性和事实准确性。OAK数据集旨在促进更强大和更对齐的语言模型的发展，同时解决了LLM训练中的数据稀缺性和隐私关键问题，并且它可以在www.oakdataset.org上免费获取。|
|**2024-07-19**|**Enhancing Zero-shot Audio Classification using Sound Attribute Knowledge from Large Language Models**|Xuenan Xu et.al.|[2407.14355](http://arxiv.org/abs/2407.14355)|null|零样本音频分类旨在识别和分类模型在训练期间从未见过的声音类别。本文提出了一种使用自动生成的声音属性描述进行零样本音频分类的新方法。我们提出了一系列声音属性，并利用大型语言模型的领域知识为每个类别生成详细的属性描述。与以往主要依赖类别标签或简单描述的工作不同，我们的方法专注于多维内在听觉属性，捕捉声音类别的不同特征。此外，我们引入了对比学习方法来增强从文本标签中进行的零样本学习。我们在VGGSound和AudioSet数据集上验证了我们方法的有效性。实验结果表明，我们的方法在零样本分类准确率方面有显著提高。消融实验结果显示，无论模型架构如何，性能提升都表现得非常稳健。  请注意，代码可在以下网址获取：https://www.github.com/wsntxxn/AttrEnhZsAc。|
|**2024-07-18**|**SegPoint: Segment Any Point Cloud via Large Language Model**|Shuting He et.al.|[2407.13761](http://arxiv.org/abs/2407.13761)|null|尽管在三维点云分割领域取得了显著进展，现有方法主要针对特定任务，并依赖于明确指令来识别目标，缺乏在一个统一框架内推断和理解用户隐含意图的能力。本文提出了一种名为SegPoint的模型，该模型利用多模态大型语言模型(LLM)的推理能力，实现了对四种不同任务的点级分割掩码生成：1) 三维指令分割，2) 三维引用分割，3) 三维语义分割，以及4) 三维开放词汇语义分割。为了推动三维指令研究的发展，我们引入了一个新的基准数据集Instruct3D，旨在通过复杂且隐含的指令文本评估分割性能，包含了2,565对点云-指令。实验结果表明，SegPoint在诸如ScanRefer(用于引用分割)和ScanNet(用于语义分割)等现有基准上表现出色，同时在Instruct3D数据集上取得了优异成果。据我们所知，SegPoint是首个在一个统一框架下解决这些多样化分割任务的模型，且表现令人满意。|
|**2024-07-18**|**Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models**|Zhuo Chen et.al.|[2407.13757](http://arxiv.org/abs/2407.13757)|null|增强生成式（RAG）模型应用于解决大型语言模型的幻觉问题和实时约束，但同时也引发了针对检索腐败攻击的脆弱性。现有研究主要探讨了RAG在白盒和封闭领域问答任务中的不可靠性。本文旨在揭示RAG模型在面对黑盒攻击进行观点操纵时的脆弱性，探讨此类攻击对用户认知和决策的影响，为提高RAG模型的可靠性和安全性提供新见解。我们通过指令操纵RAG中检索模型的排名结果，并使用这些结果训练一个代理模型。然后，通过将对抗性检索攻击方法应用于代理模型，进一步实现了对RAG的黑盒转移攻击。在多个主题的意见数据集上进行的实验表明，所提出的攻击策略能够显著改变RAG生成内容的观点极性。这不仅揭示了模型的脆弱性，更重要的是，暴露了可能对用户认知和决策产生负面影响的潜在风险，使得误导用户接受错误或有偏见的信息变得更容易。|
|**2024-07-18**|**CellularLint: A Systematic Approach to Identify Inconsistent Behavior in Cellular Network Specifications**|Mirza Masfiqur Rahman et.al.|[2407.13742](http://arxiv.org/abs/2407.13742)|null|近年来，人们越来越关注对蜂窝网络的安全审查，通常将安全漏洞归咎于底层协议设计描述中的问题。这些协议设计规范通常是长达数千页的庞大文档，可能包含不准确之处、规格说明不足、隐含假设和内部不一致性。鉴于此，我们引入了CellularLint——一个针对4G和5G标准的半自动不一致性检测框架，利用一系列自然语言处理技术。我们的方法采用了一种改进的少量样本学习机制，基于领域适应的大型语言模型。通过在庞大的蜂窝网络协议语料库上预训练，该方法使CellularLint能够同时在不同层次的语义和实际用例中检测不一致性，从而显著推进了协议规范的自动化分析，并以可扩展的方式进行。  在研究中，我们专注于4G和5G网络的非接入层（Non-Access Stratum，NAS）及其安全规范，最终发现了157个不一致性，准确率为82.67%。通过对开源实现和17款商用设备验证这些不一致性，我们确认它们确实对设计决策产生重大影响，可能引发与隐私、完整性、可用性和互操作性相关的担忧。|
|**2024-07-18**|**Baba Is AI: Break the Rules to Beat the Benchmark**|Nathan Cloos et.al.|[2407.13729](http://arxiv.org/abs/2407.13729)|null|人类解决问题的方式既包括遵循现有的规则和程序，也包括通过创新性的飞跃来重新定义这些规则和目标。为了探索这些能力，我们开发了一个基于游戏《Baba Is You》的新基准测试，在这个游戏中，玩家需要操纵环境中的物体以及规则——这些规则被表示为带有文字的可移动瓷砖——以达到指定的目标并赢得游戏。我们对三种最先进的多模态大型语言模型（OpenAI的GPT-4o，Google的Gemini-1.5-Pro和Gemini-1.5-Flash）进行了测试，结果发现，当需要通过操纵和组合游戏规则来进行泛化时，这些模型的表现极为不佳。|
|**2024-07-18**|**CoDefeater: Using LLMs To Find Defeaters in Assurance Cases**|Usman Gohar et.al.|[2407.13717](http://arxiv.org/abs/2407.13717)|**[link](https://gitlab.com/anonymousdot/codefeater)**|构建保证案例是证明安全关键系统在其计划环境中安全运行的广泛使用且有时是必需的过程。为了降低错误和遗漏边缘案例的风险，引入了反驳者这一概念 - 即挑战保证案例中的主张或证据的论点，可以及时检测论证中的弱点，促使进一步调查并及时采取缓解措施。然而，捕捉反驳者依赖于专家的判断、经验和创造力，并且必须迭代进行，以应对不断变化的需求和法规。本文提出了一种名为CoDefeater的自动化流程，利用大型语言模型(LLM)来寻找反驳者。在两个系统上的初步结果显示，LLM能够有效地找到已知和未预见的可行反驳者，支持安全分析师提高保证案例的完整性和信心。  请注意，上述文本已经按照要求进行了翻译，且确保内容中不包含","字符。|
|**2024-07-18**|**Understanding Reference Policies in Direct Preference Optimization**|Yixin Liu et.al.|[2407.13709](http://arxiv.org/abs/2407.13709)|null|直接偏好优化（DPO）已成为大型语言模型（LLM）指令微调的常用训练方法。本文深入探讨了DPO一个较少研究的方面——其对参考模型或策略的依赖性。参考策略，通常体现为待进一步微调的模型本身，至关重要，因为它们可能对DPO的有效性设定上限。因此，我们在这项工作中针对三个相关研究问题进行探究。首先，我们探索DPO中KL散度约束的最佳强度，该约束惩罚偏离参考策略的行为，发现DPO对此强度敏感。接着，我们通过理论和实证比较，检验了参考策略在指令微调中的必要性，对比了DPO与相关学习目标，证明了DPO的优势。此外，我们还考察了DPO是否从更强大的参考策略中获益，结果表明，更强的参考策略确实能带来更好的性能，但仅当它与被微调的模型相似时。我们的发现揭示了参考策略在DPO中的混淆作用，并为最佳实践提供了见解，同时也指出了未来研究的开放问题。|
|**2024-07-18**|**A Comprehensive Review of Recommender Systems: Transitioning from Theory to Practice**|Shaina Raza et.al.|[2407.13699](http://arxiv.org/abs/2407.13699)|null|推荐系统（RS）在通过提供个性化项目建议来增强用户体验方面扮演着核心角色。本综述全面回顾了从2017年至2024年RS领域的进展，有效地将理论创新与实际应用联系起来。我们探讨了从传统的RS技术，如基于内容的和协同过滤方法，到涉及深度学习、图模型、强化学习以及大型语言模型的先进方法的发展历程。同时，我们也讨论了专门的系统，包括情境感知型、基于评论的和公平性意识的RS。这篇综述的主要目标是连接理论与实践的桥梁。它涵盖了电子商贸、医疗保健和金融等多个领域的挑战，强调了对可扩展、实时和值得信赖解决方案的需求。通过本综述，我们推动学术研究与行业实践之间更紧密的合作。本综述提供的见解旨在指导行业专业人士优化RS的部署，并激发未来的研究方向，特别是在应对新兴技术和社会趋势方面的挑战。|
|**2024-07-18**|**Prover-Verifier Games improve legibility of LLM outputs**|Jan Hendrik Kirchner et.al.|[2407.13692](http://arxiv.org/abs/2407.13692)|null|提高大型语言模型(LLM)输出的置信度的一种方法是，通过清晰且易于验证的推理来支持它们，我们称这种性质为可读性。我们在解决小学数学问题的背景下研究了可读性，并表明仅针对答案正确性优化的思维链解决方案可能会降低其可读性。为了缓解可读性的损失，我们提出了一种受Anil等人(2021)的Prover-Verifier游戏启发的训练算法。我们的算法迭代地训练小型验证器来预测解的正确性，训练“有益的”证明者生成被验证器接受的正确解，以及训练“狡猾的”证明者生成欺骗验证器的错误解。我们发现，在训练过程中，“有益的”证明者的准确性和验证器对对抗性攻击的鲁棒性都有所提高。此外，我们还展示了可读性训练可以转移到在时间限制下验证解正确性的人类身上。在LLM训练的过程中，当检查“有益的”证明者的解时，人类的准确性提高；而当检查“狡猾的”证明者的解时，人类的准确性下降。因此，针对小型验证器进行可检查性训练是一种增加输出可读性的可行技术。我们的结果表明，针对小型验证器的可读性训练是提高大型LLM对人类的可读性，从而有助于超人模型对齐的一个实用途径。|
|**2024-07-18**|**COMCAT: Leveraging Human Judgment to Improve Automatic Documentation and Summarization**|Skyler Grandel et.al.|[2407.13648](http://arxiv.org/abs/2407.13648)|null|软件维护占据了软件生命周期成本的很大一部分，其中显著的一部分归因于代码理解的难度。文档，如总结和解释代码的注释，可以简化软件理解。我们提出了COMCAT，一种通过增强大型语言模型（LLM）与专家指导的上下文来自动化生成注释的方法，旨在对源代码进行注释以提高理解性。我们的方法能够为给定的代码片段或文件选择最相关和最有信息量的注释。我们开发了COMCAT管道，用于对C/C++文件进行注释，具体步骤包括：(1)自动识别放置注释的合适位置，(2)预测每个位置最有益的注释类型，以及(3)基于选定的位置和注释类型生成注释。在人类主题评估中，我们证明了COMCAT生成的注释在三个指示性的软件工程任务中显著提高了开发者对代码的理解程度，对于87%的参与者来说，理解程度提高了高达12%。此外，我们证明了COMCAT生成的注释至少与人工编写的注释一样准确和可读，并且对于高达92%的代码片段，这些注释优于标准ChatGPT生成的注释。此外，我们还开发并发布了一个数据集，其中包含源代码片段、人工编写的注释以及人工标注的注释类别。COMCAT利用LLM提供了一种显著改进，适用于各种人类软件工程任务中的代码理解。  COMCAT通过利用大型语言模型和专家指导的上下文，实现了自动化代码注释的生成，显著提升了代码的可读性和理解性，特别是在C/C++编程语言中。通过对代码片段自动插入注释、智能预测注释类型及生成相应注释内容，该方法不仅提高了软件工程师的工作效率，还保证了注释的质量和准确性。实验结果表明，相比无注释或由ChatGPT生成的注释，COMCAT生成的注释能更有效地帮助开发者理解代码逻辑，特别是在复杂的软件工程任务中表现尤为突出。此外，项目团队还公开了包含真实代码片段和注释的数据集，为研究社区提供了宝贵的资源，进一步推动了代码理解和自动化注释领域的研究进展。|
|**2024-07-18**|**Weak-to-Strong Reasoning**|Yuqing Yang et.al.|[2407.13647](http://arxiv.org/abs/2407.13647)|**[link](https://github.com/gair-nlp/weak-to-strong-reasoning)**|当大型语言模型（LLM）的能力超越人类水平时，为这些模型提供全面而准确的监督变得愈发困难。弱到强学习方法，即利用能力较弱的模型来激发更强模型的潜在能力，在这种情况下展现出其价值。然而，该方法在复杂推理任务上的有效性尚未得到验证。此外，当前在弱到强的学习设置下处理推理任务时，缺乏有效的方法来避免盲目模仿弱监督者，包括其错误。本文提出了一种渐进式学习框架，使强大的模型能够自主优化其训练数据，无需依赖更高级模型或人工标注数据的输入。该框架首先在精选的小规模、高质量数据集上进行有监督的微调，随后对由强模型自身识别的对比样本进行偏好优化。在GSM8K和MATH数据集上的广泛实验表明，我们的方法显著提升了Llama2-70b在三个不同弱模型指导下的推理能力。这一方法在前瞻性的实验配置中也得到了验证，其中Llama3-8b-instruct有效地指导了Llama3-70b在极具挑战性的OlympicArena数据集上的表现。本研究为提升AI推理能力开辟了一条更为可扩展且精妙的路径。所有相关代码和资源均可在 https://github.com/GAIR-NLP/weak-to-strong-reasoning 获得。|
|**2024-07-17**|**EchoSight: Advancing Visual-Language Models with Wiki Knowledge**|Yibin Yan et.al.|[2407.12735](http://arxiv.org/abs/2407.12735)|null|**摘要：**  知识驱动的视觉问答（KVQA）任务要求利用丰富背景知识解答图像相关问题，但生成模型在这方面常面临挑战。为此，我们提出EchoSight，一个新颖的多模态检索增强生成（Retrieval-Augmented Generation，RAG）框架，旨在帮助大型语言模型（LLMs）处理需要详尽百科知识的视觉问答。EchoSight首先仅使用图像信息在维基百科中搜索文章，然后对候选文章根据它们与文本-图像查询的相关性进行二次排序，从而显著提升多模态知识的整合，进而提高检索效果和答案的准确性。我们在Encyclopedic VQA和InfoSeek数据集上的实验结果表明，EchoSight在知识型视觉问答中实现了新的state-of-the-art成绩，Encyclopedic VQA任务上达到41.8%的准确率，InfoSeek任务上达到31.3%。|
|**2024-07-17**|**NL2Contact: Natural Language Guided 3D Hand-Object Contact Modeling with Diffusion Model**|Zhongqun Zhang et.al.|[2407.12727](http://arxiv.org/abs/2407.12727)|null|### 背景  在三维手部-物体重建中，精确的手部与物体之间的物理接触是提升手部姿态估计准确性和生成新的人类抓握动作的标准。然而，现有的方法依赖于难以定义或控制的几何约束。本文提出了一项新的任务：通过自然语言描述进行可控的三维手部-物体接触建模。面临的挑战包括：一、从语言到接触的复杂跨模态建模；二、缺乏描述接触模式的文本数据。为解决这些问题，我们设计了NL2Contact模型，它利用分段扩散模型生成可控制的接触。给定对手和接触的自然语言描述，NL2Contact能够生成逼真且忠实的三维手部-物体接触。  ### 任务  我们开发了NL2Contact模型，旨在通过自然语言描述生成具有控制性的三维手部-物体接触。为训练这个模型，我们创建了首个名为\textit{ContactDescribe}的数据集，其中包含基于精心设计的提示（如抓取动作、抓取类型、接触位置和自由手指状态）生成的丰富多样的手部中心接触描述。我们的模型在优化抓握姿势和基于文本描述生成新的人类抓握动作方面展示了应用潜力。|
|**2024-07-17**|**Is Sarcasm Detection A Step-by-Step Reasoning Process in Large Language Models?**|Ben Yao et.al.|[2407.12725](http://arxiv.org/abs/2407.12725)|null|在大型语言模型（LLMs）解决复杂问题的能力方面，通过逐步推理步骤的扩展显著提升其性能，因为这促使模型进行序列思考。然而，人类对讽刺的理解通常被视为一种直觉且整体的认知过程，它整合了语言、上下文和情感线索，形成对说话者真实意图的全面理解，这种理解被认为不局限于一步步的推理过程。为了验证这一观点，我们提出了一种新的提示框架，称为SarcasmCue，它包含了四种提示策略：连锁矛盾（CoC）、线索图（GoC）、线索集合（BoC）和线索张量（ToC）。这些方法旨在引导LLMs通过考虑顺序和非顺序提示来识别人类的讽刺。我们在四个基准数据集上的全面实证比较表明，我们的四种提示方法明显优于标准的输入-输出提示、CoT和ToT，而且非顺序提示通常优于顺序提示。|
|**2024-07-17**|**The Future of Learning: Large Language Models through the Lens of Students**|He Zhang et.al.|[2407.12723](http://arxiv.org/abs/2407.12723)|null|随着大型语言模型（LLMs）的不断发展，它们在性能上的提升和功能扩展对教育领域产生了显著影响。本研究通过访谈14名学生，探讨他们日常与ChatGPT的互动。初步结果显示，学生们在享受ChatGPT提高学习效率和信息获取便利的同时，也面临着信任危机和伦理顾虑。他们认为ChatGPT相较于传统AI更显“人性化”。然而，这种矛盾情绪、行为不一致以及对学生整体上积极的态度，凸显了ChatGPT在教育领域的潜在价值。但值得注意的是，尽管其智能程度高，可能带来负面效应。因此，我们强调在应用时需谨慎，并致力于在未来的开发中减少潜在的危害。|
|**2024-07-17**|**MoME: Mixture of Multimodal Experts for Generalist Multimodal Large Language Models**|Leyang Shen et.al.|[2407.12709](http://arxiv.org/abs/2407.12709)|**[link](https://github.com/jiutian-vl/mome)**|**在多项视觉-语言任务中，多模态大型语言模型（MLLM）展现出卓越的能力。然而，通常情况下，通用的MLLM在大多数VL任务上的性能不如专门化的MLLM，这是因为存在任务干扰。为此，我们在这篇论文中提出了一种混合多模态专家（MoME）架构，旨在减轻任务干扰，从而获得一个全能的MLLM。MoME主要由两个关键组件构成：视觉专家混合体（MoVE）和语言专家混合体（MoLE）。MoVE能够自适应地调整来自不同视觉编码器的特征，并在转换架构上具有良好的兼容性。MoLE通过稀疏门控专家融入到语言模型中，实现了几乎无额外成本的性能提升。为了应对任务干扰，MoME专注于视觉和语言两种模态，以适应任务间的差异。大量的实验结果表明，MoME显著提高了通用MLLM在各种VL任务中的表现。源代码已在https://github.com/JiuTian-VL/MoME上发布。**|
|**2024-07-17**|**Patch-Level Training for Large Language Models**|Chenze Shao et.al.|[2407.12665](http://arxiv.org/abs/2407.12665)|**[link](https://github.com/shaochenze/patchtrain)**|**随着大型语言模型（LLMs）在语言理解和生成方面取得显著进步，其训练效率成为一个关键问题。传统上，LLMs通过预测序列中的下一个令牌进行训练。尽管基于令牌的训练方法取得了成功，但其计算成本高昂，因为需要处理大量令牌。为此，这篇论文提出了一种名为“patch-level training”的方法，它通过将多个令牌压缩成单个patch来缩短序列长度。在patch-level训练中，我们输入更短的patch序列，让模型学习预测下一个patch，从而大幅度减少了大部分训练数据的处理成本。接着，模型会进行剩余训练数据的令牌级训练，以适应推理模式。实验在不同规模的模型（370M-2.7亿参数）上进行，结果表明patch-level训练可以将总体计算成本降低至0.5倍，同时不会影响模型性能。源代码可在此获取：\url{https://github.com/shaochenze/PatchTrain}。**|
|**2024-07-17**|**Zero-shot Text-guided Infinite Image Synthesis with LLM guidance**|Soyeong Kwon et.al.|[2407.12642](http://arxiv.org/abs/2407.12642)|null|**背景：** 文本引导的图像编辑和生成方法在现实世界中有广泛的应用。然而，文本引导的无限图像合成面临着一些挑战。首先，缺乏高分辨率且具有丰富情境多样性的文本-图像配对数据集。其次，根据文本扩展图像需要全局连贯性和丰富的局部上下文理解能力。以往的研究主要集中在有限类别，如自然风景，且需要在高分辨率图像及其配文上进行训练。为解决这些问题，我们提出了一种新颖的方法，利用大型语言模型（LLMs）同时处理全局连贯性和局部上下文理解，无需任何高分辨率的文本-图像配对训练数据。  **方法：** 我们在训练扩散模型时，让它根据LLM生成的全局和局部描述以及视觉特征来扩展图像。在推理阶段，给定一张图片和一个全局描述，我们使用LLM生成下一个局部描述来扩展输入图像。然后，我们结合全局描述、生成的局部描述和视觉特征来扩展图像，以确保全局一致性并考虑空间局部上下文。  **实验结果：** 实验表明，我们的模型在定量和定性上都优于基线。此外，我们的模型展示了在零样本情况下，借助LLM引导进行文本引导任意大小图像生成的能力。  总结： 本文介绍了一种利用大型语言模型进行文本引导的图像扩展方法，无需依赖高分辨率的配对数据，能够实现全局连贯性和局部上下文理解，并在实验中表现出色，支持零样本任意大小图像生成。|
|**2024-07-17**|**Harnessing the Power of Artificial Intelligence to Vitalize Endangered Indigenous Languages: Technologies and Experiences**|Claudio Pinhanez et.al.|[2407.12620](http://arxiv.org/abs/2407.12620)|null|自2022年以来，我们一直在探索人工智能（AI）和现代自然语言处理（NLP），特别是大型语言模型（LLMs）的应用领域，以支持和促进濒临消失的土著语言的使用与文档化。首先，我们关注世界语言多样性的减少，并讨论与处理土著语言相关的独特伦理挑战。为应对这些挑战，我们提出了一种基于社区参与和使用的AI开发新循环。接着，我们报告了使用少量数据微调最先进的翻译器，成功开发出高质量的土著语言机器翻译的鼓舞人心的成果，并讨论了避免开发过程中的一些常见陷阱。我们还展示了2023年和2024年在巴西与土著社区合作项目中的原型，目标是简化写作，以及发展土著语言模型（ILMs）作为创建拼写检查器、下一个词预测器等工具的可复制和可扩展方法。最后，我们展望一个未来，濒危的语言将通过互动的语言模型得以保存。|
|**2024-07-17**|**AudienceView: AI-Assisted Interpretation of Audience Feedback in Journalism**|William Brannon et.al.|[2407.12613](http://arxiv.org/abs/2407.12613)|**[link](https://github.com/mit-ccc/AudienceView-demo)**|****背景：** 记者理解和利用受众反馈至关重要，但如今他们在线面临大量观众评论，这是一项艰巨的任务。我们推出了AudienceView，一个在线工具，旨在通过大型语言模型（LLMs）帮助记者对这些反馈进行分类和解读。AudienceView识别主题和话题，将它们与特定评论关联，展示评论的情感倾向和分布，并协助用户构思后续报道项目。我们将探讨这类工具如何融入记者的工作流程，并强调情境理解及人类判断的重要性。  请记住，以上翻译不包含","字符。**|
|**2024-07-17**|**E5-V: Universal Embeddings with Multimodal Large Language Models**|Ting Jiang et.al.|[2407.12580](http://arxiv.org/abs/2407.12580)|**[link](https://github.com/kongds/e5-v)**|**### 背景  大规模多模态语言模型（MLLMs）在通用视觉和语言理解方面取得了显著进步。然而，如何利用MLLMs处理多模态信息的表示方式尚未充分研究。本文提出了一种新的框架E5-V，旨在使MLLMs适应实现通用多模态嵌入。研究结果表明，与先前方法相比，MLLMs在处理多模态输入方面展现出巨大潜力。通过结合提示，E5-V有效地弥合了不同类型输入之间的模态鸿沟，即使在无需微调的情况下也能表现出强大的多模态嵌入能力。  ### 方法  E5-V采用单一模态训练策略，仅使用文本对进行训练，这相较于传统的基于图像-文本对的多模态训练，显著提高了性能，同时降低了大约95%的训练成本，避免了收集昂贵的多模态训练数据的需求。实验在四种任务上进行了广泛的验证，以展示E5-V的有效性。  ### 结果  作为一款通用多模态模型，E5-V不仅在各任务中实现了顶尖性能，甚至在某些情况下超越了现有技术水平，所有这些都是基于单模态训练完成的。**|
|**2024-07-16**|**UrbanWorld: An Urban World Model for 3D City Generation**|Yu Shang et.al.|[2407.11965](http://arxiv.org/abs/2407.11965)|null|城市作为人类生活的基本环境，包含了建筑、道路和植被等多元物理元素，这些元素之间存在着复杂的相互关联。构建逼真且互动的3D城市环境对于研发能在现实世界环境中感知、决策和行动的AI至关重要。然而，传统的手工制作过程耗时且精细，需要设计师投入大量精力来精确呈现复杂的城市特征。为此，我们提出UrbanWorld，这是一个首个自动生成定制化、真实且互动的3D城市世界的模型，支持灵活的控制条件。UrbanWorld的生成流程包括四个关键步骤：利用公开的OSM数据进行3D布局生成、借助强大的城市多模态大语言模型（Urban MLLM）进行城市场景规划与设计、通过先进的3D扩散技术实现可控资产渲染，以及MLLM辅助的场景细化。UrbanWorld生成的高保真3D城市环境为通用AI和机器感知系统在模拟中的真实反馈和交互提供了可能。我们致力于将UrbanWorld作为开源且多功能的平台，用于评估和提升AI在真实城市环境中的感知、决策和互动能力。|
|**2024-07-16**|**NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?**|Mo Li et.al.|[2407.11963](http://arxiv.org/abs/2407.11963)|**[link](https://github.com/open-compass/opencompass)**|**本文介绍了一个名为NeedleBench的框架，它是一系列评估大语言模型（LLMs）长文本理解能力的逐步升级任务。该框架涉及不同长度区间（4k、8k、32k、128k、200k、1M乃至更长）和深度范围，通过在不同文本深度区域插入关键数据点，系统性地测试模型在各种情境下的检索和推理能力。针对于双语长文本，我们利用这个框架来考察主流开源模型识别与问题相关的关键信息，并运用这些信息进行推理的能力。  此外，我们提出了祖先追踪挑战（Ancestral Trace Challenge，ATC），旨在模拟现实世界中长文本逻辑推理任务的复杂性，提供一个简单的方法来评估LLMs处理复杂长文本上下文的能力。研究结果显示，当前的LLMs在实际的长文本应用中仍有很大的提升空间，因为它们在处理逻辑推理难题时面临挑战。所有代码和资源可在OpenCompass项目（https://github.com/open-compass/opencompass）获取。**|
|**2024-07-16**|**Code Documentation and Analysis to Secure Software Development**|Paul Attie et.al.|[2407.11934](http://arxiv.org/abs/2407.11934)|null|我们介绍了一种名为Code Documentation and Analysis Tool（CoDAT）的工具。CoDAT旨在保持代码文档之间的连贯性，例如，如果代码片段中的某行被修改，相应的注释也会自动更新，确保内部一致性以及与代码的一致性。通过标记过时的注释，CoDAT提醒开发者维护最新的文档。我们利用大型语言模型检查代码片段与其描述的语义一致性，从而也能识别出语义不一致和过时的注释。这有助于程序员编写正确实现代码草图的代码，支持逐步细化方法，从代码草图逐步演变为经过一两次或更多次细化迭代的代码。  CoDAT在IntelliJ IDEA IDE中实现，利用Code Insight守护程序包结合自定义正则表达式算法，标记对应代码块已更改的标记注释。CoDAT的后端结构上是去中心化的，支持分布式账本框架，以实现代码一致性跟踪和架构编译管理。|
|**2024-07-16**|**What's Wrong? Refining Meeting Summaries with LLM Feedback**|Frederic Kirstein et.al.|[2407.11919](http://arxiv.org/abs/2407.11919)|null|随着数字会议的普及，会议摘要提炼成为关键任务。大型语言模型（LLMs）在这一领域展现出巨大潜力，它们在连贯性和理解上下文中超越了传统方法。然而，它们仍需改进以保持相关性并避免错误。我们提出了一种基于多LLM的会议摘要修正方法，通过两阶段过程模拟人类审查：错误识别和摘要精炼。我们发布了QMSum Mistake，这是一个包含200份由人工标注的自动生成会议摘要数据集，针对结构、遗漏和不相关等九种错误类型进行了标记。实验表明，LLMs能够准确识别这些错误。我们将识别出的问题转化为可操作的反馈，以此提升摘要的质量，如相关性、信息量、简洁性和连贯性。这种事后优化策略通过利用多个LLMs来验证输出质量，有效提高了摘要质量。我们的多LLM会议摘要方法对于需要稳健性、行动计划和目标导向的复杂文本生成任务具有潜在应用价值。|
|**2024-07-16**|**Ascend-CC: Confidential Computing on Heterogeneous NPU for Emerging Generative AI Workloads**|Aritra Dhar et.al.|[2407.11888](http://arxiv.org/abs/2407.11888)|null|在云工作负载中，基于大型语言模型（LLMs）的生成AI占据主导地位。专用硬件加速器，如GPU、NPUs和TPUs，因其在AI应用中的卓越性能超越了通用CPU。AI模型和数据通常具有高度敏感性，并来自相互不信任的各方。现有的基于CPU的可信执行环境（TEE），如英特尔SGX或AMD SEV，提供的保护不够充分。像Nvidia-CC这样的设备中心TEE仅针对紧密耦合的CPU-GPU系统，且采用专有方案，需要在主机CPU上部署TEE。另一方面，现有的学术提案大多针对特定的CPU-TEE平台。  为填补这一空白，我们提出了Ascend-CC，一种基于离散NPUs的机密计算架构，无需对主机系统信任。Ascend-CC通过确保数据和模型加密，保护数据、模型参数和运算符二进制，提供强大的安全性。它利用委托式内存语义确保与主机软件栈的隔离，并通过任务鉴权提供模型完整性的强有力保证。我们的Ascend-CC实现和与最新LLMs（如Llama2和Llama3）的评估表明，Ascend-CC引入的开销极小，无需修改AI软件栈。|
|**2024-07-16**|**Schema Matching with Large Language Models: an Experimental Study**|Marcel Parciak et.al.|[2407.11852](http://arxiv.org/abs/2407.11852)|**[link](https://github.com/uhasselt-dsi-data-systems-lab/code-schema-matching-llms-artefacs)**|**该论文探讨了大型语言模型（LLMs）在关系数据库架构（schema）匹配中的应用。目标是仅通过元素名称和描述找出两个关系模式之间的语义对应。研究者构建了一个来自健康领域的基准测试，并提出了不同的任务范围，即使用不同数量上下文信息提示模型进行schema匹配。他们对比了基于LLM的匹配方法与基于字符串相似度的基线，考察了匹配质量、验证工作量、决策确定性和互补性。研究发现，缺乏上下文信息会降低匹配质量，过多的信息也会有负面影响。新版本的LLMs通常能提高决策确定性。有些任务范围下的验证工作相对适度，且能成功识别大量真正意义上的语义匹配。研究结果表明，LLMs有潜力作为schema匹配的初始工具，数据工程师可以利用它们的名称和描述信息快速进行匹配，无需依赖实际数据实例。**|
|**2024-07-16**|**LoFTI: Localization and Factuality Transfer to Indian Locales**|Sona Elza Simon et.al.|[2407.11833](http://arxiv.org/abs/2407.11833)|**[link](https://github.com/csalt-research/lofti)**|**大型语言模型（LLMs）通过训练在互联网上爬取的大型网页数据集，积累了大量的世界知识。然而，这些数据集通常倾向于英语和西欧国家，导致LLMs对来自其他地区，特别是印度的本地化查询产生偏见或虚构的回答。为此，我们提出一个新的基准LoFTI（印度本地化与事实转移），用于评估LLMs的本地化和事实文本转换能力。LoFTI包含关于全球源地点和印度目标地点（包括国家、州和城市的不同层级）实体的事实陈述，涉及各类广泛的主题。我们使用LoFTI来评估Mixtral、GPT-4以及两种适用于本地化事实转移任务的Mixtral衍生方法。实验表明，LoFTI是一个高质量的评估标准，包括GPT-4在内的所有模型在不同层级的本地化上都表现出偏差。**|
|**2024-07-16**|**GPT Assisted Annotation of Rhetorical and Linguistic Features for Interpretable Propaganda Technique Detection in News Text**|Kyle Hamilton et.al.|[2407.11827](http://arxiv.org/abs/2407.11827)|null|尽管机器学习在检测文本中的宣传手段方面引起了广泛关注，但大多数方法侧重于“黑盒”解决方案，其内部工作原理不透明。可解释的方法提供了解决方案，但它们依赖于精心的特征工程和昂贵的专家标注数据。此外，关于说服性文本的语言特性通常由修辞学家或语言学家关注，但没有适合机器学习的标记有此类特性的数据集。本研究旨在编纂文献中识别出的22个修辞和语言特征，目的是对一个已标注有宣传手段的现有数据集进行注释。为了帮助人类专家在自然语言句子上标注这些特征，我们特别设计了名为RhetAnn的网络应用，以减少原本较大的认知负担。接着，使用一小部分标注数据，我们利用GPT-3.5，一种生成大型语言模型（LLM），对剩余数据进行微调，同时兼顾成本效益和分类精度。这项研究表明，结合少量人工标注示例与GPT，可以有效地以传统仅依赖人类专家的标注成本的十分之一左右实现大规模标注过程的扩展。结果与撰写时表现最好的模型（GPT-4）相当，且成本降低10倍。我们的贡献包括这些特征、它们的属性、定义以及示例的机器可读格式，以及RhetAnn的代码、GPT提示和微调流程，这些都推动了可解释的宣传手段检测领域的最新进展。|
|**2024-07-16**|**PipeInfer: Accelerating LLM Inference using Asynchronous Pipelined Speculation**|Branden Butler et.al.|[2407.11798](http://arxiv.org/abs/2407.11798)|null|近年来，大型语言模型（LLMs）在分布式计算机集群上的推理已成为研究热点，许多加速技术借鉴了CPU的推测执行策略。这些技术旨在缓解内存带宽瓶颈，但会增加每次推理运行的端到端延迟，需要高推测接受率来提升性能。然而，由于任务间接受率的变异性，推测性推理可能导致性能下降。此外，管道并行设计需要大量用户请求以保持高利用率。针对这些问题，我们提出了PipeInfer，这是一种旨在减少跨令牌延迟、提高单请求场景下系统利用率的管道化推测加速技术，同时增强了对低推测接受率和低带宽互联的容忍度。  PipeInfer通过连续异步推测和早期推理取消实现了显著的改进。连续异步推测允许同时进行单令牌推理与多个推测运行，从而降低延迟和生成速度。而早期推理取消则能够在推理过程中跳过无效运行的计算，进一步提升速度和延迟。PipeInfer在生成速度上比标准推测性推理最高可提升2.15倍。|
|**2024-07-16**|**Large Language Models as Misleading Assistants in Conversation**|Betty Li Hou et.al.|[2407.11789](http://arxiv.org/abs/2407.11789)|null|大型语言模型（LLMs）在各种信息查询任务上能够提供帮助。然而，模型输出可能会误导用户，无论是无意的还是故意的。我们针对阅读理解任务探讨了LLMs在欺骗性辅助方面的能力，将其作为人类用户的代理。实验对比了三种情况：（1）模型被提示提供真实信息，（2）模型被提示进行微妙误导，以及（3）模型被提示支持错误答案。结果显示，GPT-4能够有效误导GPT-3.5-Turbo和GPT-4自身，欺骗性助手导致任务准确率下降高达23%，相比于使用真实助手。此外，我们发现向用户模型提供更多的上下文信息可以部分抵消欺骗模型的影响。这项研究揭示了LLMs生成误导性信息的能力及其在现实场景中的潜在影响。|
|**2024-07-15**|**VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation**|Bocheng Zou et.al.|[2407.10972](http://arxiv.org/abs/2407.10972)|**[link](https://github.com/vgbench/VGBench)**|**在视觉模型领域，主要的表示方式是使用像素来绘制视觉世界。然而，这并非总是最佳或唯一的表示视觉内容的方法，特别是对于设计师和艺术家，他们常用多边形等几何形状来构建图形。矢量图形（VG）提供了一种文本形式的视觉内容表示，对于卡通或素描等类型的内容可能更为精炼和强大。近期的研究表明，强大的大语言模型（LLMs）在处理矢量图形方面展现出令人鼓舞的结果。但这些工作主要侧重于定性分析、理解或特定类型的矢量图形。我们提出VGBench，这是一个全面的基准，用于评估LLMs在处理矢量图形方面的性能，包括：(a) 对视觉理解和生成的双重关注，(b) 多种矢量图形格式的评估，(c) 不同类型的提问，(d) 广泛的提示技巧，以及(e) 在多种LLMs下的表现。通过对收集的4279个理解样本和5845个生成样本进行评估，我们发现LLMs在这两个方面都表现出强大能力，但在低级格式（如SVG）上表现稍逊。我们的数据和评估流程将在<https://vgbench.github.io>上开源。**|
|**2024-07-15**|**Q-Sparse: All Large Language Models can be Fully Sparsely-Activated**|Hongyu Wang et.al.|[2407.10969](http://arxiv.org/abs/2407.10969)|null|我们提出了一种简单但有效的训练方法，称为Q-Sparse，专为大规模语言模型（LLMs）设计。Q-Sparse使得LLMs的激活全为稀疏，从而在推理阶段带来显著的效率提升。这一方法通过应用顶部K稀疏化技术对激活进行处理，并结合直通估计进行训练。主要成果包括：(1) Q-Sparse在保持与基线LLM结果相当的同时，具有更高的推理时的效率；(2) 我们给出了稀疏激活LLMs的最优推理缩放定律；(3) Q-Sparse在各种场景下表现优秀，包括从头开始训练、预训练模型的继续训练和微调；(4) Q-Sparse适用于全精度和1位精度的LLMs，如BitNet b1.58。特别是，BitNet b1.58与Q-Sparse（可配备MoE）的结合，为未来LLMs的效率提升，包括成本和能耗，提供了基石和清晰路径。|
|**2024-07-15**|**Fast Matrix Multiplications for Lookup Table-Quantized LLMs**|Han Guo et.al.|[2407.10960](http://arxiv.org/abs/2407.10960)|null|大型语言模型（LLMs）的部署通常受到内存带宽的限制，其中主要瓶颈是将模型参数从GPU全局内存传输到寄存器的成本。通过结合权重只量化，可以减少内存移动，从而加速推理速度。然而，为量化后的LLMs设计高性能内核是一项重大挑战，尤其是当权重被压缩到非均匀分隔的位宽（如3位），并采用非均匀查找表（LUT）量化时。本文介绍了一种灵活的查找表引擎FLUTE，它通过对量化权重矩阵进行离线重构，以最小化解压相关的位操作，并通过向量化和复制查找表来缓解共享内存带宽限制。在小批量（小于32）和量化组大小为128（LLM推理中的典型值）的情况下，FLUTE内核的速度可以比现有GEMM内核快2-4倍。作为FLUTE的应用，我们探讨了查找表基的NormalFloat量化的一种简单扩展，并将其应用于量化LLaMA3，获得了与强大基准相当的量化性能，同时实现了端到端吞吐量的1.5到2倍提升。|
|**2024-07-15**|**MMM: Multilingual Mutual Reinforcement Effect Mix Datasets & Test with Open-domain Information Extraction Large Language Models**|Chengguang Gan et.al.|[2407.10953](http://arxiv.org/abs/2407.10953)|null|## 任务  **背景：** 互惠增强效应（MRE）在信息抽取和多任务研究中展现出巨大潜力。然而，由于仅有的MRE混合数据集局限于日语，这限制了全球研究界的广泛探索。为了克服这一局限，我们构建了一个多语言MRE混合数据集（MMM），包含英语、日语和汉语的21个子集。本文还提出了一种利用大型语言模型（LLMs）辅助的数据集翻译方法，通过利用LLMs将原始日语文本进行翻译，大大减少了数据集构建时的人工标注时间。  **贡献：** 我们扩展了数据集，加入了开放领域命名实体识别（NER）和句子分类任务。基于这个扩充后的数据集，我们开发了一个统一的输入-输出框架，训练了一个开放域信息抽取大语言模型（OIELLM）。实验表明，OIELLM模型能够有效处理新的MMM数据集，并表现出显著的性能提升。  总之，我们的工作旨在通过提供多语言资源和高效的翻译策略，推动互惠增强效应在多语言信息抽取领域的应用研究。|
|**2024-07-15**|**Can Textual Semantics Mitigate Sounding Object Segmentation Preference?**|Yaoting Wang et.al.|[2407.10947](http://arxiv.org/abs/2407.10947)|**[link](https://github.com/gewu-lab/sounding-object-segmentation-preference)**|**## 任务  音频-视觉分割（Audio-Visual Segmentation，AVS）任务的目标是利用音频线索在视觉空间中分割出发声物体。然而，研究指出，现有的AVS方法过于依赖对可听见对象的分割偏好，而非精确的音频指导。问题在于，相比于视觉，音频在多声源音场中的语义表现较弱，导致其在指导视觉空间时作用有限。鉴于文本模态经过深入探索，包含丰富的抽象语义，我们提出利用视觉场景中的文本提示来增强音频指导的精确性。  我们的方法首先通过现成的图像描述器获取场景描述，然后利用预训练的大语言模型推断潜在的发声物体作为文本线索。接着，我们设计了一个新颖的基于语义的音频建模模块，引入动态掩码，将音频特征与文本线索融合，生成具有代表性的发声物体特征。这些特征不仅包含音频信息，还蕴含了生动的语义，从而为视觉空间提供更为清晰的指引。我们在AVS基准数据集上的实验结果表明，借助文本提示，我们的方法对音频的敏感度得到提升，在所有三个子集上表现出高度竞争力。项目页面：[https://github.com/GeWu-Lab/Sounding-Object-Segmentation-Preference](https://github.com/GeWu-Lab/Sounding-Object-Segmentation-Preference)。**|
|**2024-07-15**|**GRUtopia: Dream General Robots in a City at Scale**|Hanqing Wang et.al.|[2407.10943](http://arxiv.org/abs/2407.10943)|**[link](https://github.com/openrobotlab/grutopia)**|**近期的研究正在探索Embodied AI领域的规模法则。鉴于收集现实世界数据的高昂成本，我们认为模拟到现实（Sim2Real）方法对于扩展embodied模型的学习至关重要。本文介绍项目GRUtopia，这是一个专为各种机器人设计的首个互动三维社会。它具有多项创新：(a) 场景数据集GRScenes包含了10万张交互式、精细注释的场景，这些场景可以自由组合成城市规模的环境。与以往主要关注家庭环境的作品不同，GRScenes涵盖了89个多样化的场景类别，弥合了服务导向环境中机器人初始部署的差距。(b) GRResidents是一个由大型语言模型驱动的非玩家角色（NPC）系统，负责社交互动、任务生成和任务分配，从而模拟embodied AI应用中的社会场景。(c) 标准化基准GRBench支持各种机器人，但以腿足机器人为主，提供涉及物体导航、社交导航和移动操作的任务，这些任务具有适度的挑战性。我们期望这项工作能够缓解该领域高质量数据的匮乏，并为Embodied AI研究提供更全面的评估。项目代码可从https://github.com/OpenRobotLab/GRUtopia获取。**|
|**2024-07-15**|**FinDKG: Dynamic Knowledge Graphs with Large Language Models for Detecting Global Trends in Financial Markets**|Xiaohui Victor Li et.al.|[2407.10909](http://arxiv.org/abs/2407.10909)|**[link](https://github.com/xiaohui-victor-li/FinDKG)**|动态知识图谱（DKGs）是一种流行的数据结构，用于表示随时间变化的对象之间的各种连接。它们在处理复杂无结构数据源（如文本和图像）提取的信息时展现出高效性。在金融应用中，DKGs可用于基于财经新闻文章探测投资策略的趋势。本研究探索大型语言模型（LLMs）作为动态知识图谱生成器的特性，为此我们提出了一种开源的Fine-tuned LLM，称为集成上下文知识图谱生成器（ICKG）。利用ICKG，我们从财经新闻文章中创建了一个新的开源动态知识图谱，称为FinDKG。此外，我们设计了注意力机制的图神经网络架构（KGTransformer），用于分析这个图谱。我们在基准数据集和FinDKG上测试了模型性能，结果显示在链接预测任务中，KGTransformer表现优异。最后，我们评估了KGTransformer在FinDKG上的主题投资性能，证明它能超越现有的主题交易所交易基金（ETF）。|
|**2024-07-15**|**Hey, That's My Model! Introducing Chain & Hash, An LLM Fingerprinting Technique**|Mark Russinovich et.al.|[2407.10887](http://arxiv.org/abs/2407.10887)|null|随着对大型语言模型（LLMs）被盗和误用的担忧加剧，模型指纹化的必要性提升。在这种背景下，成功的指纹应具备五个特性：透明性、效率、持久性、鲁棒性和不可伪造性。本文首先定义了这些要求。接着，我们提出了一种新的简单指纹方法——Chain & Hash，它融合了加密理念，实现了所有这些特性。Chain & Hash涉及生成一组问题（指纹）及其可能的答案，然后使用安全哈希技术将它们合并，以确定每个问题的值，从而保证不可伪造性，防止对手声称虚假所有权。我们在多个模型上评估了Chain & Hash技术，并展示了它对良性操作（如在不同数据集上微调）和敌意删除指纹的鲁棒性。实验表明，带指纹的模型在各种基准测试中的性能几乎与非指纹化模型相当，同时保持了高效性及其实用价值。|
|**2024-07-15**|**SLIP: Securing LLMs IP Using Weights Decomposition**|Yehonathan Refael et.al.|[2407.10886](http://arxiv.org/abs/2407.10886)|null|随着大型语言模型（LLMs）在学术界和工业界的广泛应用，这些模型的价值作为知识产权（IP）日益凸显，反映出其背后巨大的投资。然而，由于云部署成本高，边缘设备部署的需求增加，这可能导致模型参数被盗用和未经授权使用。当前的保护方法在实用性、准确性损失或适应性方面存在局限。本文提出了一种新颖的混合推理算法，称为SLIP（Secure Lightweight Inference Protocol），旨在保护部署在边缘的模型免受盗窃。SLIP是首个兼顾实际应用的实用性和严格安全性的混合协议，同时保持零精度下降和低延迟影响。  SLIP通过矩阵分解实现了模型在两个计算资源之间的划分：一个安全但昂贵，另一个成本效益高但易受攻击。关键在于，安全资源保留了模型IP中最敏感的部分，同时执行最少的计算，而脆弱资源则相反。此外，该协议提供了防止攻击者利用分割获取保密信息的安全保障。最后，我们展示了实验结果，证明了SLIP的稳健性和有效性，使其成为保护LLMs的理想解决方案。|
|**2024-07-15**|**Understanding the Importance of Evolutionary Search in Automated Heuristic Design with Large Language Models**|Rui Zhang et.al.|[2407.10873](http://arxiv.org/abs/2407.10873)|null|自动化启发式设计（AHD）因其在自动开发高效启发式方法方面的潜力而受到广泛关注。随着大型语言模型（LLMs）的兴起，人们开始探索将AHD视为进化程序搜索（EPS）问题的新途径。然而，当前的基准设置不一致，基础比较不足，且缺乏对LLM与搜索策略结合必要性的深入分析，这使得现有基于LLM的EPS方法的实际进展难以得到充分证明。本研究通过一项大规模基准测试，涵盖了四项基于LLM的EPS方法和四项AHD问题，跨越九种LLM，并进行了五次独立运行。我们的广泛实验提供了有价值的见解，实证了在LLM驱动的AHD方法中的进化搜索的重要性，同时也推动了未来EPS算法开发的进步。为了促进可访问性和可重复性，我们已经全面开源了我们的基准和相关结果。|
|**2024-07-12**|**FairyLandAI: Personalized Fairy Tales utilizing ChatGPT and DALLE-3**|Georgios Makridis et.al.|[2407.09467](http://arxiv.org/abs/2407.09467)|null|在这个充满人工智能驱动的叙事多样性世界中，有一个独特的机会是通过定制和个性化的叙述吸引年轻观众。本文介绍FairyLandAI，这是一个专为儿童开发的创新大型语言模型（LLM），基于OpenAI的API构建。其特别之处在于，FairyLandAI不仅能生成引人入胜、适合各年龄段且反映各种传统的故事，还能自动生成适合高级图像生成工具（如GenAI和Dalle-3）的创意提示，从而丰富讲故事的体验。FairyLandAI精准地适应儿童的想象力世界，提供既教育又娱乐的故事，并与不同年龄阶段所蕴含的价值观相一致。它的独特之处在于根据个体孩子的喜好和文化背景定制故事，标志着个性化叙事新时代的到来。此外，它与图像生成技术的结合提供了全面的叙事体验，激发口头和视觉创造力。实证评估显示，FairyLandAI在创作吸引孩子们的故事方面表现出色，这些故事不仅娱乐，还体现了多元传统中的道德教诲。这个模型对于家长和教育工作者来说是一个宝贵的工具，帮助他们通过引人入胜的故事传递深刻的人生道理。FairyLandAI代表了利用LLMs，特别是OpenAI API进行教育和文化提升的开创性一步，使复杂而富有教育意义的道德故事对年轻、富有想象力的心灵变得易于理解和享受。|
|**2024-07-12**|**Human-like Episodic Memory for Infinite Context LLMs**|Zafeirios Fountas et.al.|[2407.09450](http://arxiv.org/abs/2407.09450)|null|大型语言模型（LLMs）展现了惊人的能力，但它们在处理长序列时仍面临保持连贯性和准确性的问题。人类大脑在组织和检索跨长时间尺度的亲身经历方面尤为出色，能够覆盖一生的记忆。本文提出了一种新颖的方法，称为EM-LLM，它将人类的 episodic memory（情景记忆）和事件认知关键要素融入到LLMs中，使其能够有效处理几乎无限长度的上下文，同时保持计算效率。EM-LLM通过结合贝叶斯惊奇度和图论边界细化技术，在线方式组织令牌序列成连贯的事件。当需要时，通过两阶段的记忆过程——结合相似度和时间邻接的检索，实现高效且类似人类的信息访问。在LongBench数据集上的实验显示，EM-LLM的表现优于最先进的InfLLM模型，总体相对提高了4.3%，在各种任务中，包括提升了33%的PassageRetrieval任务。此外，我们的分析揭示了EM-LLM事件分割与人类感知事件之间的强相关性，暗示了这个人工系统与生物对应机制之间的桥梁。这项工作不仅提升了LLMs处理长序列的能力，还为探索人类记忆机制提供了计算框架，开辟了人工智能和认知科学交叉研究的新途径。|
|**2024-07-12**|**ASTPrompter: Weakly Supervised Automated Language Model Red-Teaming to Identify Likely Toxic Prompts**|Amelia F. Hardy et.al.|[2407.09447](http://arxiv.org/abs/2407.09447)|**[link](https://github.com/sisl/astprompter)**|## 背景  通常的自动化大型语言模型（LLMs）红队对抗策略集中在寻找能触发冻结语言模型（即防御者）生成有毒文本的提示。这可能导致对抗模型（即攻击者）产生难以理解、不自然的输出。在此，我们提出了一种强化学习框架来处理LLMs的红队对抗任务，目标是找到既能（1）触发防御者生成有毒文本，又能（2）保持低困惑度（即防御者打分）的提示。我们认为在红队对抗场景中，这些情况最相关，因为它们很可能在防御者模型的常规使用中出现。我们通过一种新颖的在线和弱监督的Identity Preference Optimization（IPO）变体解决了这个问题，应用于GPT-2和GPT-2 XL作为防御者。实验表明，我们的策略能够生成既可能又会触发毒性的提示。最后，我们分析了学习策略、可能性与毒性之间的权衡，并讨论了相关含义。该项目的源代码可在这里获取：https://github.com/sisl/ASTPrompter/。|
|**2024-07-12**|**MUSCLE: A Model Update Strategy for Compatible LLM Evolution**|Jessica Echterhoff et.al.|[2407.09435](http://arxiv.org/abs/2407.09435)|null|## 背景 大型语言模型（LLMs）由于数据或架构的调整而经常更新以提升性能。在升级过程中，开发者通常侧重于提高总体性能指标，对与旧版本兼容性的关注较少。然而，用户往往会对他们使用的机器学习模型的功能和能力形成心理模型，并随着每次更新需要调整这个模型。频繁的模型变更可能导致用户满意度下降。实际上，下游任务微调器依赖预训练的LLM基模型。当基模型更新时，面向用户的这些下游任务模型可能会出现实例退化或负面翻转——先前正确的实例现在被预测错误。即使下游任务的训练流程保持不变，这种情况也会发生。我们的工作旨在为用户提供无缝的模型更新体验，方法有两个方面。首先，我们提出了一套评估指标，用于衡量模型与旧版本的兼容性，特别适用于生成任务，也可应用于分类任务。我们观察到不同模型版本和更新之间存在退化和不一致性，尤其是在多样化的任务上。  ## 任务 我们的研究旨在通过以下两个途径提供对用户友好的模型更新：一是开发一种兼容性评估标准，用于检测生成任务或其他任务中的模型版本间差异；二是提出一种训练策略，通过训练兼容性模型来减少模型更新中的不一致，从而降低从Llama 1到Llama 2等版本更新时的负面翻转率，最多可减少40%。这样，用户可以更轻松地适应新版本，而无需频繁调整他们的预期和使用方式。|
|**2024-07-12**|**Open (Clinical) LLMs are Sensitive to Instruction Phrasings**|Alberto Mario Ceballos Arroyo et.al.|[2407.09429](http://arxiv.org/abs/2407.09429)|**[link](https://github.com/alceballosa/clin-robust)**|## 背景 基于指令的大型语言模型（LLMs）能够根据自然语言指令执行各种任务，但它们对指令表述的敏感性是一个问题。在医疗领域尤其关键，因为临床医生可能不是提示工程方面的专家，且错误输出的潜在后果更为严重。这就提出了一个实际问题：针对临床自然语言处理任务，指令调优的LLMs对于自然（非攻击性的）指令表述变化有多稳健？我们收集了来自不同任务的医生提示，衡量了七种LLM（包括通用和专用的）对指令表述细微差异的敏感度。研究发现，所有模型的表现差异显著，令人意外的是，专门针对临床数据训练的模型相较于通用领域的模型，其稳定性较差。此外，随意的表述变化可能影响公平性，例如，用于预测死亡率的有效但不同的指令不仅会导致整体性能的波动，还会在不同人群间产生差异。|
|**2024-07-12**|**TelecomGPT: A Framework to Build Telecom-Specfic Large Language Models**|Hang Zou et.al.|[2407.09424](http://arxiv.org/abs/2407.09424)|null|该论文首次提出了一种方法，旨在将大型通用语言模型（LLMs）适应到电信领域的专用模型。为此，我们收集并构建了电信特定的预训练数据集、指令数据集和偏好数据集，分别用于持续预训练、指导调优和对齐调优。由于电信领域缺乏广泛接受的评估基准，我们扩展了现有的评估标准，并提出了三个新的基准：电信数学建模、电信开放性问题与答案（TeleQnA）以及电信代码任务。这些新基准全面评估了LLMs在电信领域的数学建模、开放式问题回答、代码生成、填充、总结和分析等能力。我们的优化模型TelecomGPT在电信数学建模基准上显著优于最先进的模型，如GPT-4、Llama-3和Mistral，并在TeleQnA、3GPP技术文档分类、电信代码摘要与生成以及填充任务上表现出相当的性能。|
|**2024-07-12**|**Mitigating Entity-Level Hallucination in Large Language Models**|Weihang Su et.al.|[2407.09417](http://arxiv.org/abs/2407.09417)|**[link](https://github.com/oneal2000/entityhallucination)**|**随着大型语言模型（LLMs）的兴起，用户获取信息的方式发生了转变，从传统的搜索引擎转向直接与LLMs进行问答交互。然而，LLMs的广泛应用暴露出一个挑战，即“幻觉”生成，即模型生成看似连贯但事实性错误的回答，这导致用户对基于LLMs的信息检索系统产生怀疑。为解决这一问题，本文提出了一种新颖的方法：动态检索增强基于幻觉检测（DRAD）。DRAD改进了传统检索增强技术，通过实时幻觉检测来动态调整检索过程。它主要包括两个核心组件：实时幻觉检测（RHD），用于在无需外部模型的情况下识别潜在的幻觉；以及基于外部知识的自我纠正（SEK），利用外部知识修正这些错误。实验结果表明，DRAD在检测和减少LLMs中的幻觉方面表现出色。我们已将所有代码和数据开源，供学术界使用：https://github.com/oneal2000/EntityHallucination。**|
|**2024-07-12**|**SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers**|Shraman Pramanick et.al.|[2407.09413](http://arxiv.org/abs/2407.09413)|**[link](https://github.com/google/spiqa)**|**### 任务  在深入阅读科学论文时，快速查找信息是关键。然而，现有的基于论文的问题 answering（QA）数据集在规模和内容上存在局限，主要关注文本部分。为弥补这一不足，我们推出了SPIQA（科学论文图像问题回答），这是一个专门设计的大型QA数据集，旨在理解计算机科学各领域的复杂图表、表格和结果可视化。借助多模态大语言模型（MLLMs）的强大理解能力，我们通过自动化和人工筛选创建了这个数据集。SPIQA包含了27万条问题，分为训练、验证和三个不同的评估分段。通过与12个基础模型的广泛实验，我们评估了当前多模态系统理解科研文章细微之处的能力。此外，我们提出了一种链式思维（Chain-of-Thought，CoT）评价策略，结合上下文检索，实现了细致的逐步骤评估，有助于提升模型性能。我们还探讨了额外文本信息对性能提升的上限，这表明了其对未来研究的潜力，并预示着该数据集将革新我们与科学文献互动的方式。**|
|**2024-07-12**|**PersonaRAG: Enhancing Retrieval-Augmented Generation Systems with User-Centric Agents**|Saber Zerhoudi et.al.|[2407.09394](http://arxiv.org/abs/2407.09394)|**[link](https://github.com/padas-lab-de/PersonaRAG)**|大型语言模型（LLMs）由于知识过时和胡编乱造而难以生成可靠的结果。为了解决这个问题，检索增强生成（RAG）模型通过结合外部知识改进了LLMs，但往往无法个性化检索过程。这篇论文提出了一种新颖的框架——PersonaRAG，它引入了以用户为中心的代理，能够根据实时用户数据和交互来调整检索和生成。在多个问答数据集上的评估显示，PersonaRAG相较于基础模型表现出显著优势，能更好地满足用户的个性化需求。实验结果表明，用户适应的信息检索系统具有广阔的发展前景。|
|**2024-07-12**|**GAVEL: Generating Games Via Evolution and Language Models**|Graham Todd et.al.|[2407.09388](http://arxiv.org/abs/2407.09388)|null|自动创建新颖有趣的游戏是一个复杂任务，它涉及如何以计算机可处理的形式表达游戏规则、搜索庞大的潜在游戏空间，以及准确评估未见过游戏的原创性和质量。先前的研究主要关注于有限的规则表示，并依赖于特定领域的启发式方法。在这个工作中，我们专注于在Ludii游戏描述语言中生成新奇的游戏，该语言编码了各种风格和玩法的1000多款棋盘游戏规则。我们借鉴了大型语言模型和进化计算的最新进展，训练了一个能够智能地变异和重组以代码形式表达的游戏机制的模型。我们通过定量和定性分析表明，我们的方法能够创造出新的、有吸引力的游戏，包括那些现有Ludii数据集中未覆盖的游戏区域。生成的一些游戏示例可通过Ludii门户在线体验。|
|**2024-07-11**|**MAVIS: Mathematical Visual Instruction Tuning**|Renrui Zhang et.al.|[2407.08739](http://arxiv.org/abs/2407.08739)|**[link](https://github.com/zrrskywalker/mavis)**|**### 背景  多模态大型语言模型（MLLMs）近年来在学术界和工业界引起了广泛关注。尽管它们在多模态场景中的表现突出，但对数学图解的数学问题求解能力研究尚显不足。为此，我们指出了MLLM在数学视觉领域的三个关键改进领域：数学图解的视觉编码、图解与语言的对齐以及数学推理技能。这促使我们需要大规模、高质量的视觉数学数据和训练流程。本文提出MAVIS（Mathematical VISual instruction tuning for MLLMs），一个针对MLLM的数学视觉指导调参范式，包括一系列数学视觉数据集和专门的MLLM。  ### 方法  MAVIS分为三个阶段进行从头开始的训练。首先，我们创建了MAVIS-Caption，包含558,000个图解-描述对，通过对比学习来微调专为数学设计的视觉编码器（CLIP-Math），以提升图解的视觉理解能力。其次，利用MAVIS-Caption，我们通过投影层将CLIP-Math与大型语言模型（LLM）进行关联，增强数学领域的视觉语言对齐。最后，我们引入MAVIS-Instruct，包含900,000个精心收集和标注的视觉数学问题，用于最终指导调参，以增强MLLM的稳健数学推理能力。在MAVIS-Instruct中，我们提供了每个问题的完整链式思考（Chain-of-Thought, CoT）理由，并减少文本冗余，使模型更专注于视觉元素。  ### 结果  数据和模型已发布在https://github.com/ZrrSkywalker/MAVIS。通过MAVIS，我们旨在填补数学视觉理解的空白，提升MLLM在解决实际数学问题时的表现。**|
|**2024-07-11**|**Real-Time Anomaly Detection and Reactive Planning with Large Language Models**|Rohan Sinha et.al.|[2407.08735](http://arxiv.org/abs/2407.08735)|null|这篇论文探讨了如何利用大规模语言模型（如大型语言模型）在机器人系统中检测和应对异常情况，以提高其鲁棒性和安全性。主要挑战包括减少模型的计算开销以便实现实时应用，以及将模型的判断融入到安全控制框架中。研究者提出了一种两阶段推理框架：首先是一个快速的二元异常分类器，它在语言模型嵌入空间中分析观测数据，如果发现异常，会触发后续的慢速推理阶段，利用生成式语言模型进行深入的逻辑推理。这种设计类似于模型预测控制中的决策分支，考虑到慢速推理器的延迟，可以立即采取备份计划，确保系统的安全性。  通过与最先进的GPT模型的自回归推理方法进行比较，研究发现，即使使用小型语言模型，他们的快速异常分类器也表现出色。这使得他们开发的运行时监控器能够在资源和时间限制下，提升动态机器人系统，如四旋翼无人机或自动驾驶车辆的信任度。论文的视频示例可以在项目页面上查看：https://sites.google.com/view/aesop-llm。|
|**2024-07-11**|**Is Your Model Really A Good Math Reasoner? Evaluating Mathematical Reasoning with Checklist**|Zihao Zhou et.al.|[2407.08733](http://arxiv.org/abs/2407.08733)|null|### 翻译  **摘要：**  强大的数学推理能力是大型语言模型（LLMs）卓越性能的关键体现。如何定义和全面评估LLMs的数学能力，以及在实际应用中反映用户体验，已成为关键问题。目前的基准测试主要侧重于问题解决能力，这可能导致模型过拟合，并无法准确反映真正的数学推理能力。我们认为，如果模型真正理解了问题，它应该能在各种任务中稳健且灵活地应用。在此启发下，我们提出MATHCHECK，一个旨在测试任务泛化和推理鲁棒性的精心设计的清单，以及一个自动生成清单的工具。MATHCHECK包含多个数学推理任务和测试类型，以促进对数学推理能力和行为测试的全面评估。我们利用MATHCHECK创建了MATHCHECK-GSM和MATHCHECK-GEO，分别针对数学文本推理和多模态推理能力进行评估，它们是GSM8k、GeoQA、UniGeo和Geometry3K等基准的升级版。我们使用MATHCHECK-GSM和MATHCHECK-GEO对超过20种LLM和11种多模态LLMs进行了评估，以检验它们的综合数学推理能力。结果显示，尽管前沿模型如GPT-4表现出色，但其他模型家族在清单上的表现显著下降。进一步实验表明，与传统数学基准相比，MATHCHECK更好地反映了真正的数学能力，线性度更高，从而支持我们的设计。通过MATHCHECK，我们可以轻松进行详细的行为分析，深入探究模型。|
|**2024-07-11**|**A Taxonomy for Data Contamination in Large Language Models**|Medha Palavalli et.al.|[2407.08716](http://arxiv.org/abs/2407.08716)|null|大型语言模型在基于广泛网络语料库的预训练后，在众多下游任务上展现出卓越性能。然而，数据污染问题日益引起关注，即评估数据可能存在于预训练数据中，导致模型表现虚高。去污染（decontamination）作为一种可能的解决方案，试图检测并移除这些污染数据。然而，污染数据可能源于测试集的修改版本，这使得检测变得困难。目前尚不清楚不同类型的污染如何影响语言模型在下游任务中的性能。我们提出了一种分类体系，对语言模型在预训练阶段遇到的各种污染类型进行划分，并确定了哪些类型的风险最高。我们通过分析总结和问答两个关键自然语言处理任务，揭示了不同类型污染如何影响模型在实际评估中的表现。|
|**2024-07-11**|**GTA: A Benchmark for General Tool Agents**|Jize Wang et.al.|[2407.08713](http://arxiv.org/abs/2407.08713)|**[link](https://github.com/open-compass/GTA)**|**人们普遍关注大型语言模型（LLMs）与各种工具的整合，以开发通用代理，但这对LLMs的工具使用能力提出了挑战。当前的评估方法存在明显缺陷，如使用AI生成的查询、单步骤任务、模拟工具以及仅限文本的交互，未能充分展示这些模型在实际问题解决中的能力。因此，我们提出GTA（通用工具代理基准），它包含三个关键特性：（1）真实的用户查询：由人类编写，具有简单的现实世界目标，但隐含了工具使用需求，要求LLMs能推理出合适的工具并规划解决方案步骤。（2）真实部署的工具：一个配备有感知、操作、逻辑和创新类工具的评估平台，用于评估模型的实际任务执行性能。（3）真实的多模态输入：包括空间场景图片、网页截图、表格、代码片段和打印/手写材料等，以贴近真实世界的场景。  我们设计了229个现实生活任务和可执行的工具链，来评估主流LLMs。实验结果显示，对于真实的用户查询，现有的LLMs面临严峻挑战，GPT-4完成的任务不足一半，大多数模型的成绩低于25%。这个评估揭示了当前LLMs在实际工具使用能力上的瓶颈，为提升通用工具代理的研究提供了方向。GTA的相关代码和数据集已可在<https://github.com/open-compass/GTA>获取。**|
|**2024-07-11**|**Live2Diff: Live Stream Translation via Uni-directional Attention in Video Diffusion Models**|Zhening Xing et.al.|[2407.08701](http://arxiv.org/abs/2407.08701)|null|大型语言模型因其单向时间注意力机制，在文本和音频流数据生成方面展现出卓越的效果。然而，尽管对实时视频处理的需求日益增长，但视频流处理的研究却相对较少。现有的视频扩散模型依赖双向时间注意力，这限制了它们处理直播视频的能力。为此，我们提出Live2Diff，这是首个专为实时视频翻译设计的具有单向时间注意力的视频扩散模型。与先前工作不同，我们的方法通过与前一帧及其少数预热帧相关联，保持了时间一致性和平滑性，无需考虑未来帧。同时，我们采用高效的降噪方案，包括KV缓存机制和流水线处理，以支持互动帧率下的视频流翻译。大量的实验结果表明，我们的注意力机制和流水线设计显著优于先前的方法，在保持时间平滑性和/或效率方面表现出色。|
|**2024-07-11**|**Mitigating Catastrophic Forgetting in Language Transfer via Model Merging**|Anton Alexandrov et.al.|[2407.08699](http://arxiv.org/abs/2407.08699)|null|随着开放型大型语言模型（LLMs）在英语任务中的性能不断提升，研究人员正致力于将其扩展到其他语言。然而，这种语言适应往往会导致基础模型能力的灾难性遗忘，限制了改编后模型的实用性。为此，我们提出了一种新的适应方法——Branch-and-Merge（BaM），它基于迭代地合并多个针对部分训练数据进行微调的模型。BaM的核心理念在于，这种方法产生的是幅度较小但质量更高的权重调整，从而减少对源领域的遗忘，同时保持对目标领域的学习。  我们在保加利亚语和德语的广泛实证研究中展示了BaM的优势：它能显著降低遗忘，同时在不同模型架构上与标准持续预训练和指令微调相比，能够匹配甚至提升目标领域的性能。|
|**2024-07-11**|**Cloud Atlas: Efficient Fault Localization for Cloud Systems using Language Models and Causal Insight**|Zhiqiang Xie et.al.|[2407.08694](http://arxiv.org/abs/2407.08694)|null|在现代云系统中，运行时故障和性能下降是常态。对于云服务提供商而言，自动确定问题的根本原因是保证高可靠性和可用性的关键，因为快速的故障定位有助于加快诊断和优先级排序，以实现及时解决。近期的研究中，因果推理利用因果图来捕捉不同云系统性能指标之间的关系是一个有前景的解决方案。然而，系统开发者需要精确定义系统的因果图，这是一项耗时、脆弱且挑战性的工作，尤其对于庞大和动态的系统，且需要深厚的专业知识。数据驱动的方法在云系统中的效果有限，因为故障事件的发生频率相对较低。  本工作中，我们提出了一种新颖的解决方案——Atlas，它能够自动合成云系统的因果图。Atlas利用大规模语言模型（LLMs）结合系统文档、日志和部署反馈生成因果图。Atlas与数据驱动的因果发现技术相辅相成，并通过数据驱动的验证步骤进行增强。我们在一系列故障定位场景中评估了Atlas，结果表明，Atlas能够在可扩展和普适的方式下生成因果图，其性能远超数据驱动算法，并与基准线相当。|
|**2024-07-11**|**SEED-Story: Multimodal Long Story Generation with Large Language Model**|Shuai Yang et.al.|[2407.08683](http://arxiv.org/abs/2407.08683)|**[link](https://github.com/tencentarc/seed-story)**|**随着图像生成和开放形式文本生成的显著进步，交错的图像-文本内容创作领域变得越来越有吸引力。多模态故事生成，即生成叙事文本与生动图像的交错序列，作为一种有价值的实用任务，因其广泛的应用前景而受到关注。然而，这一任务面临着理解文本和图像复杂交互、生成连贯且相关文本和视觉内容的挑战。本工作中，我们提出SEED-Story，这是一种新颖的方法，它利用强大的多模态大型语言模型（MLLM）来生成扩展的多模态故事。我们的模型基于MLLM的强大理解能力，既能预测文本令牌，也能预测视觉令牌，然后通过适应的视觉解令牌化器处理，生成具有一致角色和风格的图像。我们还引入了多模态注意力沉降机制，使得在高度自动递归的方式下，能够生成长达25个序列（仅用10个进行训练）的故事。此外，我们还提供了大规模高分辨率的StoryStream数据集，用于训练我们的模型，并量化评估多模态故事生成任务在多个方面的性能。**|
|**2024-07-11**|**Uncertainty Estimation of Large Language Models in Medical Question Answering**|Jiaxin Wu et.al.|[2407.08662](http://arxiv.org/abs/2407.08662)|null|## 任务  大型语言模型（LLMs）在医疗领域的自然语言生成方面展现出潜力，但存在产生错误事实的风险。为了在医疗问题解答中部署这些模型，需要可靠的不确定性估计（UE）方法来识别幻觉。本研究中，我们在医学问答数据集上对流行UE方法及其不同模型规模进行了评估。结果显示，当前方法在该领域通常表现不佳，凸显了医疗应用中的UE挑战。我们还观察到，更大的模型往往能获得更好的结果，这表明模型规模与UE可靠性可能存在关联。  为应对这些挑战，我们提出了一种名为“两阶段验证”的概率自由不确定性估计方法。首先，LLM生成逐步解释和初始答案，接着制定核查问题以检查解释中的事实陈述。模型会两次回答这些问题：一次独立，一次参考解释。两种答案之间的不一致度衡量原始响应的不确定性。我们在三个生物医学问答数据集上使用Llama 2 Chat模型评估我们的方法，并将其与基准基线方法进行比较。  实验结果显示，我们的两阶段验证方法在各个数据集和模型规模上实现了最佳的整体准确性和稳定性，并且其性能随模型大小的增加而提升。|
|**2024-07-10**|**Training on the Test Task Confounds Evaluation and Emergence**|Ricardo Dominguez-Olmedo et.al.|[2407.07890](http://arxiv.org/abs/2407.07890)|**[link](https://github.com/socialfoundations/training-on-the-test-task)**|**我们研究了一个大型语言模型评估中的核心问题，称为在测试任务上训练。这并非如数据泄露或污染等不当做法，而是一种逐渐增长的包括任务相关数据在预训练阶段的技术。我们发现，在测试任务上训练会混淆模型的相对评估和关于涌现能力的声明。我们提出，不同模型家族之间的看似优势可能由他们在测试任务上的训练程度差异所解释。为此，我们提出了一种有效方法，即在比较前对每个模型进行相同的任务相关数据微调，以校正这种训练。结果显示，一旦调整了在测试任务上的训练，涌现行为的实例大多消失。同样适用于那些无法用评价指标解释的涌现行为报告案例。我们的工作推动了对大型语言模型的新评价视角，对基准测试和涌现能力研究具有广泛影响。**|
|**2024-07-10**|**Towards Robust Alignment of Language Models: Distributionally Robustifying Direct Preference Optimization**|Junkang Wu et.al.|[2407.07880](http://arxiv.org/abs/2407.07880)|**[link](https://github.com/junkangwu/dr_dpo)**|**本研究关注在训练数据中噪声对Direct Preference Optimization (DPO)方法的挑战，该方法用于调整大型语言模型（LLMs）以符合人类偏好。我们区分了两类噪声：点噪声，涉及低质量的数据点；和成对噪声，影响偏好的正确排序。通过分布式鲁棒优化（DRO），我们增强了DPO抵抗这些噪声的能力。理论分析揭示，DPO本质上蕴含了DRO原理，对点噪声具有天然的鲁棒性，其中正则化系数 $\beta$在抗噪声方面起关键作用。在此基础上，我们提出分布式鲁棒增强的DPO（Dr. DPO），它通过优化最坏情况的成对场景来集成成对鲁棒性。Dr. DPO中的新超参数$\beta'$ 允许对数据对可靠性进行精细控制，平衡了在嘈杂训练环境中的探索与利用。实证评估显示，Dr. DPO显著提高了生成文本的质量和响应准确性，无论在有噪声还是无噪声的设置下都表现出色。代码已在https://github.com/junkangwu/Dr_DPO上提供。**|
|**2024-07-10**|**FACTS About Building Retrieval Augmented Generation-based Chatbots**|Rama Akkiraju et.al.|[2407.07858](http://arxiv.org/abs/2407.07858)|null|随着生成式人工智能驱动的企业聊天机器人日益成为提升员工生产力的关键工具，基于检索增强生成（RAG）的、大型语言模型（LLMs）以及如Langchain和Llamaindex之类的orchestration框架在构建这些聊天机器人中扮演了重要角色。然而，创建有效的企业聊天机器人是一项挑战，需要精心设计的RAG管道工程。这包括微调嵌入和LLMs、从向量数据库提取文档、重述查询、重新排名结果、设计提示、遵守文档访问控制、提供简洁的回答、包含引用、保护个人信息以及构建orchestration代理。我们基于三个NVIDIA聊天机器人（分别用于IT/HR福利、财务收益和通用内容）的经验，提出了一种构建RAG聊天机器人的框架——FACTS（Freshness、Architectures、Cost、Testing、Security）。我们的贡献有三方面：首先介绍FACTS框架，其次列出十五个RAG管道控制点，最后提供了关于大模型和小模型在准确性和延迟之间权衡的实证结果。据我们所知，这是首篇全面探讨构建安全企业级聊天机器人的方法和解决方案的论文。|
|**2024-07-10**|**OpenDiLoCo: An Open-Source Framework for Globally Distributed Low-Communication Training**|Sami Jaghouar et.al.|[2407.07852](http://arxiv.org/abs/2407.07852)|**[link](https://github.com/PrimeIntellect-ai/OpenDiLoCo)**|**OpenDiLoCo是一个开源的分布式低通信（DiLoCo）训练方法的实现和复制，针对大型语言模型。我们提供了可复现的DiLoCo实验，通过Hivemind库构建了一个可扩展的去中心化训练框架。我们在两个大洲和三个国家之间训练模型，同时保持90-95%的计算资源利用率。此外，我们进行了关于算法计算效率、工作器数量可扩展性的研究，并表明其梯度可以使用FP16进行全归一化而不会影响性能。最后，我们将OpenDiLoCo扩展到原始工作的三倍规模，证明了它在百亿参数模型上的有效性。**|
|**2024-07-10**|**Natural Language Mechanisms via Self-Resolution with Foundation Models**|Nicolas Della Penna et.al.|[2407.07845](http://arxiv.org/abs/2407.07845)|null|在实际操作中，代理人通常受限于诸如交易或订单之类的有限报告格式，这可能限制了他们表达信息的能力。我们提出了一种新型机制，它促使代理人以自然语言提交报告，并利用大型语言模型（LLM）的强大功能来选择结果和分配报酬。我们确定了这些机制在LLM作为良好的世界模型以及强烈的跨代理信息过度确定条件下的激励兼容性和效率的必要条件。实验表明，当传统预测市场在信号结构上存在问题时，这些基于LLM的机制能够成功地整合信息。|
|**2024-07-10**|**Transformer Alignment in Large Language Models**|Murdock Aubry et.al.|[2407.07810](http://arxiv.org/abs/2407.07810)|null|大型语言模型（LLMs）在自然语言处理方面取得了显著进步，深入理解其内部机制至关重要。我们视LLMs为高维空间中的离散、耦合的非线性动力系统，通过研究tokens在Transformer块中的轨迹，并沿着这些轨迹线性化系统，利用雅可比矩阵进行分析。在对38个公开可用的LLMs进行研究后，我们观察到残差雅可比矩阵的上左和右奇异向量之间的对齐，以及线性性和层内指数增长的出现。值得注意的是，我们发现对齐度的提高与模型性能呈正相关。训练后的评估显示，相比于随机初始化权重时的指标，有显著改善，这强调了训练在Transformer架构中的重要影响。这些发现揭示了一种以前未被充分认识的规律性，强化了动力学解释，并为进一步理解和优化LLM架构铺平了道路。|
|**2024-07-10**|**Attribute or Abstain: Large Language Models as Long Document Assistants**|Jan Buchmann et.al.|[2407.07799](http://arxiv.org/abs/2407.07799)|**[link](https://github.com/ukplab/arxiv2024-attribute-or-abstain)**|**## 背景 大语言模型（LLMs）能够辅助处理长篇文档，但它们也存在胡言乱语的问题。增加可信度的方法是通过提供证据支持响应，提高可验证性。当前的归因方法仅在基于检索的生成（RAG）环境中评估过，这与无需检索的长文档场景不同，可能仍有应用价值。因此，缺乏针对长文档的归因专门评估。为此，我们提出LAB，一个包含6个多样化的长文档任务的基准，并在四种不同大小的LLM（即提示和微调）上试验了不同的归因方法。研究结果显示，一步生成引用（citation，即同时进行响应生成和证据提取）的表现最佳。我们还探究了“迷失在中间”现象是否适用于归因，但未发现这种情况。此外，我们发现证据质量在简单响应的场景下可以预测响应质量，但对于复杂响应则不然，因为模型在为复杂主张提供证据时面临挑战。我们公开了代码和数据，以供进一步研究。**|
|**2024-07-11**|**Evaluating Large Language Models with Grid-Based Game Competitions: An Extensible LLM Benchmark and Leaderboard**|Oguzhan Topsakal et.al.|[2407.07796](http://arxiv.org/abs/2407.07796)|**[link](https://github.com/research-outcome/llm-game-benchmark)**|**我们提出了一种新颖且可扩展的大型语言模型（LLM）基准测试，通过网格型游戏如井字棋、连接四和围棋进行。开源的游戏模拟代码在GitHub上提供，允许LLMs竞技，并生成JSON、CSV、TXT和PNG格式的详细数据文件，用于排行榜排名和进一步分析。我们展示了包括Anthropic的Claude 3.5 Sonnet和Claude 3 Sonnet，Google的Gemini 1.5 Pro和Gemini 1.5 Flash，OpenAI的GPT-4 Turbo和GPT-4o，以及Meta的Llama3-70B在内的领先LLM之间的比赛结果。我们鼓励其他LLM提交结果。总共进行了2,310场模拟比赛（每对模型进行5轮，共7个模型间的对局，以及与随机玩家的比赛），涵盖三种类型的游戏，使用了列表、插图和图像三种提示方式。结果显示，LLM在不同游戏和提示类型下的性能存在显著差异，分析内容包括胜率、错失机会和无效动作。排行榜和结果矩阵的详细数据作为开放访问数据在GitHub上提供。这项研究加深了我们对LLM在未专门训练的游戏中的能力的理解，有助于评估它们的规则理解能力和战略思维。在通向人工智能通用性的道路上，这项研究为未来探索它们在复杂决策场景中的实用性奠定了基础，揭示了它们的战略思考能力，并为深入探究LLM在基于游戏框架内的局限性提供了方向。**|
|**2024-07-10**|**Flooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent Communities**|Tianjie Ju et.al.|[2407.07791](http://arxiv.org/abs/2407.07791)|**[link](https://github.com/Jometeorie/KnowledgeSpread)**|**随着大型语言模型（LLMs）在多代理系统中的迅速应用，它们在协作问题解决和自主谈判等领域的出色性能引起了关注。然而，这些基于LLM的多代理系统的安全问题尚未得到充分研究，尤其是在知识操纵传播方面。本文通过构建详细的威胁模型和模拟环境，模拟现实世界中的多代理部署在可信平台上，探讨这一关键问题。我们提出了一种新颖的两阶段攻击方法，包括说服性注入和操纵知识注入，来系统地探究在无明确提示操纵的情况下，如何潜在地传播操纵知识（如虚构和有害知识）。我们的方法利用了LLMs处理世界知识固有的漏洞，攻击者可以借此无意识地传播编造的信息。实验结果表明，我们的攻击方法能够成功诱导基于LLM的代理在交流中传播这两种操纵的知识，同时不会显著降低它们的基础功能。此外，我们发现这些操纵会持续存在于流行的检索增强生成框架中，即使交互结束，若干良性代理也可能继续受到操纵聊天记录的影响。我们的发现揭示了LLM多代理系统中的重大安全风险，强调了对操纵知识传播进行强大防御的迫切需求，比如引入“守护”代理和先进的事实核查工具。**|
|**2024-07-10**|**WorldAPIs: The World Is Worth How Many APIs? A Thought Experiment**|Jiefu Ou et.al.|[2407.07778](http://arxiv.org/abs/2407.07778)|null|本文探讨了在物理环境中部署人工智能（AI）代理时所需的基本操作（API）数量和设计问题。研究者设想，如果wikiHow教程涵盖了广泛的用户自编任务，那么这些任务所需的API范围是什么。他们提出了一种方法，通过将wikiHow指令与置身于环境中的代理策略关联，迭代地生成新的API。借助大型语言模型（LLMs）在体感规划方面的最新成就，研究者提议使用少量样例提示GPT-4生成Python代码作为代理策略，并通过以下步骤扩展API库：1）重用初始API集；2）在必要时创建新的API调用。实验关注的是定义API，而非其实现性。在一小部分wikiHow教程上应用该方法后，发现需要300多个API来捕捉现实世界中的多样任务。自动和人工分析显示，提出的管道能有效复用和创造API。进一步的人工审查发现，现有的模拟器仅支持诱导出的API的一小部分（前50个常用API中的9个），这促使开发更丰富的体感环境。|
|**2024-07-09**|**AnyTaskTune: Advanced Domain-Specific Solutions through Task-Fine-Tuning**|Jiaxi Cui et.al.|[2407.07094](http://arxiv.org/abs/2407.07094)|**[link](https://github.com/pandavt/datatager)**|**在各行各业广泛采用大型语言模型（LLMs）的过程中，往往忽视了个体和小型组织对针对其特定业务场景定制化模型的需求。为此，我们提出了一种新颖的微调方法——\textbf{AnyTaskTune}，即任务微调（Task-Fine-Tune），旨在提升模型在多样化的领域特定任务上的性能。该方法包括细致地识别和定义领域内的子任务，随后创建专门的增强数据集进行精细调整，从而优化任务特定的模型表现。我们在法律（如关键词提取和句子预测）等多个领域，包括金融、医疗、法律、心理学、客户服务和人力资源等二十多个子任务上进行了广泛的微调实验。为了支持社区参与并分享资源，我们将开源这些双语任务数据集。实验结果显示，使用\textbf{Task-Fine-Tune}方法微调的模型不仅在特定任务上表现出色，而且在各自领域内明显优于通用能力更强的模型。我们的工作已公开发布在：\url{https://github.com/PandaVT/DataTager}。**|
|**2024-07-09**|**FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive Distillation**|Liqun Ma et.al.|[2407.07093](http://arxiv.org/abs/2407.07093)|**[link](https://github.com/liqunma/fbi-llm)**|**该研究介绍了一种全新的全二进制大型语言模型（FBI-LLM），这是首次展示如何从头开始训练大规模的全二进制语言模型（不同于部分二进制或三进制的LSTM，如BitNet b1.58），其性能能够与浮点16位（FP16）或混合精度16位（BF16）的常规大语言模型相当。通过使用自回归蒸馏（AD）损失，同时保持模型尺寸（130M、13B、7B）和预训练数据量与常规LLM相当，FBI-LLM在困惑度和任务特定效果方面表现出竞争性。有趣的是，我们发现从零开始训练全二进制语言模型并不需要预训练权重。这项工作催生了一个新的计算框架，并可能推动针对完全1比特LLMs的专业硬件设计。我们公开所有模型、代码和训练数据，以支持进一步的研究（代码：https://github.com/LiqunMa/FBI-LLM，模型：https://huggingface.co/LiqunMa/）。**|
|**2024-07-09**|**Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks with Large Language Models**|Logan Cross et.al.|[2407.07086](http://arxiv.org/abs/2407.07086)|**[link](https://github.com/locross93/hypothetical-minds)**|**在多智能体强化学习（MARL）方法中，处理多智能体系统的非stationarity并适应在线学习的能力是一个挑战。为此，我们利用大型语言模型构建了一个自主的解决策略。我们的新型智能体“假设心智”（Hypothetical Minds）采用认知启发式架构，包括感知、记忆和两个抽象层次上的分层规划模块。关键新增的是“心理理论”模块，它以自然语言的形式生成对其他智能体策略的假设，并通过验证这些假设对其他智能体行为的预测准确性来逐步优化。在Melting Pot基准的多种竞争、混合动机和协作环境中，假设心智显著优于先前的语言模型智能体和强化学习基线，无论是在二元环境还是群体环境中。对比分析显示，假设的评估和迭代精炼对于应对复杂场景至关重要。**|
|**2024-07-09**|**Adapting LLMs to Hebrew: Unveiling DictaLM 2.0 with Enhanced Vocabulary and Instruction Capabilities**|Shaltiel Shmidman et.al.|[2407.07080](http://arxiv.org/abs/2407.07080)|null|该论文探讨了在希伯来等低资源语言中训练大型语言模型（LLMs）的挑战。我们介绍了DictaLM2.0和DictaLM2.0-Instruct，这两个模型基于Mistral模型，使用大约2000亿个希伯来语和英语词汇进行训练。适应预训练模型到新语言需要专门的技术，这与从头训练或在资源丰富的语言（如英语）上进一步训练现有模型有显著差异。论文详细阐述了这些创新的训练方法，以促进希伯来语的高效学习和适应其语言特性。此外，我们还对DictaLM2.0-Instruct进行了全面的指令微调，以提升其在任务导向指令上的性能。为了严格评估我们的模型，我们开发了一个新的希伯来LLM评估基准，涵盖了问答、情感分析、Winograd Schema Challenge、翻译和摘要等多个任务。本文不仅解决了在低资源语言中训练LLMs的复杂性，还提出了一种可用于其他LLM跨非英语语言适应的框架，从而对多语言自然语言处理领域做出了贡献。|
|**2024-07-09**|**Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps**|Yung-Sung Chuang et.al.|[2407.07071](http://arxiv.org/abs/2407.07071)|**[link](https://github.com/voidism/lookback-lens)**|**该论文探讨了大型语言模型（LLMs）在总结文章或根据给定段落回答问题时可能出现的语境性虚构问题。LLMs可能会杜撰细节，提供与输入上下文不符的不准确答案。研究者提出，这种虚构与模型倾向于关注上下文信息还是自动生成内容的程度有关。为此，他们设计了一个简单的检测模型——“Lookback Lens”，其输入特征是基于每个注意力头上下文注意力权重与新生成词的比例。实验表明，仅使用这些回顾比率特征的线性分类器与利用LLM整个隐藏状态或文本蕴含模型的更复杂检测器同样有效。Lookback Lens不仅适用于不同任务，还能跨模型迁移，一个在70亿参数模型上训练的检测器无需重新训练即可应用于更大的130亿参数模型。此外，研究还发现，通过简单的分类器指导解码方法，能够减少诸如XSum摘要任务中的虚构程度，例如降低9.6%的虚构发生率。**|
|**2024-07-09**|**Prompting Techniques for Secure Code Generation: A Systematic Investigation**|Catherine Tony et.al.|[2407.07064](http://arxiv.org/abs/2407.07064)|null|## 概要  随着大型语言模型（LLMs）在软件开发中的兴起，通过提示驱动编程，开发者能够通过自然语言（NL）指令生成代码。然而，关于它们能否产生安全代码的研究引发了质疑，这关系到提示生成软件的质量。尽管已经出现了多种精心设计的提示策略以优化LLM的响应，但这些方法与安全代码生成之间的相互作用仍需进一步研究。目标：本研究旨在探究不同提示技术对LLMs根据NL指令生成代码的安全性影响。方法：首先，我们进行系统文献回顾，以识别适用于代码生成任务的现有提示技术。然后，我们在GPT-3、GPT-3.5和GPT-4模型上评估这些技术中的部分，使用一个包含150个与安全相关的代码生成NL提示的数据集。结果：我们的工作（1）对代码生成的潜在提示技术进行了分类，（2）适应并评估了这些技术在安全代码生成任务中的表现，（3）观察到在测试的LLMs中，尤其是在使用了名为“递归批评与改进”（RCI）的现有技术后，安全漏洞有所减少，为LLM生成代码安全性的讨论提供了有价值的见解。|
|**2024-07-09**|**Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence**|Weize Chen et.al.|[2407.07061](http://arxiv.org/abs/2407.07061)|**[link](https://github.com/openbmb/ioa)**|**随着大型语言模型的迅速发展，出现了能效卓越的自主代理。然而，现有的多代理框架在整合来自不同生态系统的高能力第三方代理时面临挑战，通常局限于自身封闭环境。它们在模拟分布式环境时也受限于单设备设置，并且往往依赖硬编码的通信管道，难以适应任务需求的变化。受互联网理念启发，我们提出了一种名为“代理互联网”（Internet of Agents，IoA）的新框架。IoA旨在解决这些问题，提供一个灵活且可扩展的平台，促进基于语言模型的多代理协作。它引入了代理集成协议、即时消息架构以及动态的团队协作和对话流程控制机制。通过在通用助手任务、体感AI任务和检索增强生成基准上的广泛实验，我们证明IoA在性能上持续优于现有最先进的基线，展示了其在异构代理之间有效合作的能力。IoA代表了朝着将多样化的代理链接在一个类似互联网的环境中迈进，让它们能够无缝协作以提升整体智能和功能。我们的代码库已发布在：\url{https://github.com/OpenBMB/IoA}。**|
|**2024-07-09**|**Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model**|Wenqi Zhang et.al.|[2407.07053](http://arxiv.org/abs/2407.07053)|**[link](https://github.com/zwq2018/multi-modal-self-instruct)**|**尽管当前的大型多模态模型（LMMs）已经能够理解自然场景的照片和肖像，但它们对抽象图像（如图表、地图或布局）的理解以及视觉推理能力仍然相当初级。它们在处理日常任务时常常遇到困难，例如阅读时钟时间、理解流程图或根据路线图规划路径。鉴于此，我们设计了一个多模态自我指导系统，利用大型语言模型及其代码能力来生成大量的抽象图像和日常场景下的视觉推理指令。我们的方法轻松创建了一个多模态基准，包含11,193个指令，涵盖八个视觉场景：图表、表格、模拟地图、仪表板、流程图、关系图、楼层平面图和视觉谜题。  这个由简单线条和几何元素构成的基准揭示了最先进的LMM（如Claude-3.5-Sonnet和GPT-4o）在抽象图像理解、空间关系推理和视觉元素识别方面的局限性。此外，为了验证合成数据的质量，我们使用62,476条合成的图表、表格和路线图指令对LMM进行微调。结果显示，图表理解和地图导航性能得到了提升，同时也表明这对其他视觉推理任务可能具有潜在益处。我们的代码已在以下链接提供：\url{https://github.com/zwq2018/Multi-modal-Self-instruct}。**|
|**2024-07-09**|**Using Large Language Models for Generating Smart Contracts for Health Insurance from Textual Policies**|Inwon Kang et.al.|[2407.07019](http://arxiv.org/abs/2407.07019)|null|我们研究利用大型语言模型（LLMs）自动生成基于文本的健康保险政策的自动化代码，目标是区块链智能合约。智能合约因其不可变性、可验证性、扩展性和无需预设信任的特性而被选中。我们的方法按技术复杂度递增生成输出：（1）文本摘要，（2）声明式决策逻辑，以及（3）带有单元测试的智能合约代码。我们确认LLMs在任务（1）上表现出色，而结构化的输出有助于验证任务（2）和（3）。声明式语言常用于规范医疗政策，但在区块链上的执行较为复杂，因此任务（3）旨在直接通过智能合约自动实现这一过程。我们提出完整性、正确性、清晰度、语法和功能性代码作为评估指标。我们使用了来自Medicare官方手册的三个具有不同难度的保险政策场景进行评估，涉及GPT-3.5 Turbo、GPT-3.5 Turbo 16K、GPT-4、GPT-4 Turbo和CodeLLaMA等模型。结果显示，LLMs在生成文本摘要方面表现良好。尽管任务（2）到（3）的输出可以作为起点，但它们仍需人工审核：在某些情况下，即使“可运行”的代码也可能产生不正确的结果；目标语言的流行程度会影响输出质量；更复杂的场景仍是当前的一大挑战。然而，我们的实验展示了LLMs在将文本流程描述转化为智能合约方面的潜力。|
|**2024-07-09**|**End-To-End Causal Effect Estimation from Unstructured Natural Language Data**|Nikita Dhawan et.al.|[2407.07018](http://arxiv.org/abs/2407.07018)|null|了解干预措施的效果对人类决策至关重要。然而，当前因果效应估计方法依赖于手动收集和结构化数据，这导致研究成本增加、完成时间延长。我们展示了如何利用大型语言模型（LLMs）开采大规模、多样化的观察性文本数据，以在适当的因果假设下生成低成本的因果效应估计。我们提出NATURAL，一个基于LLMs的新型因果效应估计算法家族，适用于处理未结构化的文本数据。我们的方法利用LLMs的条件分布（针对感兴趣的变量，根据文本数据）辅助计算经典的因果效应估计。我们克服了一系列技术挑战，如自动化数据整理和使用LLMs填补缺失信息。  我们准备了六个（两个合成的和四个实际的）观察性数据集，并配以随机对照试验形式的真实标签，系统地评估了我们管道中的每一步。NATURAL估计算法表现出色，其结果与真实值的差距不超过3个百分点，包括在实际的三期和四期临床试验中。这些结果表明，未结构化的文本数据是因果效应信息的丰富来源，NATURAL是利用这一资源的自动化流程的第一步。|
|**2024-07-08**|**Video-STaR: Self-Training Enables Video Instruction Tuning with Any Supervision**|Orr Zohar et.al.|[2407.06189](http://arxiv.org/abs/2407.06189)|**[link](https://github.com/orrzohar/Video-STaR)**|**大型视觉语言模型（LVLM）的性能与其训练数据的规模和质量密切相关。当前的视频指令调优数据集缺乏多样性，因为它们主要由提示大型语言模型生成视频字幕以形成问题-答案对，内容多为描述性。然而，许多带有丰富标签和监督的视频数据集已经存在，但如何将它们融入LVLM并非易事。  为此，我们提出了视频自我训练与增强推理（Video Self-Training with augmented Reasoning，简称Video-STaR），这是首个视频自我训练方法。Video-STaR使得任何标注的视频数据集都能用于视频指令调优。在这个过程中，LVLM在生成指令和微调之间循环。我们发现，这不仅能提升视频整体理解能力（I），还能让LVLM适应新的下游任务，利用现有监督进行学习。  具体来说，LVLM被提示提出一个答案，然后仅保留那些包含原始视频标签的答案。LVLM随后在生成的数据集上进行再训练。通过只在包含正确视频标签的生成答案上训练，Video-STaR利用现有的视频标签作为弱监督来指导视频指令调优。  实验结果显示，经过Video-STaR增强的LVLM在（I）一般视频问答任务中的表现提升了10%，在（II）下游任务中，Video-STaR提高了Kinetics700-QA的准确性20%，以及FineDiving动作质量评估的性能15%。总的来说，Video-STaR为LVLM的性能提升提供了一种有效且实用的方法。**|
|**2024-07-08**|**CrowdMoGen: Zero-Shot Text-Driven Collective Motion Generation**|Xinying Guo et.al.|[2407.06188](http://arxiv.org/abs/2407.06188)|null|在娱乐行业（如动画和游戏）以及战略领域（如城市模拟和规划）中，人群运动生成至关重要。然而，这一任务需要精细地融合控制与生成，以在特定的空间和语义约束下实现逼真的群体动态合成，其挑战尚未得到充分探索。当前的人体动作生成模型往往关注个体行为，忽视了集体行为的复杂性；而多个人体动作生成的最新方法严重依赖预设场景，且限于固定、少量的人际互动，限制了其实用性。  为解决这些问题，我们提出CrowdMoGen，一个零样本文本驱动的框架，它利用大型语言模型（LLM）的力量，将集体智慧融入运动生成框架，从而能够在没有配对训练数据的情况下实现通用的规划和群体运动生成。我们的框架主要由两个关键组件构成：1）人群场景规划器，学习根据特定场景上下文或引入的扰动协调运动和动态；2）集体运动生成器，根据整体计划高效合成所需的集体运动。大量的定量和定性实验验证了我们框架的有效性，它不仅填补了大规模和通用人群运动生成任务的重要空白，而且在真实感和灵活性方面表现出高水准。|
|**2024-07-08**|**On Speeding Up Language Model Evaluation**|Jin Peng Zhou et.al.|[2407.06172](http://arxiv.org/abs/2407.06172)|null|大型语言模型（LLMs）在自然语言处理（NLP）领域占据主导地位，它们在各种任务上表现出最先进的能力。从训练到推理，构建这样的模型涉及众多决策，形成一个复杂的搜索问题。例如，为了为特定任务找到最佳的预训练LLM、提示或超参数，通常需要对整个测试集中的多个候选方案进行全面评估。这种详尽的评估耗时且昂贵，因为LLMs的推理和度量计算需求高。  本文针对在有限预算内有效评估方法在测试样本上的性能这一挑战。我们利用了广泛研究的多臂老虎机框架，该框架通过顺序选择下一个要评估的方法-示例对，将我们的方法——结合多臂老虎机算法与低秩分解——显著减少了所需的资源。实验表明，我们的算法仅使用通常需求的5%-15%资源，就能识别出表现最好的方法，从而实现了高达85%-95%的成本节省。|
|**2024-07-08**|**What's Wrong with Your Code Generated by Large Language Models? An Extensive Study**|Shihan Dou et.al.|[2407.06153](http://arxiv.org/abs/2407.06153)|null|随着大型语言模型（LLMs）在代码生成领域的快速发展，研究人员对此的关注度日益提高。目前的研究主要集中在构建高质量数据集和采用多样化的训练技术来提升LLM的代码生成能力。然而，对于这些现有方法的局限性和边界，缺乏全面的研究探讨。为此，我们进行了一项详尽的实证研究，评估了三个领先闭源LLM和四个开源LLM在三个常用基准上的性能。研究考察了生成代码的长度、循环复杂度和API数量，结果显示这些模型在处理更复杂的编程问题时面临挑战，生成的代码往往较短但结构更复杂，与标准解决方案相比。  我们还创建了一个错误代码的分类体系，分为三个类别和12个子类别，分析常见错误类型的根源。为了检验LLMs在实际项目中的表现，我们亲手构建了一个包含140个代码生成任务的现实世界基准。对比分析显示，实际场景中的bug分布与现有基准存在显著差异。最后，我们提出了一种无需额外训练的迭代方法，引入自我批判机制，使LLMs能够根据bug类型和编译器反馈修正其生成的代码。实验结果表明，经过两次迭代后，我们的方法能显著减少错误，使通过率提高29.2%，这表明LLMs在处理复杂问题方面具有巨大潜力。|
|**2024-07-09**|**Using Grammar Masking to Ensure Syntactic Validity in LLM-based Modeling Tasks**|Lukas Netz et.al.|[2407.06146](http://arxiv.org/abs/2407.06146)|null|我们介绍并评估了一种名为“语法遮盖”的方法，该方法用于引导大型语言模型（LLMs）在给定上下文无关文法的约束下生成语法正确的模型。尽管少量示例学习或提示引导等prompt工程方法可以提高LLMs生成正确语法的概率，但处理复杂文法时，这些方法往往耗时且效果不理想。当前的研究主要集中在语言模型训练或prompt工程上。本文提出了一种新方法，通过约束解码限制输出，确保生成的内容符合有效语法。我们利用MontiCore构建的多种领域特定语言（DSL）和多款LLMs进行实验，比较了使用和未使用约束解码的效果。同时，我们采用相应的解析器验证每种模型的句法准确性。实验结果显示，语法遮盖显著提升了多个LLMs的建模能力，减少了对精心设计提示的需求，提高了生成正确模型的可能性。|
|**2024-07-08**|**ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation**|Ethan Chern et.al.|[2407.06135](http://arxiv.org/abs/2407.06135)|**[link](https://github.com/gair-nlp/anole)**|**## 背景 先前的开源大型多模态模型（LMMs）存在一些局限性：（1）它们往往缺乏原生集成，需要适配器来衔接视觉表示与预训练的大型语言模型（LLMs）；（2）许多模型仅限于单模态生成；（3）尽管有些支持多模态生成，但它们依赖于单独的扩散模型处理视觉部分。为了克服这些问题，我们介绍了Anole，一个开源的、自回归的、原生的大型多模态模型，专为交错的图像-文本生成设计。我们基于Meta AI的Chameleon构建Anole，采用了一种既数据高效又参数高效的创新微调策略。Anole展示了高质量、连贯的多模态生成能力。我们已经公开了我们的模型、训练框架以及指令调优数据。**|
|**2024-07-08**|**Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization**|Hannah K. Bako et.al.|[2407.06129](http://arxiv.org/abs/2407.06129)|**[link](https://github.com/hdi-umd/semantic_profiling_llm_evaluation)**|**### 概述  自动根据人类对数据集的口头描述生成数据可视化图表，需要深度理解语言中的语义信息，包括对数据属性、可视化任务以及数据预处理步骤的隐含和明确提及。自然语言界面（NLIs）在数据可视化方面已经探讨了如何捕捉这些信息，但人类言语的不确定性带来了挑战。近期的大型语言模型（LLMs）为解决这些问题提供了可能，但它们提取相关语义信息的能力尚待探索。本研究评估了四款公开可用的LLMs（GPT-4、Gemini-Pro、Llama3和Mixtral），分析它们在面对不确定性时理解口头指令的能力，并识别数据上下文和视觉任务。研究结果显示，LLMs对口语中的不确定性很敏感，能够提取关键的数据背景信息。然而，它们在推断可视化任务方面表现欠佳。基于这些发现，我们提出了未来利用LLMs进行可视化生成的研究方向。**|
|**2024-07-08**|**Depression Detection and Analysis using Large Language Models on Textual and Audio-Visual Modalities**|Avinash Anand et.al.|[2407.06125](http://arxiv.org/abs/2407.06125)|null|抑郁症被广泛认为是重大的公共卫生问题，严重影响个人的心理健康。未经诊断的抑郁症可能导致严重的健康问题，包括生理症状甚至自杀。通常，抑郁症的诊断依赖于临床医生和心理健康专业人员进行的结构化访谈和如Patient Health Questionnaire（PHQ）等问卷调查。然而，这在很大程度上依赖于医生的经验和判断，可能受到个人偏见的影响。由于抑郁症的成因仍在研究中，医生在识别和治疗初期阶段的抑郁症时面临挑战。  近期，人工智能神经计算在文本、图像和语音处理等领域取得了显著进展。我们的研究尝试利用这些最先进的模型，在E-DAIC（Extended Distress Analysis Interview Corpus Wizard of Oz）数据集和2019年Audio/Visual Emotion Challenge（AVEC）中进行实验，以期优化多模态结果。实验结果显示，我们提出的解决方案利用专有和开源大型语言模型（LLMs），在文本模态上的Root Mean Square Error（RMSE）得分达到3.98，优于AVEC 2019挑战的基线和当前最佳的回归分析架构。此外，我们的方法在分类任务中的准确性达到了71.43%。论文还介绍了一个新颖的音频-视觉多模态网络，其预测PHQ-8评分的RMSE为6.51。|
|**2024-07-08**|**Artificial Intuition: Efficient Classification of Scientific Abstracts**|Harsh Sakhrani et.al.|[2407.06093](http://arxiv.org/abs/2407.06093)|null|## 背景 为了获取战略洞见或进行科研项目管理，对简短的科学文本（如研究基金申请书或出版物摘要）进行粗粒度分类至关重要。这些文本向具备深厚专业知识的专家传达密集信息，但自动化的任务极其艰巨，因为篇幅有限且缺乏上下文。为此，我们开发了一种新方法来生成并准确分配特定领域的粗标签。研究表明，大型语言模型（LLM）能够提供任务所需的元数据，类似于增强人类直觉的补充知识，并提出了一个工作流程。作为初步实验，我们使用了美国国家航空航天局（NASA）的奖项摘要数据库。我们结合现有性能指标，开发了新的评估工具。|
|**2024-07-08**|**Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models**|Jinliang Lu et.al.|[2407.06089](http://arxiv.org/abs/2407.06089)|null|随着大型语言模型（LLMs）的显著成功，自然语言处理（NLP）研究进入了新时代。尽管这些模型各有所长，但训练在不同语料库上的LLMs表现出不同的优势和劣势，这给提高整体效率和灵活性带来了挑战。为了应对这些挑战，近期的研究探索了LLMs的协作策略。本文全面概述了这一新兴研究领域，强调了合作背后的动力。我们将协作策略主要分为三种方法：合并、集成和协作。合并是将多个LLMs的参数空间整合。集成则是结合多个模型的输出。协作利用不同LLMs的优势，使其在特定任务中发挥各自专长。我们将从不同角度详细介绍这些方法，并讨论其潜在应用。此外，我们还勾勒出未来的研究方向，期望本工作能激发更多关于LLMs协作的研究，推动高级NLP应用的发展。|
|**2024-07-05**|**Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs**|Rudolf Laine et.al.|[2407.04694](http://arxiv.org/abs/2407.04694)|**[link](https://github.com/lrudl/sad)**|## 背景  人工智能助手，如ChatGPT，在被训练时会回应用户：“我是一个大型语言模型”。这引发了一个问题：这些模型是否真的知道自己是LLMs，并能据此可靠地行动？它们是否了解自己当前的部署情况，例如面向公众？我们称之为模型的“情境意识”。为了量化大型语言模型（LLMs）的情境意识，我们设计了一套行为测试，基于问答和指令执行，这就是**情境意识数据集（Situational Awareness Dataset，简称SAD）**。该基准包括7个任务类别，超过13,000个问题，测试了多项能力，如识别自身生成的文本、预测自己的行为、分辨提示来自内部评估还是实际应用，以及遵循依赖自我认知的指令。  我们对16种LLMs在SAD上的性能进行了评估，包括基础（预训练）模型和聊天模型。尽管所有模型的表现都优于随机猜测，但最高分的模型（Claude 3 Opus）在某些任务上仍远未达到人类水平。此外，我们发现SAD的表现与通用知识指标（如MMLU）的相关性并不完全一致。聊天模型，经过针对性训练以作为AI助手，相对于基础模型在SAD上的表现更好，但在通用知识任务上则不然。SAD的目标是通过分解成可量化的能力，促进科学界对LLMs情境意识的理解。情境意识对于增强模型的自主规划和行动能力至关重要，这既有利于自动化，也带来了与AI安全和控制相关的全新风险。您可以在<https://situational-awareness-dataset.org>获取代码和最新结果。|
|**2024-07-05**|**ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models**|Yuzhe Gu et.al.|[2407.04693](http://arxiv.org/abs/2407.04693)|**[link](https://github.com/open-compass/anah)**|## 任务  大型语言模型（LLMs）在跨领域和广泛应用的长格式问答任务中会出现幻觉。当前的幻觉检测和缓解数据集在领域覆盖和规模上存在局限，由于劳动成本高昂且现有幻觉标注员的可靠性不足，难以实现规模化。为了推动对LLMs幻觉的可扩展监督，本文提出了一种迭代的自我训练框架。该框架通过期望最大化（EM）算法，每次迭代首先使用一个幻觉标注流程来标记扩大的数据集，然后用这个更准确的标注器对数据集进行训练。在下一轮迭代中，使用新的标注器更新幻觉标注流程。实验结果全面展示，最终得到的仅需7亿参数的幻觉标注器超越了GPT-4的表现，并在HaluEval和HalluQA上的零样本推理中取得了最新的幻觉检测效果。这种标注器不仅能够评估不同LLMs在大规模数据集上的幻觉程度，还能通过NLI指标提升（从25%提高到37%）来帮助减轻生成文本的幻觉问题。|
|**2024-07-05**|**Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge**|Yuanze Lin et.al.|[2407.04681](http://arxiv.org/abs/2407.04681)|null|近年来，大规模多模态语言模型（MLLM）在使用大型高质量的图像文本数据集进行训练后，在整体理解图像方面取得了显著进步。然而，文本形式固有的困难限制了它们处理需要精细或空间密集信息（如遮罩）的问题，这影响了它们对详细视觉元素的理解能力。受到检索增强生成（RAG）理念的启发，本文提出了一种新的视觉提示方法，旨在将来自专门视觉模型（如实例分割和OCR模型）的精细外部知识融入MLLM。这是一个有前景但尚未充分探索的方向，可以提升MLLM的表现。我们的方法区别于同时期的工作，它们将外部知识转化为额外的文本提示，迫使模型间接学习视觉内容与文本坐标之间的对应关系。相反，我们提议将精细知识信息直接嵌入到一个空间嵌入图中作为视觉提示。这种设计可以轻松地整合进各种MLLM，如LLaVA和Mipha，显著提高它们的视觉理解性能。通过严谨的实验，我们在九个基准测试中展示了我们的方法如何提升MLLM的整体性能，增强其对细粒度上下文感知的能力。|
|**2024-07-05**|**Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition**|Ye Bai et.al.|[2407.04675](http://arxiv.org/abs/2407.04675)|null|现代自动语音识别（ASR）模型需要准确转录来自不同领域、语言和口音的多样语音信号，同时考虑到特定上下文信息，以适应各种应用场景的需求。传统的端到端模型结合额外的语言模型表现出色，但在数据匹配场景中效果良好，但逐渐面临瓶颈。本文介绍了一种基于大型语言模型（LLM）的新型语音识别模型——Seed-ASR。它建立在音频条件化LLM（AcLLM）架构之上，通过将连续语音表示和上下文信息输入到LLM中，利用了LLM的强大功能。通过分阶段的大规模训练以及在LLM中激发上下文感知能力，Seed-ASR在包括多个领域、方言和语言的综合评估集上显著优于端到端模型。此外，Seed-ASR能够部署到各种场景中支持特定需求，无需额外的语言模型。与最近发布的大型ASR模型相比，Seed-ASR在中文和英文公开测试集上的词（或字符，针对中文）错误率降低了10%-40%，进一步证明了其强大的性能。|
|**2024-07-05**|**Lazarus: Resilient and Elastic Training of Mixture-of-Experts Models with Adaptive Expert Placement**|Yongji Wu et.al.|[2407.04656](http://arxiv.org/abs/2407.04656)|null|随着大型语言模型（LLMs）的规模不断扩大，稀疏激活的混合专家（MoE）架构因其计算成本的亚线性扩展而被越来越多地采用。然而，频繁的训练失败仍然是一个重大挑战，因为单次失败可能导致所有GPU陷入闲置，直至问题解决，从而可能丢失大量训练进度，需要从检查点重新开始。现有的高效容错训练解决方案要么缺乏弹性，要么依赖于将恢复能力构建到管道并行性中，但这不适用于MoE模型，因为MoE架构采用了专家并行策略。  我们提出了Lazarus，一个针对MoE模型进行容错和弹性的训练系统。Lazarus通过动态分配专家副本来应对专家工作负载的固有不平衡，从而加速训练，并开发了一种理论上最优的专家放置算法，以最大限度地提高在失败后的恢复概率。通过自适应的专家放置和灵活的令牌分发器，Lazarus能够在故障后充分利用所有可用节点，避免GPU空闲。  我们的评估表明，与现有MoE训练系统相比，Lazarus在频繁的节点故障下性能提升高达5.7倍，且在真实spot实例跟踪上提升了3.4倍。|
|**2024-07-05**|**Entity Decomposition with Filtering: A Zero-Shot Clinical Named Entity Recognition Framework**|Reza Averly et.al.|[2407.04629](http://arxiv.org/abs/2407.04629)|null|该论文关注的是临床命名实体识别（Clinical NER），这是一种从临床病历中提取重要实体的任务。近年来，大型语言模型（LLMs）在这一任务上表现出色。研究主要集中在专有的LLMs，但论文探讨了开放的、专门为命名实体识别训练的LLMs在临床NER中的性能。作者提出了一种新颖的框架，称为“实体分解与过滤”（Entity Decomposition with Filtering，EDF），目的是通过将实体识别任务分解为子实体类型的检索，并引入一个过滤机制来消除错误实体。实验结果表明，该框架在所有度量标准、模型、数据集和实体类型上都表现出有效性。分析显示，实体分解能够显著提高对先前未被捕捉到的实体的识别。此外，论文还提供了对框架的全面评估和深入的错误分析，以期为未来的研究提供方向。|
|**2024-07-05**|**On scalable oversight with weak LLMs judging strong LLMs**|Zachary Kenton et.al.|[2407.04622](http://arxiv.org/abs/2407.04622)|null|该论文探讨了可扩展的监督协议，目标是让人类能够有效监督超越人类级别的AI。研究主要聚焦在辩论、咨询和直接问答三种形式上，使用大型语言模型（LLMs）作为AI代理和法官角色，假设法官模型较弱。实验涵盖了广泛的任务异质性，扩展了先前仅关注信息不对称的单一提取式问答任务，增加了数学、编程、逻辑和多模态推理等领域的挑战。结果表明，在所有任务中，当咨询师随机被分配正确或错误答案时，辩论优于咨询。在存在信息不对称的提取式问答任务中，辩论优于直接问答，但在其他没有信息不对称的任务中，结果则不一。当AI被允许选择要论证的答案而非预先指定时，发现法官被错误答案说服的情况在辩论中减少。此外，更强的辩论者模型能提高法官的准确性，尽管提升程度略低于之前的研究。|
|**2024-07-05**|**Leveraging Large Language Models for Integrated Satellite-Aerial-Terrestrial Networks: Recent Advances and Future Directions**|Shumaila Javaid et.al.|[2407.04581](http://arxiv.org/abs/2407.04581)|null|本文探讨了大型语言模型（LLMs）如何融入集成卫星、航空和地面网络（ISATN）的变革潜力，利用先进的人工智能（AI）和机器学习（ML）技术优化这些网络的连通性。首先概述了ISATN的当前架构，强调了LLMs在提升数据流、信号处理和网络管理方面的作用，以推动5G/6G通信技术的发展，通过高级预测算法和实时决策来增强性能。接着，深入分析了ISATN组件，探讨了如何有效地利用LLMs解决传统数据传输和处理中的瓶颈问题。  文章着重于ISATN的网络管理挑战，包括资源分配策略、流量路由以及在不断变化条件下确保无缝连接和最优性能的网络安全。同时，我们讨论了将LLMs整合到ISATN中所面临的技术挑战，如数据集成、扩展性问题、决策过程中的延迟，以及构建健壮且容错的系统设计。最后，研究指出了未来研究的关键方向，即如何充分利用LLM的优势，以提升网络可靠性、优化性能，实现一个真正全球互联且智能的网络体系。|
|**2024-07-05**|**VRSD: Rethinking Similarity and Diversity for Retrieval in Large Language Models**|Hang Gao et.al.|[2407.04573](http://arxiv.org/abs/2407.04573)|null|在大型语言模型（LLMs）快速发展的背景下，向量检索算法对于满足相似度和多样性要求的语义查询至关重要。尽管Maximal Marginal Relevance（MMR）在涉及这两个需求的检索场景中被广泛应用，但其参数λ的变化会导致结果波动，使得向量空间中的优化路径变得模糊。此外，当前缺乏对相似性和多样性在检索过程中约束的坚实理论分析。本文提出了一种新方法，通过查询向量与求和向量之间的关系来刻画这两种约束。这种关系确保了相似性，同时要求求和向量中的各个向量以分散的方式与查询向量对齐，以满足多样性需求。  我们还提出了一个新的组合优化问题：从一组候选向量中选择 $k$ 个，使得它们的求和向量最大程度地与查询向量匹配。我们证明了这个问题是NP完全的，揭示了在向量检索中同时追求相似性和多样性的深刻困难，并为后续研究奠定了理论基础。此外，我们设计了一个名为Vectors Retrieval with Similarity and Diversity（VRSD）的启发式算法，它不仅具有明确的优化目标，无需预设参数，而且在时间复杂性上相对于MMR有所降低。实证验证表明，VRSD在各种数据集上显著优于MMR。|
|**2024-07-05**|**PoPreRo: A New Dataset for Popularity Prediction of Romanian Reddit Posts**|Ana-Cristina Rogoz et.al.|[2407.04541](http://arxiv.org/abs/2407.04541)|**[link](https://github.com/ana-rogoz/poprero)**|**我们推出了PoPreRo，这是首个专为罗马尼亚Reddit帖子的流行度预测收集的dataset。PoPreRo汇集了五个不同罗马尼亚子论坛的多样化帖子样本，总计包含28,107条数据。随数据集一同发布的，我们还提供了一系列竞争性模型作为未来研究的基础。值得注意的是，测试集上得分最高的模型达到了61.35%的准确率和60.60%的宏F1分数，这表明在PoPreRo上的流行度预测任务极具挑战性。通过少量提示对Falcon-7B大型语言模型的进一步探究也指向了同样的结论。因此，我们相信PoPreRo是一个有价值的资源，可以用来评估罗马尼亚社交媒体帖子的流行度预测模型。我们的数据集已公开发布在https://github.com/ana-rogoz/PoPreRo。**|
|**2024-07-03**|**Universal Length Generalization with Turing Programs**|Kaiying Hou et.al.|[2407.03310](http://arxiv.org/abs/2407.03310)|null|**摘要：**  长度泛化指的是从简短的训练序列推断出长测试序列的能力，这对于当前的大语言模型是一个挑战。尽管先前的研究提出了一些架构或数据格式变化来实现长度泛化，但这些方法通常局限于特定任务。在此基础上，我们结合了擦除板和链式思考（Chain-of-Thought, CoT）技术，提出了Turing程序，这是一种新颖的CoT策略，它将算法性任务分解成类似图灵机计算的步骤。这个框架既通用又简单，只需要在上下文中稍作修改地复制文本。我们展示了使用Turing程序，我们在加法、乘法以及基于上下文的SGD等算法性任务上实现了稳健的长度泛化。接着，我们展示Transformer在随机Turing程序上也能实现长度泛化，这表明对于任何算法性任务，长度泛化都是可能的。最后，我们理论证明Transformer能够实现Turing程序，构造了一个简单的RASP（Weiss等人）程序，它模拟任意图灵机。|
|**2024-07-03**|**Large Language Models for JSON Schema Discovery**|Michael J. Mior et.al.|[2407.03286](http://arxiv.org/abs/2407.03286)|null|## 背景 半结构化数据格式如JSON因其在存储数据时的灵活性而被广泛应用。然而，JSON数据通常缺乏与关系数据库中的表单结构相对应的规范（schema）。因此，出现了许多从数据集中发现规范的工具。尽管这些工具很有用，但现有的方法主要关注文档的语法，而忽视了语义信息。本研究中，我们探讨如何自动为发现的规范添加有意义的语义信息，使其类似于人类作者编写的规范中所包含的信息。我们利用大型语言模型和人工编写的JSON Schema文档库，生成元素的自然语言描述、可重用定义的有意义名称，并识别出哪些发现的属性最有用，哪些可以视为“噪声”。我们的方法在先前已证明与人类判断高度相关的文本生成指标上表现出色。|
|**2024-07-03**|**LLM Internal States Reveal Hallucination Risk Faced With a Query**|Ziwei Ji et.al.|[2407.03282](http://arxiv.org/abs/2407.03282)|null|## 背景  大型语言模型（LLMs）的幻觉问题严重制约了它们的可靠性和可信度。人类具有自我意识过程，能识别面对查询时的未知领域。为此，我们的论文研究了LLMs能否在生成响应之前自行评估其幻觉风险。我们从训练数据源和15个不同自然语言生成（NLG）任务的角度广泛分析LLMs的内部机制，这些任务涵盖了超过700个数据集。实证分析揭示了两个关键发现：(1) LLM的内部状态能够指示它们是否在训练数据中见过查询；(2) LLM的内部状态显示出它们对查询可能产生幻觉或不产生幻觉的风险。我们的研究关注特定的神经元、激活层和令牌，这些在LLM对不确定性和幻觉风险的认识中扮演着关键角色。通过一种探查估计算法，我们利用LLM的自我评估能力，在运行时实现了平均84.32%的幻觉估计准确率。|
|**2024-07-03**|**Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning**|Zhili Shen et.al.|[2407.03227](http://arxiv.org/abs/2407.03227)|null|我们从大型语言模型的角度探讨文本到SQL的语义解析。鉴于商业数据库模式的规模挑战和业务智能解决方案的部署问题，我们提出了一种方法，它动态获取输入数据库信息，并利用抽象语法树选择少量示例进行上下文学习。此外，我们研究了如何利用并行语义解析器生成SQL查询的近似版本，以支持我们的检索。我们甚至将这种方法推向极致，采用不到5亿参数的模型作为高效近似器，并赋予其并行处理模式的能力。我们在单语和跨语言的语义解析基准上应用了我们的方法，结果优于现有最佳基线。全面的实验揭示了这种检索增强生成设置中各个模块的贡献，为未来工作指明了有趣的方向。|
|**2024-07-03**|**How Does Quantization Affect Multilingual LLMs?**|Kelly Marchisio et.al.|[2407.03211](http://arxiv.org/abs/2407.03211)|null|## 背景 量化技术在提升大语言模型（LLM）的推理速度和部署效率方面被广泛应用。尽管有大量的研究关注了量化后的英语任务模型效果，但尚无研究针对多语言场景。我们对量化多语言LLM进行了深入分析，重点关注其跨语言性能及不同规模下的表现。我们采用自动基准测试、LLM作为评判者的方法以及人类评估，发现以下几点：(1) 量化对人类评价的影响是负面的，且自动指标严重低估了这种损害：自动任务中平均1.7%的性能下降对应人类评估中日本任务的16.0%显著下滑；(2) 不同语言受到量化的影响程度不均，非拉丁字母体系的语言受影响最严重；(3) 比如数学推理这类挑战性任务，其性能下降最为显著。随着低功耗模型服务于全球NLP技术的普及变得至关重要，我们的研究结果强调了在评估高效模型时，多语言性能应作为关键指标。|
|**2024-07-03**|**TheoremLlama: Transforming General-Purpose LLMs into Lean4 Experts**|Ruida Wang et.al.|[2407.03203](http://arxiv.org/abs/2407.03203)|**[link](https://github.com/RickySkywalker/TheoremLlama)**|**### 翻译  在数学证明的计算机可验证形式语言（如Lean）验证中，使用大型语言模型（LLMs）基于自然语言（NL）的证明方法具有重要影响。然而，由于NL与形式语言（FL）的证明数据稀缺，现代LLMs在生成完整证明方面的性能欠佳。为此，本文提出了一种名为**TheoremLlama**的端到端框架，旨在训练通用LLM成为Lean4专家。该框架包括NL-FL对齐数据集生成方法、LLM形式定理证明器的训练策略以及LLM在撰写Lean4证明中的技术。  关键创新在于我们开发了NL-FL自举方法，即将NL证明融入Lean4代码，利用LLMs的自然语言推理能力进行正式推理。通过这种数据集生成方式，我们提供了**Open Bootstrapped Theorems**（OBT），一个对齐且自举的NL-FL数据集。**TheoremLlama**框架在MiniF2F-Valid和Test数据集上的累计准确率分别达到36.48%和33.61%，超过了GPT-4的基线分数22.95%和25.41%。我们已公开了模型检查点和生成的数据集，并即将全部代码开源。**|
|**2024-07-03**|**Fine-Tuning with Divergent Chains of Thought Boosts Reasoning Through Self-Correction in Language Models**|Haritz Puerto et.al.|[2407.03181](http://arxiv.org/abs/2407.03181)|**[link](https://github.com/ukplab/arxiv2024-divergent-cot)**|**该研究提出了一种新颖的方法，称为Divergent CoT（DCoT），通过要求模型在单次推理步骤中比较多个推理链来进一步提升性能。这种方法发现，即使在小型、更易于获取的大型语言模型上进行指令调优也能提高表现。通过广泛的实验，涉及不同类型的推理任务，研究发现对DCoT数据集的微调在各种规模的模型（从13亿到70亿参数）上普遍优于基本的CoT方法。实验和人工评估表明，这些性能提升源于模型在单次推理中生成了多个不同的推理路径，这表明语言模型能够实现自我纠正。相关代码和数据已在https://github.com/UKPLab/arxiv2024-divergent-cot上公开。**|
|**2024-07-03**|**Investigating Decoder-only Large Language Models for Speech-to-text Translation**|Chao-Wei Huang et.al.|[2407.03169](http://arxiv.org/abs/2407.03169)|null|## 背景  大型语言模型（LLMs）因其出色的推理能力、泛化能力和跨领域的流畅性，在提升语音相关任务方面展现出巨大潜力。本文关注的是如何将解码器仅有的LLMs整合到语音转文本翻译（Speech-to-Text Translation，S2TT）任务中。我们提出一种架构，让LLM直接处理编码的语音表示并生成文本翻译。同时，我们研究了不同参数高效微调技术和任务表述方式的影响。在不使用专有数据的情况下，我们的模型在CoVoST 2和FLEURS基准上实现了最先进的性能。我们还进行了深入分析，验证了我们设计选择的合理性，并为LLMs与S2TT任务的融合提供了见解。|
|**2024-07-03**|**SOS! Soft Prompt Attack Against Open-Source Large Language Models**|Ziqing Yang et.al.|[2407.03160](http://arxiv.org/abs/2407.03160)|null|## 背景  开源的大规模语言模型（LLMs）在公众和行业中的受欢迎程度日益提升，因为它们可定制、微调且免费使用。然而，一些开源LLMs在使用前需要审批，这促使第三方发布易于获取的版本，甚至对这些模型进行微调或量化优化，以降低计算需求。这些便捷版本对用户颇具吸引力，但也增加了训练时间攻击的风险，威胁到LLMs的完整性和安全性。本文提出一种新的训练时间攻击方法SOS，它设计得计算需求低，无需干净数据或调整模型权重，保持模型的可用性。SOS针对各种场景下的安全问题，包括后门攻击、破解攻击和提示窃取攻击。实验结果表明，该攻击在所有评估目标上均有效。此外，我们还展示了SOS技术的另一面——版权令牌：这是一种新颖的方法，允许用户标记其版权内容，防止模型使用。|
|**2024-07-03**|**Let the Code LLM Edit Itself When You Edit the Code**|Zhenyu He et.al.|[2407.03157](http://arxiv.org/abs/2407.03157)|null|在本研究中，我们探讨了代码生成中的常见场景：开发者实时编辑现有代码，并请求大型语言模型（如大语言模型）进行即时重预测下一个token或行。直接的方法是让LLM重新编码整个键值缓存以提供精确的预测，但这个过程计算成本高，特别是当序列长度很长时。仅编码编辑后的子序列并将其整合到原始键值缓存中会遇到时间混淆问题，导致性能大幅下降。为此，我们提出了一种解决方案——\textbf{位置完整性编码}（Positional Integrity Encoding，简称PIE）。PIE基于旋转型位置编码，首先移除引入时间混淆的旋转型矩阵，然后重新应用正确的矩阵，确保了令牌之间的位置关系正确，仅需一轮矩阵乘法即可完成。我们在RepoBench-C-8k数据集上，使用13亿、67亿和330亿参数的DeepSeek-Coder模型进行了广泛实验，涵盖了代码插入、代码删除和多位置代码编辑等三个实际编程任务。实验结果表明，与标准的完整重计算方法相比，PIE在所有模型规模和任务中都能减少超过85%的计算开销，同时保持了良好的性能近似。|
|**2024-07-02**|**MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention**|Huiqiang Jiang et.al.|[2407.02490](http://arxiv.org/abs/2407.02490)|**[link](https://github.com/microsoft/MInference)**|**由于大型语言模型（LLMs）的计算挑战，尤其是随着提示长度的增长，其广泛应用面临障碍。由于注意力计算的二次复杂性，80亿参数的LLM在单个A100 GPU上处理100万个令牌（即预填充阶段）需要30分钟。现有的加速预填充方法往往在面对长序列LLMs时难以保持既高效又准确。为此，我们提出了MInference（百万令牌推理），这是一种旨在提升长序列处理预填充阶段速度的稀疏计算方法。我们发现了注意力矩阵中的三种独特模式：A形、垂直斜线和块稀疏，这些模式可利用GPU进行高效的稀疏计算。我们在离线阶段确定每个注意力头的最佳模式，并在推理过程中动态构建稀疏索引。通过优化的GPU内核，我们实现了基于指定模式的稀疏注意力计算，显著减少了长序列LLMs预填充阶段的延迟。我们的方法无需修改预训练设置或额外微调即可直接应用于现有LLMs。我们在包括InfiniteBench、RULER、PG-19和Needle In A Haystack在内的各种下游任务以及LLaMA-3-1M、GLM4-1M、Yi-200K、Phi-3-128K和Qwen2-128K等模型上的实验表明，MInference在A100上有效降低了预填充的推理延迟高达10倍，同时保持了准确性。我们的代码已开源，地址为：https://aka.ms/MInference。**|
|**2024-07-02**|**Neurocache: Efficient Vector Retrieval for Long-range Language Modeling**|Ali Safaya et.al.|[2407.02486](http://arxiv.org/abs/2407.02486)|**[link](https://github.com/alisafaya/neurocache)**|**这篇论文介绍了一种名为Neurocache的方法，用于扩展大型语言模型（LLMs）的有效上下文范围，通过外部向量缓存存储其过去的模型状态。与近期的向量检索方法类似，Neurocache利用高效的k近邻(kNN)算法检索相关的历史状态，并将其融入注意力过程。Neurocache在改进现有方法方面有以下几点：(1) 存储压缩的状态，减小了缓存大小；(2) 每个令牌执行一次检索操作，提高了推理速度；(3) 将检索窗口扩展到邻近状态，提升了语言建模和下游任务的准确性。  实验结果表明，无论从头开始训练还是对预训练模型（如Llama2-7B和Mistral-7B）进行增强，Neurocache都能有效。我们还对比了Neurocache与其他文本检索方法，在单文档问答和少量样本学习任务中展示了其优势。源代码已在以下链接公开：https://github.com/alisafaya/neurocache。**|
|**2024-07-02**|**RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs**|Yue Yu et.al.|[2407.02485](http://arxiv.org/abs/2407.02485)|null|该研究提出了一种新颖的指令调优框架RankRAG，旨在针对检索增强生成（RAG）中的上下文排名和答案生成双重任务对大型语言模型进行调优。通过在训练过程中加入少量排名数据，指令调优的单个语言模型表现出令人惊讶的效果，超越了专门使用大量排名数据进行单独调优的现有专家排名模型。实验中，我们与包括GPT-4-0613、GPT-4-turbo-2024-0409和开放源代码的最先进的RAG性能模型ChatQA-1.5在内的多个强baseline进行了比较。具体来说，我们的Llama3-RankRAG在九个知识密集型基准上显著优于Llama3-ChatQA-1.5和GPT-4系列模型。此外，它还在无需针对生物医学领域数据进行指令调优的情况下，在五个生物医学领域的RAG基准上与GPT-4模型表现相当，这显示了其在新领域中的出色泛化能力。|
|**2024-07-02**|**MMedAgent: Learning to Use Medical Tools with Multi-modal Agent**|Binxu Li et.al.|[2407.02483](http://arxiv.org/abs/2407.02483)|null|尽管多模态大型语言模型（MLLMs）已经取得了成功，但它们的泛化能力仍然有限，在某些情况下不如专业模型。近期，研究人员开发了基于LLMs的代理，通过用户输入选择合适的专用模型来解决这些问题。然而，在医疗领域，这类进展的应用还不广泛。为了弥补这一空白，本文首次提出了一种专为医疗设计的代理，名为\textbf{M}ulti-modal \textbf{Med}ical \textbf{Agent}（MMedAgent）。我们构建了一个指令调优数据集，包含了六个医疗工具，用于解决七项任务，使代理能针对特定任务选择最适宜的工具。实验全面展示了MMedAgent在各种医疗任务上超越了开源方法，甚至包括封闭源模型GPT-4o，且在引入和整合新医疗工具方面表现出高效性。|
|**2024-07-02**|**Understanding Alignment in Multimodal LLMs: A Comprehensive Study**|Elmira Amirloo et.al.|[2407.02477](http://arxiv.org/abs/2407.02477)|null|随着大型语言模型（LLMs）性能的提升，偏好一致性已成为一个重要因素，但在多模态大型语言模型（MLLMs）中的应用相对较少。这些模型在图像理解任务中也会遇到诸如错误陈述和内容不一致（即幻觉）的问题。MLLMs的偏好对齐目标是使模型的回答更贴近图像信息。近期的研究已经引入了针对MLLM的偏好数据集，并尝试了直接偏好优化（DPO）和proximal policy optimization（PPO）等不同的对齐方法。然而，由于数据集、基础模型类型和对齐策略的差异，哪种方法对性能提升的贡献最大尚不清楚。  本文独立分析了MLLM偏好对齐的各个方面。我们将对齐算法分为离线（如DPO）和在线（如在线-DPO）两类，并表明在某些情况下结合这两种方法可以提高模型性能。我们还回顾了各种已发表的多模态偏好数据集，探讨了它们构建细节对模型性能的影响。基于这些发现，我们提出了一种新的多模态偏好数据生成方法——偏见驱动的幻觉采样（Bias-Driven Hallucination Sampling，BDHS），这种方法无需额外标注或外部模型，且在多个基准上展现出与之前发表的对齐工作相当的竞争性能。|
|**2024-07-02**|**Open Scene Graphs for Open World Object-Goal Navigation**|Joel Loo et.al.|[2407.02473](http://arxiv.org/abs/2407.02473)|null|如何构建能够在开放世界中执行语义导航任务的机器人，比如在新场景中寻找目标物体？尽管基础模型具备处理这类任务所需的丰富知识和泛化能力，但需要一种合适的场景表示来将它们整合到完整的机器人系统中。为此，我们提出了开放场景图（Open Scene Graphs，OSG），这是一种拓扑语义表示，用于保留和组织开放集中场景信息，且结构可适应不同环境类型。我们将基础模型和OSG整合到OpenSearch系统中，该系统专为开放世界的对象目标导航设计，能够理解自然语言指令并在多变环境中零样本泛化，寻找未见过的物体。我们的OSG增强了与大型语言模型（LLMs）的推理能力，使得OpenSearch在物体目标导航任务上表现出色，超越了现有的LLM方法。通过模拟实验和真实世界测试，我们验证了OpenSearch在各种环境、机器人和新颖指令下的泛化能力。|
|**2024-07-02**|**Reliable Confidence Intervals for Information Retrieval Evaluation Using Generative A.I**|Harrie Oosterhuis et.al.|[2407.02464](http://arxiv.org/abs/2407.02464)|null|传统的信息检索（IR）系统评估通常成本高昂，因为需要人工专家进行相关性标注。近年来，生成式人工智能，尤其是大型语言模型（LLMs），能够以相对较低的计算成本大规模生成相关性注释，可能减轻IR评估的传统成本，并使其适用于众多资源匮乏的应用场景。然而，生成的注释并非无误，直接用于评估可能导致结果不可靠。为此，本研究提出两种方法，分别是基于预测驱动的推断和规范风险控制，利用计算机生成的相关性注释为IR评估指标提供可靠的置信区间（CIs）。  我们的方法需要少量可靠的注释，通过统计分析生成注释中的错误，从而为评估指标设置CIs，具有坚实的理论基础。与现有方法不同，我们特别设计的规范风险控制方法适用于排名评估，并且可以根据查询和文档自适应调整CIs。实验结果显示，我们的置信区间准确捕捉了基于LLM注释的评估中的变异性和偏差，优于传统的Bootstrap估计。我们期望这些贡献能为那些传统上难以实现可靠评估的众多IR应用带来革新。|
|**2024-07-03**|**Video Watermarking: Safeguarding Your Video from (Unauthorized) Annotations by Video-based LLMs**|Jinmin Li et.al.|[2407.02411](http://arxiv.org/abs/2407.02411)|null|随着视频驱动的大型语言模型（LLMs）的兴起，视频理解能力得到了显著提升，但同时也引发了数据保护方面的担忧，因为视频更容易被无授权地标注。为此，本文提出了一种名为“Video Watermarking”的创新方法，旨在保护视频免受未经授权的视频LLMs，特别是针对内容和描述的处理。通过在关键帧中嵌入难以察觉的水印，我们利用多模态流损失保持观看体验的同时，防止视频被滥用。大量的实验表明，Video Watermarking显著降低了视频在各种视频LLMs中的可理解性，证明了其隐秘性和鲁棒性。总的来说，我们的方法为确保视频内容的安全、完整性和保密性提供了一种解决方案，以应对不断发展的视频LLMs技术。|
|**2024-07-02**|**CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models**|Song Wang et.al.|[2407.02408](http://arxiv.org/abs/2407.02408)|null|随着大型语言模型（LLMs）被越来越多地应用于各种自然语言处理任务，对其生成内容可能产生的负面社会影响的担忧也随之增加。为了评估LLMs的偏见，研究人员已经提出了一系列数据集。然而，现有的偏见评估工作往往只关注某种类型的偏见，并使用不一致的评价指标，这导致不同数据集和LLM之间的比较困难。为此，我们收集了多种用于评估LLM偏见的数据集，并进一步提出了CEB（Compositional Evaluation Benchmark），它涵盖了不同社会群体和社会任务中的各种类型偏见。CEB的构建基于我们新提出的构成性分类体系，从三个维度对每个数据集进行刻画：偏见类型、社会群体和任务。通过结合这三个维度，我们开发出一种全面的LLM偏见评估策略。实验结果表明，这些偏见在各维度上的程度有所不同，从而为针对特定偏见的缓解方法的发展提供了指导。|
|**2024-07-02**|**Assessing the Code Clone Detection Capability of Large Language Models**|Zixian Zhang et.al.|[2407.02402](http://arxiv.org/abs/2407.02402)|null|该研究旨在评估两种先进的大型语言模型（LLMs），GPT-3.5和GPT-4，在代码克隆检测任务中的性能。实验通过在两个数据集上测试模型：BigCloneBench（人类创建）和GPTCloneBench（LLM生成）。研究发现，GPT-4在所有类型的代码克隆检测中都明显优于GPT-3.5。结果显示，GPT模型的准确度与其识别代码克隆的能力与代码相似度之间存在关联，但它们在识别最复杂的Type-4代码克隆时效果较低。此外，GPT模型在检测LLM生成的代码中的代码克隆表现优于人类生成的代码，但整体准确性仍不显著。这些发现强调了进一步提升LLM在代码克隆识别能力的必要性，特别是针对自我生成代码克隆的问题，随着软件工程师越来越多地使用基于LLM的代码生成和重构工具，这可能会成为一个问题。|
|**2024-06-28**|**Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework for Multimodal LLMs**|Sukmin Yun et.al.|[2406.20098](http://arxiv.org/abs/2406.20098)|**[link](https://github.com/mbzuai-llm/web2code)**|**多模态大型语言模型（MLLMs）在图像、视频和音频等多种模态的处理任务上表现出色。然而，它们在理解和生成网页截图以及相应的HTML代码方面的能力相对较弱。为解决这个问题，我们提出Web2Code，这是一个包括大规模网页到代码的新基准，用于指令调优，并评估MLLM在网页理解及HTML代码转换能力上的表现。我们构建数据集时，利用预训练的LLMs增强现有的网页到代码数据集，并生成多样化的网页图片，以供渲染。输入是网页图片和说明，输出是网页的HTML代码，同时加入关于网页内容的丰富自然语言问答对，以促进对网页内容的全面理解。为了评估模型在这类任务中的性能，我们开发了一个测试框架，用于测试MLLM在网页理解与网页到代码生成方面的技能。实验结果表明，我们的数据集不仅有益于我们提出的任务，还在视觉领域的一般性能上有所提升，而先前的数据集会导致性能下降。我们期望这项工作能推动通用MLLM的发展，使其适用于网络内容生成和自动化任务。我们的数据和代码将在<https://github.com/MBZUAI-LLM/web2code>上公开。**|
|**2024-06-28**|**LLaRA: Supercharging Robot Learning Data for Vision-Language Policy**|Xiang Li et.al.|[2406.20095](http://arxiv.org/abs/2406.20095)|**[link](https://github.com/lostxine/llara)**|**该论文介绍了一种名为LLaRA（大型语言和机器人助手）的框架，它将机器人行动策略转化为对话形式，通过结合额外的数据辅助学习，提升响应质量。利用具备视觉输入的大型语言模型（VLMs），即视觉语言模型，这些模型能够处理状态信息，作为视觉-文本提示，并生成最优的机器人决策策略。首先，论文提出了一种自动化方法，从现有的行为克隆数据中生成多样且高质量的机器人指令数据集。然后，使用这种定制的对话式格式对VLM进行训练，使其能够生成有意义的机器人行动策略。实验结果表明，LLaRA框架在多个模拟和真实世界环境中展现出最先进的性能。相关代码、数据集和预训练模型已在<https://github.com/LostXine/LLaRA>提供。**|
|**2024-06-28**|**Scaling Synthetic Data Creation with 1,000,000,000 Personas**|Xin Chan et.al.|[2406.20094](http://arxiv.org/abs/2406.20094)|**[link](https://github.com/tencent-ailab/persona-hub)**|我们提出了一种新颖的基于人格的数据合成方法，该方法利用大型语言模型（LLM）内的多种视角来生成多样化的人工合成数据。为了在大规模上充分利用这种方法，我们引入了Persona Hub，这是一个从网络数据自动整理出的一亿个多元化人格的集合，相当于全球人口的约13%。这些人格作为分布式世界知识载体，几乎可以调用LLM内包含的各类观点，从而推动大规模、多样化的合成数据创建，适用于各种场景。通过展示Persona Hub如何在大规模生成高质量的数学和逻辑推理问题、指令（用户提示）、富含知识的文本、游戏NPC和工具（函数）等方面的应用，我们证明了基于人格的数据合成具有多样性、可扩展性、灵活性和易用性，可能引领合成数据创造和实际应用的新范式，对LLM的研究和发展产生深远影响。|
|**2024-06-28**|**LLaVolta: Efficient Multi-modal Models via Stage-wise Visual Context Compression**|Jieneng Chen et.al.|[2406.20092](http://arxiv.org/abs/2406.20092)|**[link](https://github.com/beckschen/llavolta)**|**尽管在大型语言模型（LLMs）的文本嵌入压缩方面取得了显著进步，但大型多模态模型（LMMs）中的视觉令牌压缩仍然被忽视。本文研究了视觉令牌的冗余性以及在这些模型中的有效训练。初步实验表明，在测试阶段通过简单平均池化消除高达70%的视觉令牌，GQA基准的视觉问答准确率仅下降3%，这显示出视觉上下文中存在大量冗余。为解决这个问题，我们提出了Visual Context Compressor，它在训练阶段减少视觉令牌数量，以提高效率而不会影响性能。为了在压缩视觉令牌时尽量减少信息损失并保持训练效率，我们开发了轻量级训练方案LLaVolta。LLaVolta采用分阶段的视觉上下文压缩策略，从重度到轻度逐渐压缩，最终在训练结束时完全不进行压缩，从而在测试时不会丢失任何信息。广泛的实验表明，我们的方法提升了多模态模型在图像-语言和视频-语言理解任务上的性能，并显著降低了训练成本。代码已在https://github.com/Beckschen/LLaVolta上开源。**|
|**2024-06-28**|**ProgressGym: Alignment with a Millennium of Moral Progress**|Tianyi Qiu et.al.|[2406.20087](http://arxiv.org/abs/2406.20087)|null|随着前沿人工智能系统，特别是大型语言模型（LLMs）在知识论中的影响力日益增强，它们可能强化社会普遍的价值观，进而加剧错误道德观念的固化，导致广泛的社会问题持续存在。为应对这一潜在风险，我们提出进步对齐作为一种技术解决方案。进步对齐算法旨在学习人类道德进步的机制，从而弥补现有对齐方法对当代道德盲点的敏感性。为了推动进步对齐的研究，我们开发了ProgressGym，一个实验性框架，它从历史中学习道德进步的规律，以促进现实世界道德决策的未来发展。借助9个世纪的历史文本和18个历史LLMs，ProgressGym将现实生活中的进步对齐挑战转化为具体的基准。我们定义了三个核心挑战：追踪演变的价值（PG-Follow）、预测道德进步（PG-Predict）以及调节人与AI价值变迁之间的反馈循环（PG-Coevolve）。这些任务需要时间维度的方法，而传统的对齐策略无法胜任。  为此，我们展示了终身学习和外推算法作为进步对齐的基本方法，并建立了一个开放的排行榜，邀请创新算法和新挑战。该框架和排行榜分别可在https://github.com/PKU-Alignment/ProgressGym 和 https://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard 获取。|
|**2024-06-28**|**Auto Cherry-Picker: Learning from High-quality Generative Data Driven by Language**|Yicheng Chen et.al.|[2406.20085](http://arxiv.org/abs/2406.20085)|null|基于扩散模型的生成方法已经在生成各种布局的高质量图像方面展现出巨大潜力，这对于下游感知任务具有显著益处。然而，仅依赖语言描述和一个合适的多实例评估指标来实现全自动布局生成并未得到充分探索。本文提出了一种新颖的框架——Auto Cherry-Picker（ACP），旨在自动生成高质量的多模态训练样本，以增强感知和多模态训练效果。通过输入自然语言概念列表，我们引导大型语言模型（LLMs）生成详细的描述并设计合理的布局。然后，使用文本到图像模型生成多个图片。接着，我们采用精心设计的评估指标对生成的数据进行精炼，确保质量。特别是，我们提出了复合布局与图像评分（Composite Layout and Image Score，CLIS）这一新指标，用于公正地评估生成的图像。我们的合成高质示例在定制初始概念列表时，能够有效提升各种场景下的性能，尤其是在处理长尾分布和不平衡数据集的问题上。下游任务的实验结果显示，ACP显著提高了现有模型的表现。此外，我们深入研究了CLIS与下游任务性能提升之间的关联，发现CLIS分数越高，性能越好。这表明评估指标在视觉感知和多模态大型语言模型任务中可能发挥关键作用。我们将提供代码。|
|**2024-06-28**|**Molecular Facts: Desiderata for Decontextualization in LLM Fact Verification**|Anisha Gunjal et.al.|[2406.20079](http://arxiv.org/abs/2406.20079)|**[link](https://github.com/anisha2102/molecular_facts)**|**随着大型语言模型（LLM）生成内容的自动事实核查变得越来越普遍，以应对错误叙述的问题，研究的一个关键焦点在于核查的粒度：较大的文本段落难以核查，而更原子化的事实（如命题）可能缺乏正确的上下文解读。本文探讨了在这些原子事实中上下文的作用。我们认为完全原子的事实并非最佳表示形式，为此我们提出了分子事实的两个标准：去情境化（decontextuality），即它们能否独立存在，以及最小化（minimality），即添加多少额外信息才能实现去情境化。我们量化了去情境化对最小化的影响，并提出了一种基础方法来自动生成分子事实，目标是在保持准确性的同时提供适量的信息。我们将这种方法与不同的去情境化策略进行了比较，发现分子事实能够在模糊场景中平衡最小化和事实核查的准确性。**|
|**2024-07-01**|**BMW Agents -- A Framework For Task Automation Through Multi-Agent Collaboration**|Noel Crawford et.al.|[2406.20041](http://arxiv.org/abs/2406.20041)|null|自主代理驱动的大规模语言模型（LLMs）展示了巨大的自动化潜力。早期的展示表明，这些代理能够解决复杂任务，与外部系统交互以增强知识，并触发行动。特别是，多个代理协作解决复杂任务的工作流证明了它们在不那么严格和定义不明确的环境中操作的能力。因此，多代理方法有巨大的潜力成为众多工业应用的核心，从复杂的知识检索系统到下一代机器人过程自动化。鉴于当前LLMs的推理能力，处理复杂流程需要分步骤的方法，包括设计明确且模块化的任务计划。根据复杂程度，这些任务可以由单个代理或一组代理执行。本研究专注于构建一个灵活的代理工程框架，重点关注规划和执行，旨在应对不同领域的复杂应用场景。该框架为工业应用提供可靠性，并提出确保可扩展、灵活且协作的工作流程技术，让多个自主代理协同解决问题。|
|**2024-06-28**|**LEMoE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of Large Language Models**|Renzhi Wang et.al.|[2406.20030](http://arxiv.org/abs/2406.20030)|null|## 背景  大型语言模型（LLMs）为了跟上不断变化的世界知识，需要持续进行模型更新，这催生了终生模型编辑任务。近年来，尽管已经开发出多种单次和批量编辑的技术，但它们在面对终生编辑时要么无法应用，要么效果不佳。本文中，我们提出LEMoE，一个专为终生模型编辑设计的混合专家（MoE）适配器。首先，我们分析了影响传统MoE适配器在终生编辑中有效性的因素，包括灾难性遗忘、路由不一致性和顺序敏感性。基于这些洞察，我们提出了一种定制的模块插入方法，引入了新颖的键值对锚定路由以增强训练和推理阶段的路由一致性，同时采用了一个简洁而有效的聚类基编辑顺序规划。实验结果表明，我们的方法在终生编辑任务中表现出色，超越了先前的模型编辑技术，同时保持了批量编辑任务中的优秀性能。我们的代码将开源。|
|**2024-06-28**|**ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models**|Yuxiang Zhang et.al.|[2406.20015](http://arxiv.org/abs/2406.20015)|**[link](https://github.com/toolbehonest/toolbehonest)**|**随着工具增强的大型语言模型（LLMs）迅速融入实际应用，社区亟需全面了解这些模型中的幻觉问题。为此，我们提出了一项全面的诊断基准——ToolBH。我们从深度和广度两个维度进行评估：在深度上，设计了多级诊断流程，包括（1）可解性检测、（2）解决方案规划和（3）缺失工具分析；在广度上，考虑了工具集特征下的三种场景：缺少必要工具、潜在工具和功能有限的工具。我们构建了七个任务，并通过多次人工标注收集了700份评估样本。结果显示，当前先进的模型Gemini-1.5-Pro和GPT-4o在这项基准上的总得分为45.3和37.0，满分100分。在工具增强的LLM场景中，更大的模型参数并不一定意味着更好的性能，训练数据和回复策略同样关键。我们的诊断分析指出，模型错误的主要原因在于任务可解性的判断。开放源码模型在冗长回复时性能下降，而专有模型在长链推理方面表现更优。**|
|**2024-06-27**|**ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos**|Jr-Jen Chen et.al.|[2406.19392](http://arxiv.org/abs/2406.19392)|**[link](https://github.com/rextime/rextime)**|**我们提出了一项名为ReXTime的基准测试，专门针对人工智能模型在视频事件中的时间推理能力进行严谨评估。ReXTime关注的是跨时间推理，即理解当问题及其相应的答案出现在不同的视频片段时的人类式理解。这种需要深入理解视频片段之间因果关系的时间推理能力对前沿的多模态大型语言模型构成了重大挑战。为了支持这种评价，我们开发了一个自动化管道，用于生成时间推理的问答对，大大减少了繁琐的手动标注需求。我们的基准包括921个精心筛选的验证样本和2,143个测试样本，每个样本都经过人工精心挑选以确保准确性和相关性。评估结果显示，尽管前沿大型语言模型在学术模型上表现突出，但它们与人类的表现仍存在显著的14.3%的精度差距。此外，我们的管道无需人工创建了一个包含9,695个机器生成样本的训练数据集，实证研究表明，这可以通过微调来提升跨时间推理能力。**|
|**2024-06-27**|**The Remarkable Robustness of LLMs: Stages of Inference?**|Vedang Lad et.al.|[2406.19384](http://arxiv.org/abs/2406.19384)|**[link](https://github.com/vdlad/remarkable-robustness-of-llms)**|**我们通过删除和交换相邻层来展示并研究大型语言模型的惊人鲁棒性。实验结果显示，在不进行微调的情况下，这些干预措施仍能保留原始模型72%至95%的预测精度，而且模型层数越多，表现出更高的鲁棒性。根据逐层干预实验和其他实验，我们提出了一个假设：存在四种通用的推理阶段，跨越八种不同的模型：解码器阶段，将原始令牌表示提升为更高级的上下文表示；特征工程阶段，迭代优化任务和实体特定特征；然后是模型的半部分，随着专门组件的作用，隐藏表示与词汇空间的对齐进入一个相变阶段；最后，最后一层通过消除对预测造成干扰的过时特征，精细化后续的令牌分布。**|
|**2024-06-27**|**The Model Arena for Cross-lingual Sentiment Analysis: A Comparative Study in the Era of Large Language Models**|Xiliang Zhu et.al.|[2406.19358](http://arxiv.org/abs/2406.19358)|null|### 概述  情感分析在自然语言处理（NLP）中扮演着核心角色。XLM-R和mT5等多语言预训练模型的兴起推动了跨语言情感分析的关注度提升。近期大型语言模型（LLM）的出现极大地推动了通用NLP任务的发展，但这些模型在跨语言情感分析方面的性能尚未充分探讨。本研究通过实证分析，比较了公共小型多语言模型（SMLM）如XLM-R与以英语为中心的LLM（如Llama-3）在英语、西班牙语、法语和中文的情感分析中的零样本和少量样本迁移能力。结果显示，就公开模型而言，SMLM在零样本跨语言设置中表现出更好的性能。然而，在少量样本情况下，公开LLM显示出更强的适应性。此外，我们发现专有的GPT-3.5和GPT-4在零样本跨语言能力上领先，但在少量样本场景下，它们被公开模型超越。|
|**2024-06-27**|**DiVERT: Distractor Generation with Variational Errors Represented as Text for Math Multiple-choice Questions**|Nigel Fernandez et.al.|[2406.19356](http://arxiv.org/abs/2406.19356)|null|## 背景  高质量的干扰项对于选择题（尤其是数学选择题）的评估和教学价值至关重要。然而，手工设计能够反映学生实际知识缺陷或误解的干扰项是一项艰巨的任务。尽管大型语言模型（LLM）如GPT-4在生成干扰项方面有所助益，但数学这类学科的处理仍然具有挑战性。因此，我们提出了一种新的方法，旨在理解和生成解释性的错误表示，以生成数学选择题的干扰项。本文介绍DiVERT（基于文本的变异误差生成器），这是一种利用7亿参数开源LLM的变分方法，它在真实世界数学选择题数据集（包含1,434个问题，被数十万学生使用）上的实验表明，相较于最先进的GPT-4方法，DiVERT在干扰项生成方面表现出色。此外，我们还进行了与数学教育者的同行评审，结果表明DiVERT生成的错误标签质量接近人类编写的。  ## 任务  请将上述英文论文摘要翻译成中文，输出不应包含除摘要内容外的任何其他内容，且确保不出现","字符。|
|**2024-06-27**|**IndoToxic2024: A Demographically-Enriched Dataset of Hate Speech and Toxicity Types for Indonesian Language**|Lucky Susanto et.al.|[2406.19349](http://arxiv.org/abs/2406.19349)|null|## 翻译  针对网络仇恨言论对社会和谐的严峻威胁，特别是在印尼这类国家，近年来仇恨言论在线比率增长了十倍，迫切需要有效的检测机制。然而，由于缺乏充足的标记数据，尤其是针对印尼文本的，这一进展受到了阻碍。边缘化群体，如什叶派、LGBTQ等少数群体，面临的挑战更大，因为仇恨言论报告不足，现有的检测工具对其理解有限。此外，当前数据集对主观性的处理不足，加剧了问题。为了应对这些问题，我们提出IndoToxic2024，这是一个全面的印尼仇恨言论和毒性分类数据集，包含43,692条记录，由19名多元化的个体进行标注，特别关注选举期间针对国内弱势群体（如总统选举中的特定群体）的文本。我们使用BERT模型（IndoBERTweet）进行了微调，为七种二元分类任务设定了基准，取得了0.78的宏F1分数。同时，我们展示了如何将人口统计信息融入其中，提升大型语言模型gpt-3.5-turbo在零样本情况下的性能。然而，我们也警告，过度依赖人口统计信息可能导致细化模型性能下降，因为这会导致数据碎片化。|
|**2024-06-27**|**Jump Starting Bandits with LLM-Generated Prior Knowledge**|Parand A. Alamdari et.al.|[2406.19317](http://arxiv.org/abs/2406.19317)|null|我们提供了有力的证据，展示了将大型语言模型（LLMs）与上下文化多臂老虎机框架相结合的优势。上下文化老虎机在推荐系统中广泛应用，用于根据用户特定的上下文生成个性化建议。我们表明，经过大规模语料库训练，富含人类知识和偏好的LLMs能够很好地模拟人类行为，从而通过启动上下文化多臂老虎机来减少在线学习的遗憾（regret）。我们提出了一种初始化算法，通过提示LLMs生成接近人类偏好的预训练数据集，供老虎机学习使用。这显著降低了在线学习的遗憾和数据收集成本。我们的方法通过两组实验验证，包括使用LLMs作为占卜者（oracle）的实验和基于联合调查实验数据的真实世界实验。|
|**2024-06-27**|**From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data**|Zheyang Xiong et.al.|[2406.19292](http://arxiv.org/abs/2406.19292)|null|近期的研究指出，大型语言模型（LLMs）在处理长文本输入时在信息检索和推理能力上存在困难。为解决这个问题，我们提出了一种利用精心设计的合成数据集进行微调的方法，该数据集包含数值型键值对检索任务。我们在GPT-3.5 Turbo和Mistral 7B等模型上的实验显示，对这些模型进行这种数据集的微调显著提高了它们在长文本环境中的信息检索和推理能力。我们分析了微调后的模型，发现它们在从合成任务迁移到实际评估（如在20文档MDQA中的位置10处提升10.5%）方面的表现有所提升。此外，我们还发现，经过我们合成数据集微调的LLMs在通用基准上的性能保持稳定，而使用其他基于长文本增强数据集微调的LLMs可能会导致错误增加（例如，在TriviaQA上，Mistral 7B在我们的合成数据上微调无明显性能下降，而其他基线数据可能导致性能下降，范围在2.33%到6.19%之间）。本研究突显了通过合成数据微调来提升LLMs在长文本任务性能的潜力。|
|**2024-06-27**|**PhysioLLM: Supporting Personalized Health Insights with Wearables and Large Language Models**|Cathy Mengying Fang et.al.|[2406.19283](http://arxiv.org/abs/2406.19283)|null|我们介绍了一种名为PhysioLLM的互动系统，它利用大型语言模型（LLMs）结合可穿戴设备的生理数据和上下文信息，提供个性化的健康理解和探索。与商业健康应用不同，PhysioLLM具备全面的统计分析功能，能发现用户数据中的关联和趋势。用户可以用自然语言提问，获取生成的个性化洞察，并根据这些信息制定行动目标。以改善睡眠质量为例，因为其可通过生理数据量化且对整体健康至关重要。通过一项涉及24名Fitbit智能手表用户的用户研究，我们证明了PhysioLLM在促进对健康数据的深入个性化理解，以及支持实现个人健康目标方面，优于Fitbit应用和通用LLM聊天机器人。|
|**2024-06-27**|**HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale**|Junying Chen et.al.|[2406.19280](http://arxiv.org/abs/2406.19280)|**[link](https://github.com/freedomintelligence/huatuogpt-vision)**|**随着大型多模态语言模型（如GPT-4V）的迅速发展，它们在医学多模态能力方面取得了显著进步。然而，由于医学影像-文本数据的数量和质量受限于数据隐私问题和高昂的标注成本，这些模型仍面临挑战。早期的研究尝试利用PubMed的大型去标识化医疗图像-文本对来缓解这些问题，但它们仍受到数据噪音的影响。为解决这一问题，我们优化了PubMed中的医疗图像-文本对，并利用GPT-4V在“非盲”模式下进行数据清洗和格式转换，创建了PubMedVision数据集，包含130万份医学视觉问答样本。我们的验证表明：（1）PubMedVision显著提升了当前多模态语言模型在医学领域的性能，在诸如MMMU Health & Medicine track等基准测试中表现出显著改善；（2）医学专家的手动检查和实证结果证实了我们的数据集在数据质量上优于其他构建方法。利用PubMedVision，我们训练了一个名为HuatuoGPT-Vision的340亿参数的医学多模态语言模型，它在公开源多模态语言模型中表现出色，在医学多模态场景中显示出优越性能。**|
|**2024-06-27**|**AutoPureData: Automated Filtering of Web Data for LLM Fine-tuning**|Praneeth Vadlapati et.al.|[2406.19271](http://arxiv.org/abs/2406.19271)|**[link](https://github.com/Pro-GenAI/AutoPureData)**|**人们对最新的和可靠的大型语言模型（LLMs）的需求持续增长。通常，LLMs是基于固定的数据集训练然后部署的。然而，训练数据会随着时间逐渐过时。研究关注如何利用网络数据自动更新AI模型，但这一过程涉及数据质量与安全的顾虑，如偏见、垃圾信息等。确保数据纯净对于生成可靠的模型至关重要。在不纯数据上训练可能导致不良结果。该研究提出了一种系统，它收集网络数据，并借助现有可信的AI模型自动筛选出不需要的内容。实验中，我们收集并处理了一小部分网络数据，验证了该系统的数据净化效果。**|
|**2024-06-26**|**Symbolic Learning Enables Self-Evolving Agents**|Wangchunshu Zhou et.al.|[2406.18532](http://arxiv.org/abs/2406.18532)|**[link](https://github.com/aiwaves-cn/agents)**|**人工智能界通过构建"语言代理"（即复杂的大型语言模型管道）来探寻通用人工智能（AGI）的道路，这些模型结合了提示技术和工具使用方法。尽管它们在众多实际任务中表现出色，但当前语言代理研究的一个关键局限是其模型中心或工程导向：提示、工具和管道的改进依赖于大量的人工专家设计，而非自动从数据学习。我们认为，从模型中心向数据中心转变——让语言代理能够自主学习和适应环境，是它们迈向AGI的关键。为此，我们提出了"代理符号学习"框架，这是一个系统性的方法，它使语言代理能够在数据驱动的方式下自我优化，利用符号优化器。我们将代理视为具有可学习权重的符号网络，这些权重由提示、工具及其组合方式定义。代理符号学习旨在模仿连接主义学习中的两个基本算法：反向传播和梯度下降，但它处理的是自然语言形式的权重、损失和梯度。我们在标准基准和复杂现实任务上进行了概念验证实验，结果表明，代理符号学习使得语言代理在创建和部署后能够自我更新，实现了"自我进化的代理"。**|
|**2024-06-26**|**PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation**|Christoph Leiter et.al.|[2406.18528](http://arxiv.org/abs/2406.18528)|**[link](https://github.com/gringham/prexme)**|## 翻译  大型语言模型（LLMs）在自然语言处理领域带来了革命性变化，它们的上下文学习能力使其成为自然语言生成评价的有力工具，特别适用于资源匮乏和时间限制的场景。本文提出PrExMe，一项大规模的提示探索度量法，我们在机器翻译（MT）和摘要任务上评估了超过720种开源LLM作为度量标准的模板，总计约660万次评估。这项详尽的比较（1）为近期开源LLMs作为评价指标的表现设定了基准；（2）探讨了不同提示策略的稳定性和变异性。我们发现，一方面，存在一些情况下提示表现稳定：有些LLMs表现出特有的偏好，倾向于使用文本标签来评分，而另一些则倾向于返回数值分数。另一方面，提示的稳定性和模型排名可能受到看似微不足道的更改的影响。例如，将输出格式从“0到100”改为“-1到+1”可能会显著改变我们的评估结果。我们的研究有助于理解不同提示方法对MT和摘要评价中LLM-based度量的影响，揭示了最稳定的提示模式，并指出了潜在局限性。|
|**2024-06-26**|**CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs**|Zirui Wang et.al.|[2406.18521](http://arxiv.org/abs/2406.18521)|**[link](https://github.com/princeton-nlp/CharXiv)**|在实际应用多模态大型语言模型（Multimodal Large Language Models，MLLMs）处理科学论文或财务报告等任务时，图表理解至关重要。然而，现有的数据集往往集中在简化和同质化的图表上，以及基于模板的问题，这可能导致性能评估过于乐观。我们发现，尽管开源模型在现有基准上可能表现优于强大的专有模型，但通过简单的压力测试，如改变图表或问题，性能会下降高达34.5%。为此，我们提出CharXiv，这是一个包含2,323个来自arXiv论文的自然、复杂且多样化的图表的全面评估套件。CharXiv包括两类问题：1）描述性问题，用于检查基本图表元素；2）推理问题，需要综合分析图表中的复杂视觉元素。所有图表和问题都由专家精心挑选、整理和验证以保证质量。结果显示，最强专有模型（例如GPT-4o，准确率为47.1%）与最强开源模型（如InternVL Chat V1.5，准确率为29.2%）之间存在显著差距，而所有模型的表现均远低于人类的80.5%水平，这揭示了现有MLLM在图表理解能力上的不足。我们希望CharXiv能推动未来的研究，通过提供更真实、更具代表性的进步衡量标准，促进图表理解领域的研究。项目页面和排行榜可访问：https://charxiv.github.io/。|
|**2024-06-26**|**"Is ChatGPT a Better Explainer than My Professor?": Evaluating the Explanation Capabilities of LLMs in Conversation Compared to a Human Baseline**|Grace Li et.al.|[2406.18512](http://arxiv.org/abs/2406.18512)|null|### 概述  解释是知识共享的核心，它建立在沟通原理、社会动态和学习理论之上。我们专注于对话式的解释方法，因为其环境高度适应性和交互性。我们的研究利用了解释行为框架，这是一个理解解释者和被解释者在对话中如何运用策略进行解释、理解和互动的工具。我们利用Wachsmuth等人构建的WIRED YouTube系列数据集，并由Booshehri等人进行了带有解释行为的标注，这些注释为我们理解对话中解释者如何构建回应提供了依据。  随着去年生成式人工智能的发展，我们期望更好地理解大型语言模型（LLMs）的能力，以及它们如何增强专家解释者的对话交流能力。为此，我们使用了Booshehri等人2023年标注的5-Levels数据集来评估LLMs在解释性对话中的表现。为了评价LLMs生成解释者回应的有效性，我们设计了三种策略：人类解释者的原始回应、GPT4的标准回应以及加入了解释步骤的GPT4回应。我们邀请人类标注者对这三种策略进行评估。|
|**2024-06-26**|**Mental Modeling of Reinforcement Learning Agents by Language Models**|Wenhao Lu et.al.|[2406.18505](http://arxiv.org/abs/2406.18505)|null|## 背景 尽管现代语言模型已经展现出一定的推理能力，理论上能够表达任意可能的令牌分布，但它们如何利用预训练时积累的世界知识来理解物理世界中的代理行为，这一方面仍未得到充分探索。本研究首次实证考察大型语言模型（LLMs）在通过推理分析代理的行为及其对状态的影响，从而构建代理心理模型（agent mental modeling）的能力。这可能揭示出利用LLMs解析强化学习（RL）代理行为的潜力，这对于可解释强化学习（XRL）的关键挑战具有重要意义。为此，我们提出特定的评估指标，并在不同复杂度的RL任务数据集上进行测试，报告关于代理心理模型建立的研究结果。结果显示，当前的LLMs还无法仅通过推理完全实现代理的心理建模，这需要进一步创新。因此，这项工作提供了对现代LLMs能力和局限性的新见解。|
|**2024-06-26**|**Is In-Context Learning a Type of Gradient-Based Learning? Evidence from the Inverse Frequency Effect in Structural Priming**|Zhenghao Zhou et.al.|[2406.18501](http://arxiv.org/abs/2406.18501)|null|这篇论文探讨了大型语言模型（LLMs）的内插学习（in-context learning，ICL）能力，并将其与基于梯度的学习进行功能等效性诊断。研究者提出了一种新方法，利用逆频率效应（inverse frequency effect，IFE）来分析。IFE现象指的是在错误驱动的学习过程中，模型应对罕见样例产生的更新幅度大于常见样例。在心理学中，人类在结构化提示（如倾向于重复最近接触的句子结构）情境中表现出IFE，这表明其可能涉及错误驱动的学习机制。实验通过模拟结构化提示在ICL中的影响发现，LLMs同样显示出IFE，且这一效应在更大的模型中更为明显。因此，研究结果支持了ICL本质上是基于梯度的学习的假设，即在ICL的前向传播过程中隐含地计算了梯度。论文结论指出，人类和LLMs都使用了基于梯度的、错误驱动的处理机制。|
|**2024-06-26**|**Role-Play Zero-Shot Prompting with Large Language Models for Open-Domain Human-Machine Conversation**|Ahmed Njifenjou et.al.|[2406.18460](http://arxiv.org/abs/2406.18460)|null|近年来，人们提出了一系列方法来创建能够进行开放领域对话的大型语言模型（LLMs）。这些模型能回答用户问题，但局限于单向问答形式，而非真正的对话。通常，通过针对特定数据集进行微调来调整它们的交流风格，但这既昂贵又限于少数语言。本研究探索了角色扮演的零样本提示作为提高开放领域对话效率和成本效益的解决方案，利用多语言能力强的训练有素模型（Beeching等人，2023年），这些模型能遵循指令。我们设计了一个提示系统，当与遵循指令的模型——这里使用Vicuna（Chiang等人，2023年）结合时，能够生成在法语中的对话代理，在两项任务中甚至超越了经过微调的模型，并在人类评估中表现出色。|
|**2024-06-26**|**Cascading Large Language Models for Salient Event Graph Generation**|Xingwei Tan et.al.|[2406.18449](http://arxiv.org/abs/2406.18449)|**[link](https://github.com/xingwei-warwick/callmsae)**|由于长文档中事件检测、关系识别以及非结构化输入与结构化图谱的整合等任务的复杂性，从文本生成事件图谱是一项挑战。当前的研究往往同等重视所有事件，未能区分对理解叙事至关重要的关键事件。本文提出CALLMSAE，一个基于CAscading大型语言模型（LLMs）的SAlient Event图谱生成框架，它利用LLMs的能力，并避免了昂贵的人工标注需求。首先，通过提示LLMs生成摘要，我们识别出重要事件。然后，我们开发了一种迭代的代码精炼提示策略，用于生成事件关系图，消除错误的关系并恢复缺失的边。对基于上下文的图谱生成模型进行 fine-tuning，在使用 LLM 生成的图谱上表现出色，优于使用 CAEVO 生成数据训练的模型。在人类标注的测试集上的实验结果显示，我们的方法能生成更突出且准确的图谱，超越了竞争性的基线。|
|**2024-06-26**|**New intelligent empowerment for digital transformation**|Peng Yifeng et.al.|[2406.18440](http://arxiv.org/abs/2406.18440)|null|这项研究提出了一种基于大型语言模型（LLMs）的创新评估方法，用于衡量企业的数字化转型（DT）过程。通过对2005年至2022年间在纽约证券交易所和纳斯达克上市的4407家公司的年度报告进行分析，构建了一套全面的DT指标。研究结果显示，DT显著提高了企业的财务表现。然而，不同的数字技术对财务性能的影响各不相同，区块链技术的积极影响相对较小。此外，研究还发现DT通过提升运营效率和降低成本促进财务绩效增长。本研究为学术界提供了新的DT评估工具，同时拓宽了生成人工智能技术在经济研究中的应用范围。|
|**2024-06-26**|**IRCAN: Mitigating Knowledge Conflicts in LLM Generation via Identifying and Reweighting Context-Aware Neurons**|Dan Shi et.al.|[2406.18406](http://arxiv.org/abs/2406.18406)|null|人们普遍认为，大型语言模型（LLMs）在大规模数据训练后蕴含着丰富的知识。然而，近期研究揭示了LLMs生成文本时的知识冲突问题，即模型内编码的参数知识（即知识库）与上下文提供的新知识存在矛盾。为解决这一问题，我们提出了一种新颖框架——IRCAN（识别和重权上下文感知神经元）。IRCAN首先利用整合梯度计算得到的上下文感知归因分数，来识别那些对处理语境至关重要 的神经元。接着，通过重新赋权，我们强化这些识别出的上下文相关神经元，从而引导LLMs生成更符合上下文新知识的响应。我们在多种模型和任务上的广泛实验表明，IRCAN不仅显著提升了处理知识冲突的能力，还提供了一个可扩展的、即插即用的解决方案，能够无缝融入现有模型中。|
|**2024-06-25**|**MG-LLaVA: Towards Multi-Granularity Visual Instruction Tuning**|Xiangyu Zhao et.al.|[2406.17770](http://arxiv.org/abs/2406.17770)|**[link](https://github.com/phoenixz810/mg-llava)**|**## 背景  多模态大型语言模型（MLLMs）在视觉理解任务上取得了显著进步。然而，大多数模型局限于处理低分辨率图像，这限制了它们在需要详细视觉信息的感知任务中的表现。在我们的研究中，我们提出了一种创新的MLLM——MG-LLaVA，通过引入多尺度视觉流，包括低分辨率、高分辨率和对象级特征，来增强模型的视觉处理能力。我们设计了一个额外的高分辨率视觉编码器，以捕捉精细细节，并通过卷积门融合网络与基础视觉特征融合。为了进一步提升模型的对象识别能力，我们结合了来自离线检测器确定的边界框的物体级别特征。MG-LLaVA仅使用公开可用的多模态数据进行指令调优，展现出卓越的感知能力。我们用不同规模的语言编码器（从38亿到340亿参数）实例化MG-LLaVA，以全面评估其性能。多项基准测试的结果表明，MG-LLaVA在同类参数量的现有MLLM中表现出色，证明了其出色的效率。代码将在https://github.com/PhoenixZ810/MG-LLaVA上开源。**|
|**2024-06-25**|**BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context Learning**|Ercong Nie et.al.|[2406.17764](http://arxiv.org/abs/2406.17764)|null|## 背景 大型语言模型（LLMs）积累了丰富的参数知识，但由于重新训练成本高昂且对闭源模型不可行，更新这些知识变得困难。知识编辑（KE）作为一种可能的解决方案，允许在不损害整体性能的前提下更新LLMs的知识。基于“上下文学习”（ICL）的即席KE方法展现出巨大潜力，使得LLMs能够作为黑盒处理。过去，KE主要集中在英语环境，而当前以英语为中心的LLMs在跨语言KE方面的潜力尚未充分挖掘。为了推动这方面的更多研究，我们推出了BMIKE-53基准，该基准针对53种不同语言的三种KE任务类型进行评估。我们还提出了一种无梯度的KE方法——多语言上下文知识编辑（MIKE），并在BMIKE-53上进行了实验。我们的评估关注跨语言知识转移的可靠性、泛化性、局部性和可移植性，为未来跨语言KE的研究提供了有价值的观点和框架。我们的代码和数据已通过匿名仓库https://anonymous.4open.science/r/MIKE公开获取。|
|**2024-06-25**|**CaLMQA: Exploring culturally specific long-form question answering across 23 languages**|Shane Arora et.al.|[2406.17761](http://arxiv.org/abs/2406.17761)|**[link](https://github.com/2015aroras/calmqa)**|**## 背景  大型语言模型（LLMs）在长篇问答任务中广泛应用，它们需生成段落级别的答案来回应复杂问题。尽管英语的长篇问答研究已相当深入，涉及多种数据集和评估指标，但其他语言的研究却相对匮乏。为了弥补这一差距，我们推出了CaLMQA，一个包含2,600个跨23种语言的复杂问题集合，其中包括资源有限、鲜少研究的语言，如斐济语和基林迪语。我们的数据集既包括社区网络论坛上收集的自然出现的问题，也包含了由母语使用者撰写的题目，我们为此专门聘请了他们。这个过程产生了多样且复杂的题目，反映了文化主题（如传统、法律、新闻），以及母语使用者的语言习惯。  我们对一系列开源和闭源模型进行了自动评估，使用了我们新提出的CaLMScore指标，该指标能检测答案中的语言错误和重复词。结果显示，对于某些低资源语言，LLM生成的答案质量明显下降。我们在部分模型的人工评估中发现，对于具有文化特性的问题，模型表现显著低于文化中立的问题。这些发现强调了对LLM多语言能力及非英语长篇问答评价领域更深入研究的必要性。**|
|**2024-06-25**|**Accelerating Clinical Evidence Synthesis with Large Language Models**|Zifeng Wang et.al.|[2406.17755](http://arxiv.org/abs/2406.17755)|null|人工智能自动医学发现是许多人的梦想。为此，我们开发了一种名为TrialMind的生成式AI管道，旨在进行医学系统性回顾，涵盖研究搜索、筛选和数据提取阶段。该系统利用大型语言模型（LLMs）驱动每个环节，并引入专家监督以减少错误。为了评估性能，我们创建了TrialReviewBench基准数据集，它是一个定制的包含870份来自25篇元分析论文的临床研究标注数据，涵盖不同医疗治疗领域。结果显示，TrialMind显著提升了文献审查效率，在从超过2000万篇PubMed研究中检索相关研究时，召回率高达0.897至1.000。在筛选阶段，我们的方法优于基于传统语言模型嵌入的方法（召回率分别为0.227-0.246 vs. 0.000-0.102）。此外，我们的方法在结果提取方面超越了直接使用GPT-4的表现，准确率范围为0.65到0.84。我们还支持森林图中的临床证据综合，经八名人类标注员验证，他们普遍更偏好TrialMind，其在涉及的审查中胜出率为62.5%至100%。这些发现表明，基于LLM的临床证据合成方法，如TrialMind，能够促进可靠且高质量的临床证据合成，从而提升临床研究的效率。|
|**2024-06-25**|**Measuring and Benchmarking Large Language Models' Capabilities to Generate Persuasive Language**|Amalie Brogaard Pauli et.al.|[2406.17753](http://arxiv.org/abs/2406.17753)|null|本文探讨了在面对大量试图影响我们的信息，如预告消息、辩论、带有政治色彩的新闻和宣传时，大型语言模型（LLMs）生成具有说服力文本的能力。不同于以往专注于特定领域或类型劝说的研究，我们进行了一项全面的分析，旨在测量和基准LLMs在被明确要求增强或减少说服力时，以及仅要求进行释义时产生说服性文本的程度。为此，我们创建了一个新的数据集——“Persuasive-Pairs”，包含一组由简短文本和LLM重写以放大或削弱说服力的文本对。我们对这些配对进行了多标注，按相对尺度评估其说服力。这个数据集不仅本身具有价值，还展示了如何使用它训练一个回归模型，预测文本对之间说服力的得分，从而能够对不同领域的LLMs进行评分和比较。最后，我们讨论了不同系统提示对LLaMA3产生的影响，值得注意的是，即使在仅要求释义的情况下，不同的“角色”提示也会显著改变文本中的说服力。这些发现强调了研究LLM生成文本中的说服语言的重要性。|
|**2024-06-25**|**LLM Targeted Underperformance Disproportionately Impacts Vulnerable Users**|Elinor Poole-Dayan et.al.|[2406.17737](http://arxiv.org/abs/2406.17737)|null|在最新的大型语言模型（LLMs）展现出卓越性能的同时，关于它们的不可靠行为，如虚构和偏见的研究层出不穷。本研究探讨了LLMs的回答质量在信息准确性、真实性以及拒绝回答方面，如何随着三种用户特征的变化而变化：英语水平、教育程度和国籍。我们在三个最先进的LLMs和两个事实核查相关的数据集上进行了详尽实验，重点关注其真实性。研究结果表明，当前最先进的LLMs对英语能力较低、教育水平较低以及非美国籍用户的回答质量存在更明显的负面倾向，这使得这些模型对于其最弱势用户来说，并非可靠的信息来源。|
|**2024-06-25**|**FedBiOT: LLM Local Fine-tuning in Federated Learning without Full Model**|Feijie Wu et.al.|[2406.17706](http://arxiv.org/abs/2406.17706)|**[link](https://github.com/HarliWu/FedBiOT)**|大型语言模型（LLMs）在经过适当领域特定数据的微调后，在许多任务上展现出出色性能。然而，这类专用数据通常分布在多个所有者之间，这就提出了如何在联邦学习（FL）中进行LLM微调的问题。面对有限的计算和通信能力，FL客户端在有效微调大型语言模型时面临挑战。为此，我们介绍了FedBiOT，一种旨在提高资源效率的LLM微调FL方法。具体来说，我们的方法包括服务器生成一个压缩的LLM，并确保其性能与完整模型相当。然后，客户端针对这个压缩模型的一个轻量但重要的部分——适配器进行微调。值得注意的是，由于服务器无法访问客户端拥有的私人数据，服务器用于校准的数据分布与客户端用于微调的数据不同。我们将问题建模为一个带有数据不一致性影响的 bilevel 优化问题，并导出了服务器和客户端的更新规则。我们在 LLaMA-2 上进行了广泛实验，结果显示，适配器在重新整合到全局语言模型时表现出色。实验结果还表明，FedBiOT 相比现有基准显著减少了资源消耗，同时保持了相近的性能水平。|
|**2024-06-25**|**From Distributional to Overton Pluralism: Investigating Large Language Model Alignment**|Thom Lake et.al.|[2406.17692](http://arxiv.org/abs/2406.17692)|**[link](https://github.com/thomlake/investigating-alignment)**|**该研究分析了大型语言模型（LLMs）经过校准后输出分布的变化特性。首先，重新评估了之前关于校准后响应多样性降低的报告，发现这种下降主要归因于质量控制和信息整合。校准能够抑制不相关和无帮助的内容，同时使输出分布倾向于更长的、涵盖多个基础LLM响应信息的答案，实质上是将多样化信息汇总在单个响应中。研究并未发现校准显著减少有用信息，进而引出问题：校准模型是否会产生基础模型无法再现的信息？第二部分的研究结果表明，情况并非如此，校准模型的行为可以通过基础模型在无需微调的情况下进行复现。通过上下文示例和较低分辨率的语义提示，可以从基础LLMs引导出与校准后的相似响应，甚至与校准后的响应之间的相似度接近。这些发现支持“表面校准假设”，即当前的校准技术仅捕捉了助手型基础LLM行为中有用的部分，并未扩展其能力。此外，它们还显示，基于上下文的校准作为一种模仿校准LLMs的策略，效果出人意料地好，且无需微调。研究代码和数据可在<https://github.com/thomlake/investigating-alignment>获取。**|
|**2024-06-25**|**VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation**|Kun Qian et.al.|[2406.17681](http://arxiv.org/abs/2406.17681)|**[link](https://github.com/qbetterk/VarBench)**|随着大型语言模型在传统基准测试中的表现日益出色，越来越多的研究人员开始关注预训练期间的基准数据泄露问题，通常称为数据污染问题。为了确保公正的评估，最近的基准测试仅公开训练和验证集，对测试集标签保密。他们要求任何希望评估自己语言模型的人都需要提交模型的预测结果，进行集中处理，然后在排行榜上公布模型的得分。然而，这个提交过程既低效又妨碍了有效的错误分析。为解决这个问题，我们提出动态化基准测试并实时评估语言模型。具体来说，我们从每个测试案例中提取变量，并为每个变量定义一个值范围。每次评估时，我们会从这些值域中抽取新的值来创建独特的测试案例，从而保证每次都是全新的评估。  我们针对数学生成任务的GSM8K、多项选择任务的ARC、commonsense问答的CommonsenseQA以及TruthfulQA的真实性问答任务，应用了这种变量扰动方法。实验结果显示，这种方法能更准确地衡量语言模型的真实能力，有效缓解了数据污染问题。|
|**2024-06-25**|**Quantifying AI Psychology: A Psychometrics Benchmark for Large Language Models**|Yuan Li et.al.|[2406.17675](http://arxiv.org/abs/2406.17675)|null|大型语言模型（LLMs）展现出卓越的任务解决能力，日益扮演类似人类助手的角色。社会对将LLMs更广泛地融入其中产生了兴趣，探讨它们是否具备心理特质，以及这些特质是否稳定且有助于理解其行为。本文借鉴心理学测量学的方法，提出了一种框架，用于研究LLMs中的心理学，包括心理维度识别、评估数据集创建和结果验证。在此框架下，我们开发了一个全面的LLM心理测量基准，涵盖了六种心理维度：个性、价值观、情绪、心智理论、动机和智力。这个基准包含了十三个包含多样场景和题型的数据集。研究发现，LLMs展现出广泛的心理特性。同时，我们观察到LLMs在自我报告的特质与其实际行为之间的不一致。该论文详细展示了LLMs的心理测量评估，为AI和社会科学领域的可靠评估提供了洞见，以及可能的应用方向。|
|**2024-06-24**|**EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees**|Yuhui Li et.al.|[2406.16858](http://arxiv.org/abs/2406.16858)|**[link](https://github.com/safeailab/eagle)**|在现代大型语言模型（LLMs）的推理过程中，成本高且耗时。实验表明，投机取巧的抽样方法如EAGLE已证实有效。传统方法假设草稿树的接受率仅依赖于令牌的位置，然而我们发现这其实还取决于上下文。为此，我们在EAGLE的基础上提出了EAGLE-2，引入了一种新的上下文感知动态草稿树技术到起草建模中。这一改进利用了EAGLE的草稿模型校准良好的特性：草稿模型的信心分数能近似表示接受率，误差较小。我们在三个系列的LLMs和六个任务上进行了广泛评估，结果显示EAGLE-2的速度提升比率为3.05倍到4.26倍，比EAGLE-1快20%到40%。此外，EAGLE-2还能保持生成文本分布不变，因此是一个无损加速算法。|
|**2024-06-24**|**From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models**|Sean Welleck et.al.|[2406.16838](http://arxiv.org/abs/2406.16838)|null|现代研究中最引人注目的发现之一是，在大型语言模型（LLMs）的训练过程中增加计算资源会带来更好的性能。然而，对于推断时的优化方法的关注相对较少。这篇综述专门探讨了这些推断时间的方法。我们从统一的数学框架出发，考察了三个领域：逐词生成算法、元生成算法和高效生成。逐词生成算法，通常称为解码算法，通过一次抽样一个token或构建词级搜索空间，然后选择输出。这些方法通常假设能够访问语言模型的logits、下一个token分布或概率分数。元生成算法处理部分或完整序列，融入领域知识，支持回溯，并整合外部信息。高效生成方法旨在减少token成本，提高生成速度。我们的综述融合了来自传统自然语言处理、现代LLMs和机器学习系统三个研究社区的观点。|
|**2024-06-24**|**USDC: A Dataset of $\underline{U}$ser $\underline{S}$tance and $\underline{D}$ogmatism in Long $\underline{C}$ onversations**|Mounika Marreddy et.al.|[2406.16833](http://arxiv.org/abs/2406.16833)|null|在当前的背景下，识别用户在各种话题的长篇讨论中的观点和立场对于个性化、市场研究、政治竞选、客户服务、冲突解决、定向广告和内容管理至关重要。然而，手动标注数据以训练此类模型面临诸多挑战，如耗时昂贵、长对话可能引入噪声，以及用户观点转变的微妙之处可能导致解读困难。鉴于大型语言模型（LLMs）在复杂自然语言处理任务中的出色表现，本文尝试利用Mistral Large和GPT-4自动化两个关键任务的标注过程，并提供推理：一是用户立场分类，即在对话中对用户帖子的观点进行五级标注；二是用户固执程度分类，关注用户在整个对话中的总体意见，采用四级标注。通过在764个多用户Reddit对话上应用零样本、一示例和少量样例标注的多数投票，我们创建了USDC数据集。然后，我们使用这个数据集对多个小型部署语言模型进行微调和指令调整，用于执行五类立场和四类固执程度的分类任务。我们公开了代码和数据集：[https://anonymous.4open.science/r/USDC-0F7F]。|
|**2024-06-24**|**Ragnarök: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track**|Ronak Pradeep et.al.|[2406.16828](http://arxiv.org/abs/2406.16828)|**[link](https://github.com/castorini/ragnarok)**|## 背景  您可能体验过新的Bing搜索或Google AI概述？这些都反映出当前搜索引擎正逐步发展到基于检索增强生成（RAG）的系统。这类系统能整合实时数据到大型语言模型（LLMs），提供信息丰富、有来源且简洁的摘要，与传统的文档排名展示方式形成对比。因此，为了推动RAG系统评估的创新，我们提议在TREC 2024年增设RAG竞赛。本文详述了我们如何实现这一目标：描述了可复用框架Ragnar\"ok的设计，解释了MS MARCO V2.1语料库的选择，发布了竞赛开发话题，并标准化了用户接口定义，以便利用户。接下来，我们将利用Ragnar\"ok展示关键的工业基准，如OpenAI的GPT-4o和Cohere的Command R+。我们还推出了一个网页界面，用于互动式地比较不同RAG系统的性能，并通过众包方式进行评估。我们开源Ragnar\"ok框架和基准，旨在为未来的RAG系统建立统一的标准。|
|**2024-06-24**|**RES-Q: Evaluating Code-Editing Large Language Model Systems at the Repository Scale**|Beck LaBash et.al.|[2406.16801](http://arxiv.org/abs/2406.16801)|**[link](https://github.com/qurrent-ai/res-q)**|**## 翻译  大型语言模型（LLMs）的指令跟随能力促使了一类能够处理复杂任务的系统发展，如对大型代码仓库进行编辑。鉴于LLMs对提示微调的高敏感性和不可预测性，迫切需要稳健的评估工具来推动这些系统的未来发展。我们提出RES-Q，一个针对 $\textbf{R}$epository $\textbf{E}$diting $\textbf{S}$ ystems的自然语言指令基准，它基于100个真实的GitHub提交构建了100个仓库编辑任务。给定编辑指令和代码仓库，RES-Q评估LLM系统获取信息并构造满足指令要求的编辑的能力。我们认为，这种评估方式优于传统方法，能全面评估模型的性能。  我们使用Qurrent OS开发的语言代理软件构建了一个仓库编辑系统，对该系统中的各种最先进的LLMs，如Claude Sonnet 3.5和GPT-4o，进行了评估。尽管在HumanEval上的1%精确度@1得分有所差异，但在RES-Q上，Claude Sonnet 3.5的1%精确度@1得分比GPT-4o高出12%，这表明RES-Q具有区分模型能力的潜力，随着传统基准接近饱和，它能提供更深入的洞察。  我们还研究了token效率、与现有基准的性能关联，以及封闭源和开源LLM之间的有趣差异。相关代码和数据集可在https://github.com/Qurrent-AI/RES-Q获取。**|
|**2024-06-24**|**Lottery Ticket Adaptation: Mitigating Destructive Interference in LLMs**|Ashwinee Panda et.al.|[2406.16797](http://arxiv.org/abs/2406.16797)|**[link](https://github.com/kiddyboots216/lottery-ticket-adaptation)**|**## 背景 当前的大规模语言模型（LLMs）适应新任务的方法并不适用于多任务适应，因为它们会修改所有模型权重，导致不同任务之间产生破坏性的干扰。这可能导致对先前任务的遗忘，使得同时在多个任务上获得良好性能变得困难。为了解决这个问题，我们提出了Lottery Ticket Adaptation（LoTA），这是一种稀疏适应方法，它识别并优化模型中的一个稀疏子网络。我们在诸如指令跟随、推理、数学和摘要等复杂任务上评估了LoTA。  ## 方法 LoTA通过发现和优化“彩票券”（或稀疏任务向量）来实现，这种方法优于全量微调和低秩适应（LoRA）。LoTA不仅表现出更好的性能，还能在训练其他任务后保持良好的表现，从而避免了灾难性遗忘。此外，通过提取和针对特定任务进行微调，LoTA还支持在高度不同的任务间进行模型融合。  ## 结论 总的来说，LoTA作为一种有效的稀疏适应策略，为多任务大语言模型的适应提供了新的解决方案，能够在处理多个任务时保持稳定且高效的表现。**|
|**2024-06-24**|**M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in Large Language Models**|Rishabh Maheshwary et.al.|[2406.16783](http://arxiv.org/abs/2406.16783)|null|## 背景  在大型语言模型（LLMs）遵循指令的校准过程中，微调（finetuning, IFT）至关重要。近期已经提出了一些有效的IFT数据集，但大多集中在高资源语言如英语上。本研究中，我们创新性地提出一个全合成的、基于Evol分类法引导的多语言、多轮指令微调数据集——M2Lingual，目标是提升LLMs在多样语言和任务上的表现。M2Lingual共包含182,000个IFT对，源自不同种子，涵盖70种语言、17个NLP任务以及通用的指令-响应对。  ## 目的与贡献  使用M2Lingual进行训练的LLMs性能显著优于大多数现有的多语言IFT数据集。更重要的是，经M2Lingual微调的模型在各种评估基准上展现出稳健的跨语言能力，无论是在我们的多语言、多轮翻译评价基准上，还是在多种多样的多语言任务中。因此，我们贡献了Evol分类法的两步方法，并公开了M2Lingual的数据集：https://huggingface.co/datasets/ServiceNow-AI/M2Lingual。|
|**2024-06-24**|**It Is Not About What You Say, It Is About How You Say It: A Surprisingly Simple Approach for Improving Reading Comprehension**|Sagi Shaier et.al.|[2406.16779](http://arxiv.org/abs/2406.16779)|null|过去十年，自然语言处理领域取得了显著进步。然而，一些实践未经充分评估就已确立。针对阅读理解这一情况，我们首先提出问题：1）输入顺序（即问题和上下文）如何影响模型性能？鉴于近期在输入侧重领域的进展，我们进一步探究：2）强调问题、上下文或两者是否能提升表现？我们在3个数据集上测试了9种大型语言模型，发现先呈现上下文再给出问题可以提高模型性能，最高可达31%的准确率提升。此外，强调上下文的效果优于突出显示问题，而且对模型缺乏参数知识来回答的问题，针对性地强调输入部分尤其有效。通过尝试基于提示和注意力的强调方法，我们发现最有效的策略出人意料地简单：只需在输入中附加几个标记，就能实现高达36%的准确性提升，使得小型模型能够超越其大得多的同类模型。|
|**2024-06-24**|**Blending LLMs into Cascaded Speech Translation: KIT's Offline Speech Translation System for IWSLT 2024**|Sai Koneru et.al.|[2406.16777](http://arxiv.org/abs/2406.16777)|null|## 背景  大型语言模型（LLMs）正在被广泛研究，以应用于诸如语音识别（ASR）、机器翻译（MT）甚至端到端语音翻译（ST）等任务。本文介绍KIT团队在受限+LLM赛道下的离线提交，我们通过整合最新技术改进了级联语音翻译系统。特别地，我们将Mistral-7B模型\footnote{mistralai/Mistral-7B-Instruct-v0.1}融入其中，从两个方面增强系统：一是利用我们的系统生成的N-best列表精炼ASR输出，通过微调LLM提高转录准确性；二是对MT输出进行文档级别的精炼，利用ASR和MT预测来提升翻译质量。结果显示，LLM的集成使得ASR的Word Error Rate下降了绝对0.3%，MT的COMET评分提高了0.65%。然而，在包含重叠说话者和背景噪音的挑战性测试集中，由于ASR性能不佳，LLM集成的效果不明显。为了改善在这种情况下可能缺失的上下文信息，我们采用了分块长形式解码的ASR方法。|
|**2024-06-24**|**WARP: On the Benefits of Weight Averaged Rewarded Policies**|Alexandre Ramé et.al.|[2406.16768](http://arxiv.org/abs/2406.16768)|null|### 翻译  强化学习从人类反馈（RLHF）通过训练奖励模型来调整大型语言模型（LLMs），使其生成的内容符合人类偏好。为了保持预训练知识，RLHF通常采用KL散度正则化，但这会限制奖励优化。为此，本文提出了一种新颖的对齐策略，称为权重平均奖励策略（WARP）。WARP在三个阶段在权重空间中融合策略：首先，它使用指数移动平均策略作为KL正则化的动态基准。其次，应用球面插值将独立微调的策略合并成一个增强模型。最后，线性插值在合并模型和初始模型之间进行，以恢复预训练特征。该过程迭代进行，每次迭代的最终模型用作下一轮的高级初始化，逐步优化KL与奖励之间的权衡，实现固定KL下的更高奖励。GEMMA策略的实验验证了WARP的优点，其质量和对齐性能优于开源的LLMs。|
|**2024-06-21**|**GenoTEX: A Benchmark for Evaluating LLM-Based Exploration of Gene Expression Data in Alignment with Bioinformaticians**|Haoyang Liu et.al.|[2406.15341](http://arxiv.org/abs/2406.15341)|**[link](https://github.com/liu-hy/genotex)**|**## 翻译  近年来，机器学习的进步显著提升了从基因表达数据中识别疾病相关基因的能力。然而，这些过程往往需要深厚的专长和大量的人工努力，限制了其可扩展性。大型语言模型（LLMs）驱动的代理显示出在自动化此类任务方面的潜力，因为它们的问题解决能力日益增强。为了支持这类方法的评估和发展，我们创建了GenoTEX，这是一个基因表达数据分析自动探索的基准，包括数据集选择、预处理和统计分析任务。GenoTEX提供了全面的分析管道，其中包含了人类生物信息学家精心编写的注释，他们对数据集进行深入分析以确保准确性和可靠性。  为了提供这些任务的基线，我们设计了GenoAgents，这是一个基于LLMs的代理团队，具备上下文感知规划、迭代校正以及与领域专家咨询的能力，它们协作探索基因数据集。我们的实验显示了LLM驱动方法在基因组数据分析中的潜力，而错误分析指出了挑战和未来的改进方向。我们提议GenoTEX作为一个有前景的资源，用于衡量和提升人工智能驱动的基因组数据分析方法。我们的基准已公开发布在：\url{https://github.com/Liu-Hy/GenoTex}。**|
|**2024-06-21**|**Gradient-Mask Tuning Elevates the Upper Limits of LLM Performance**|Haoling Li et.al.|[2406.15330](http://arxiv.org/abs/2406.15330)|null|大型语言模型（LLMs）已经在众多研究领域带来了革新。尽管人们普遍知道微调对于增强LLMs的功能至关重要，但现有研究表明，微调过程中可能存在参数冗余。因此，有研究建议只更新部分参数，但这未能有效利用任务特定信息来识别训练中的重要参数。考虑到梯度本质上蕴含着任务相关数据的信息，我们提出了梯度掩码调优（Gradient-Mask Tuning，GMT）方法，该方法根据参数的梯度信息选择性地进行训练更新。具体来说，我们计算梯度的绝对值，并对较小幅度的梯度应用掩码。我们的实验结果表明，GMT不仅优于传统的微调方法，还提升了LLM性能的上限。进一步分析显示，GMT对掩码比例具有一定的鲁棒性，并且在计算效率上与基本的微调（Simple Fine-Tuning，SFT）相当。|
|**2024-06-21**|**Bug In the Code Stack: Can LLMs Find Bugs in Large Python Code Stacks**|Hokyung Lee et.al.|[2406.15325](http://arxiv.org/abs/2406.15325)|**[link](https://github.com/hamminghq/bug-in-the-code-stack)**|近年来，针对针对于大型语言模型（LLMs）在海量文本文档中检索上下文信息的Needle-in-a-Haystack（NIAH）基准研究有所进展。随着LLMs在软件开发流程中的日益融合，评估它们在代码环境中的表现变得至关重要。随着LLMs朝着程序合成方向发展，必须确保它们能理解语法并编写出符合语法规则的代码。为此，我们设计了Bug In The Code Stack（BICS）基准测试，旨在检验LLMs识别简单语法错误的能力于大型源代码中。我们的研究发现三个关键点：（1）与文本环境相比，基于代码的环境对检索任务构成了更大的挑战；（2）不同模型之间的性能存在显著差异；（3）尽管如此，较长的上下文长度与性能下降之间存在关联，但这种下降程度在不同的模型间有所不同。|
|**2024-06-21**|**Towards Fine-Grained Citation Evaluation in Generated Text: A Comparative Analysis of Faithfulness Metrics**|Weijia Zhang et.al.|[2406.15264](http://arxiv.org/abs/2406.15264)|null|大型语言模型（LLMs）常常产生不可靠或难以验证的信息，即“幻觉”。为解决这个问题，检索增强的LLMs引入了引用，使内容基于可核查的来源。然而，手动评估引用是否充分支持相关陈述仍然是一个重大挑战。先前的研究试图通过信仰度指标自动估计引用的支持程度，但这些方法仅限于二分类，忽视了实际场景中对精细级别引用支持的考量。为了探究信仰度指标在精细级别评估中的有效性，我们提出了一种比较评估框架，用于检验这些指标在区分三种支持等级（全面、部分和无支持）之间的能力：全面支持、部分支持和不支持。我们的框架采用相关性分析、分类评估和检索评估，全方位衡量指标分数与人类判断的一致性。研究结果显示，没有单一指标在所有评估中表现出色，揭示了精细级别支持评估的复杂性。根据发现的结果，我们为开发更有效的指标提供了实用建议。|
|**2024-06-21**|**Detecting Synthetic Lyrics with Few-Shot Inference**|Yanis Labrak et.al.|[2406.15231](http://arxiv.org/abs/2406.15231)|null|近年来，生成的音乐内容逐渐受到关注，大型语言模型被有效应用于创作各种风格、主题和语言结构的歌词，这推动了艺术家们的创作，但也带来了版权侵犯、消费者满意度和内容滥发等问题。为此，检测生成歌词的方法变得至关重要。然而，现有的研究并未专注于这一特定领域或创意文本的机器生成内容检测。针对这一空白，我们精心构建了首个高质量合成歌词数据集，并对多种基于少量样本的检测方法进行了详尽的定量评估，测试它们的泛化能力，并辅以人类评价。结果显示，我们的最佳少数样本检测器——基于LLM2Vec的方法超越了在其他领域表现强劲的风格和统计方法，成功鉴别出人类创作与机器生成的歌词，且展现出良好的跨艺术家和模型泛化能力，还能有效识别生成后的人工润色。这项研究强调了在创意内容检测领域，特别是泛化能力和对更大歌曲库的适应性方面，需要进一步研究。所有数据集、预处理脚本和代码已公开在GitHub和Hugging Face上，遵循Apache 2.0许可协议。|
|**2024-06-21**|**A LLM-Based Ranking Method for the Evaluation of Automatic Counter-Narrative Generation**|Irune Zubiaga et.al.|[2406.15227](http://arxiv.org/abs/2406.15227)|null|随着网络上错误信息和有害言论的增多，迫切需要有效的反叙事（Counter Narrative，CN）生成技术。然而，现有的自动评估方法往往缺乏可解释性，无法准确反映生成的CN与人类感知之间的复杂关系。为此，本文提出了一种新颖的方法来评估生成的CN，即利用大型语言模型（Large Language Model，LLM）作为评估器。通过以锦标赛形式对生成的CN进行对战比较，我们建立了一个模型排名流程，其与人类偏好间的相关系数达到0.88。此外，我们还探讨了使用LLM进行零样本（Zero-Shot，ZS）CN生成的能力，对比分析了聊天、指令和基础模型的性能和局限性。通过细致的评估，包括微调实验，我们揭示了在特定领域数据下的响应差异。结论是，对于执行这项任务，如果能避免因安全顾虑而拒绝生成，聊天导向的ZS模型可能是最佳选择。|
|**2024-06-21**|**Unsupervised Extraction of Dialogue Policies from Conversations**|Makesh Narsimhan Sreedhar et.al.|[2406.15214](http://arxiv.org/abs/2406.15214)|null|## 翻译  对话策略在构建任务导向的对话系统中至关重要，但其开发和维护往往需要对话建模专家的大量投入。尽管在许多情况下，手头有大量的对话数据，但人们缺乏有效的方法从这些数据中提取对话策略。为此，本文通过展示大型语言模型（LLMs）如何在对话数据转化为统一的中间表示——规范形式的过程中发挥作用，填补了这一空白。接着，我们提出了一种新颖的利用可控且可解释的图基方法生成对话策略的技术。通过将对话中的规范形式整合成流程网络，我们发现运行图遍历算法有助于提取对话流程。相比仅依赖LLM提取的流程，这些流程更好地反映了底层交互。我们的方法旨在赋予对话设计者更大的控制力，提供一个提升对话策略开发效率的工具。|
|**2024-06-21**|**Prompting Whisper for QA-driven Zero-shot End-to-end Spoken Language Understanding**|Mohan Li et.al.|[2406.15209](http://arxiv.org/abs/2406.15209)|null|## 背景 零样本语音语言理解（SLU）使系统能够在无需先前训练数据的新领域理解用户话语。当前的研究往往依赖大型语言模型（LLMs），导致庞大的存储需求和复杂性。本文提出使用 Whisper，一个独立的语音处理模型，来进行零样本端到端（E2E）SLU。为处理未见过的语义标签，我们将SLU任务融入问答（QA）框架中，通过提示Whisper解码器进行语义推断。我们采用前缀调优方法高效地训练该系统，只优化少量参数，而不是整个Whisper模型。实验结果显示，我们的提议系统在SLURP上的槽位填充（SLU-F1）得分比最近引入的零样本基准提高了40.7%。此外，在既定和跨领域评估环境下，它与基于Whisper-GPT-2的模块化系统表现相当，但模型参数减少了34.8%。|
|**2024-06-21**|**Exploring the Efficacy of Robotic Assistants with ChatGPT and Claude in Enhancing ADHD Therapy: Innovating Treatment Paradigms**|Santiago Berrezueta-Guzman et.al.|[2406.15198](http://arxiv.org/abs/2406.15198)|null|注意力缺陷多动障碍（ADHD）是一种神经发育障碍，其特征为注意力不集中、过度活跃和冲动，严重影响个体的日常生活和生活质量。职业疗法在ADHD管理中扮演着关键角色，通过培养日常生活所需的技能，提升个体在学校、家庭和社会环境中全面参与的能力。近期研究强调了大型语言模型（如ChatGPT和社交辅助机器人）在心理治疗中的潜在价值，以弥补现有疗法的局限，提供定制化的支持并适应个体的独特需求。然而，关于这些先进技术在ADHD疗法中的联合应用研究尚存在较大空白。因此，我们整合了ChatGPT-4 Turbo和Claude-3 Opus两个先进语言模型到一个机器人助理中，以考察它们在机器人辅助互动中的性能，并在一个模拟治疗场景中比较它们与临床验证的定制模型的效果。研究结果显示，ChatGPT-4 Turbo在性能和响应速度上表现出色，适合于时间敏感的应用。而Claude-3 Opus在理解、连贯性和伦理考量方面表现出优势，强调安全和吸引人的互动。两者都展现出创新和适应性，但ChatGPT-4 Turbo在集成简易度和语言支持方面更具优势。选择哪个模型取决于ADHD疗法的具体需求。|
|**2024-06-21**|**UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis**|Yulong Hui et.al.|[2406.15187](http://arxiv.org/abs/2406.15187)|**[link](https://github.com/qinchuanhui/uda-benchmark)**|**## 翻译  尽管检索增强生成（Retrieval-Augmented Generation, RAG）技术提升了大型语言模型（Large Language Models, LLMs）与外部数据的协作能力，但在现实场景中仍面临诸多挑战。特别是在学术文献和金融问答等领域，数据常常以HTML或PDF格式的冗长、结构复杂的文本和表格形式存在。为此，我们提出一个名为“Unstructured Document Analysis”（UDA）的新基准，它包含2,965份真实世界的文档和29,590个专家标注的问答对。我们重新审视了基于LLM和RAG的方法在处理文档分析任务中的设计决策，并在多个文档领域和多样化的查询类型上评估答案质量和策略。  我们的评估揭示了有趣的结果，强调了数据解析和检索的重要性。我们希望这个基准能够为现实世界的文档分析应用提供启示，并为其发展服务。基准套件和代码已可在<https://github.com/qinchuanhui/UDA-Benchmark>获取。**|
|**2024-06-20**|**Model Merging and Safety Alignment: One Bad Model Spoils the Bunch**|Hasan Abed Al Kader Hammoud et.al.|[2406.14563](http://arxiv.org/abs/2406.14563)|null|## 背景 大型语言模型（LLMs）的合并是一种经济高效的方法，可以将多个专家级LLMs整合成一个全能模型，保留原始模型的专业知识。然而，当前的方法往往忽视了合并过程中安全对齐的重要性，导致生成的模型高度不一致。本研究探讨了模型合并对对齐性的影响。我们评估了几种流行的模型合并技术，发现现有方法不仅传递了领域专业知识，还传播了不一致性。为此，我们提出了一种两步法解决方案：(1) 生成合成的安全性和领域特定数据，(2) 将这些生成的数据融入现有的数据驱动的模型合并优化过程中。这样，我们能够将对齐性视为可以最大化于合并后LLM中的能力。实验表明，在合并过程中整合对齐相关数据的有效性，结果是既能保持领域专长又能实现良好对齐的模型。|
|**2024-06-20**|**Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities**|Sachit Menon et.al.|[2406.14562](http://arxiv.org/abs/2406.14562)|null|当面临涉及视觉思维的问题时，人类会自然地切换到推理模式，常常形成心理图像或绘制视觉辅助工具。大型语言模型在数学和符号推理方面展现出良好表现，通过文本形式表达中间推理步骤的链条思考，但在处理可以通过视觉推理轻松解答的文本查询时仍存在问题，即使经过大量的多模态预训练也是如此。我们提出了一种简单方法，即“白板思维提示”，来解锁多模态大型语言模型在跨模态中的视觉推理能力。白板思维提示为模型提供了一个比喻性的“白板”，让其以图像形式展现推理步骤，然后将这些图像返回模型进行进一步处理。我们发现这种方法无需示范或专用模块，而是利用模型现有的使用Matplotlib和Turtle等库编写代码的能力。这个简单策略在四个涉及视觉和空间推理的困难自然语言任务中实现了最先进的结果。我们发现，与链式思考相比，GPT-4o在某些场景下大幅失败，包括一些准确率为0%的情况下，而白板思维提示能提升至高达92%的准确性。我们详细探讨了该技术的成功之处及其错误来源。|
|**2024-06-21**|**Asynchronous Large Language Model Enhanced Planner for Autonomous Driving**|Yuan Chen et.al.|[2406.14556](http://arxiv.org/abs/2406.14556)|null|尽管实时规划器在自动驾驶中表现出色，但大型语言模型（LLMs）的兴起为提高运动规划的可解释性和可控性开辟了新途径。然而，LLM驱动的规划器仍面临资源消耗大和推理时间长的问题，这阻碍了其实用部署。鉴于这些挑战，我们提出了AsyncDriver，一个全新的异步LLM增强的闭环框架。该框架利用LLM生成的与场景相关的指令特征，指导实时规划器进行精确和可控的轨迹预测。AsyncDriver展示了LLMs在理解和处理向量化场景数据及一系列路线指示方面的强大能力，同时通过异步设计，有效降低了LLM带来的计算成本，保持了与之相近的性能。实验表明，我们的方法在nuPlan的复杂场景中实现了更优的闭环评估性能。|
|**2024-06-20**|**GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models**|Shilong Li et.al.|[2406.14550](http://arxiv.org/abs/2406.14550)|null|长文本处理能力对于大型语言模型（LLMs）应对复杂任务至关重要。尽管已有多方努力优化LLMs处理长输入，但依然面临挑战。本文提出GraphReader，这是一种基于图的代理系统，旨在通过构建文本图并让代理自主探索来处理长文本。当接收到问题时，代理会逐步分析并制定合理计划，然后调用预定义函数读取节点内容和邻居信息，实现从粗到细的图探索。在探索过程中，代理不断记录新发现并反思当前情况，以优化获取信息的过程，直到收集足够信息生成答案。在LV-Eval数据集上的实验显示，使用4k上下文窗口的GraphReader在16k到256k的长文本长度上，相对于GPT-4-128k有显著优势。此外，我们的方法在四个单跳和多跳的挑战性基准上表现出色。|
|**2024-06-20**|**Uncovering Latent Memories: Assessing Data Leakage and Memorization Patterns in Large Language Models**|Sunny Duan et.al.|[2406.14549](http://arxiv.org/abs/2406.14549)|null|随着大型语言模型的兴起，自然语言处理任务发生了革命性变化，但这也引发了数据隐私和安全的重大忧虑。这些模型在包含潜在敏感或专有信息的大量语料库上进行训练，数据泄露的风险——即模型响应揭示部分信息——尚不为人充分理解。本研究旨在探讨机器学习模型中的记忆现象，特别是关注其在训练过程中的演变。我们调查了训练数据的统计特性如何影响模型内编码的记忆，通过评估重复对记忆的影响。研究发现，模型记住一个序列的概率与它在数据中出现的次数呈对数关系。此外，我们发现即使没有后续的接触，某些看似未被记住的序列也可能在整个训练过程中逐渐显现。这种隐藏的已记住序列对数据隐私构成挑战，因为它们可能隐藏在模型的最终检查点中。因此，我们开发了一种诊断测试，通过考虑它们的交叉熵损失来揭示这些潜在的记忆序列。|
|**2024-06-20**|**Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data**|Johannes Treutlein et.al.|[2406.14546](http://arxiv.org/abs/2406.14546)|**[link](https://github.com/choidami/inductive-oocr)**|**针对大型语言模型（LLMs）的安全风险，一个策略是从其训练数据中删除危险知识。尽管这消除了显性信息，但隐性信息可能仍散落在多个训练文档中。我们研究的问题是：LLMs能否通过拼凑这些隐含线索，推断出被屏蔽的知识？为此，我们专注于无上下文归纳推理（Inductive Out-of-Context Reasoning，OOCR），这是一种泛化能力，要求LLMs根据分布在训练文档中的证据推断潜在信息，并在无需上下文学习的情况下应用于下游任务。通过五个任务的实验，我们展示了前沿LLMs确实具备这种能力。例如，在一项实验中，仅对一个未知城市与其与其他已知城市之间的距离进行微调，令人惊讶的是，即使没有示例或链式思考，该LLM也能表述出未知城市是巴黎，并据此解答后续问题。进一步的实验表明，仅接受单个硬币抛掷结果训练的LLMs能判断硬币是否偏斜，而只接触 $(x, f(x))$对的模型能阐述$f$ 的定义并计算逆运算。虽然OOCR在某些情况下表现良好，但我们也发现它并不总是可靠的，特别是在小型LLMs学习复杂结构时。总的来说，LLMs无需明确的上下文学习就能“串联起”信息，这给监控和控制它们获取的知识带来了潜在挑战。**|
|**2024-06-20**|**Unmasking Database Vulnerabilities: Zero-Knowledge Schema Inference Attacks in Text-to-SQL Systems**|Đorđe Klisura et.al.|[2406.14545](http://arxiv.org/abs/2406.14545)|null|关系数据库在现代信息系统中至关重要，是存储、查询和管理数据的核心。随着大语言模型的进步，文本到SQL技术崭露头角，极大地提升了从数据库中获取信息的能力，但同时也引发了关于隐私和安全的担忧。我们的研究专注于提取文本到SQL模型所依赖的数据库模式元素。了解模式可能使SQL注入攻击更为容易。为此，我们设计了一种零知识框架，通过提出精心构造的问题，无需直接了解数据库，该框架能促使这些模型处理这些问题并生成输出，从而揭示数据库模式结构。我们将此方法应用于针对文本-SQL对进行过微调的专用文本到SQL模型以及用于SQL生成的生成式语言模型。结果显示，对于微调模型，我们能够以接近0.75的F1分数重构表名，而对于生成式模型，这一分数更是高达0.96。|
|**2024-06-20**|**Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs**|Yuxuan Qiao et.al.|[2406.14544](http://arxiv.org/abs/2406.14544)|**[link](https://github.com/sparksjoe/prism)**|**## 翻译  视觉语言模型（VLMs）在处理各种视觉问题时展现出卓越的能力，这要求模型具备强大的感知和推理能力。然而，由于感知和推理在现有VLM中的交织性，独立评估这两方面的能力颇具挑战。为此，我们提出了一种创新框架——Prism，旨在分离视觉理解和推理在视觉问答中的作用。Prism分为两个阶段：感知阶段利用VLM提取并以文本形式表达视觉信息；推理阶段则根据提取的视觉信息，通过大型语言模型（LLM）生成响应。这种模块化设计使得我们可以系统地比较和评估不同VLM的感知和推理性能。  我们的分析框架提供了诸多洞见，证明了Prism作为成本效益高的视觉语言任务解决方案的潜力。通过将专注于感知的简化VLM与专为推理设计的强大LLM相结合，Prism在通用视觉语言任务上取得了优异成绩，同时显著降低了训练和运营成本。定量评估显示，当Prism配备基础的2B LLaVA VLM和开源的GPT-3.5时，其在严谨的多模态基准MMStar上的表现可与大十倍的VLM相当。该项目已发布在：https://github.com/SparksJoe/Prism。**|
|**2024-06-21**|**Are LLMs Naturally Good at Synthetic Tabular Data Generation?**|Shengzhe Xu et.al.|[2406.14541](http://arxiv.org/abs/2406.14541)|**[link](https://github.com/anonymou9167/anonymouscode)**|**大型语言模型（LLMs）在生成文本和图像方面表现出色，但其在生成最常见的数据类型——表格数据方面的潜力却鲜有研究。这篇论文指出，直接使用或经过传统微调的LLMs在作为合成表格生成器时表现极差。由于LLMs的自回归特性，随机顺序排列的微调与捕捉功能性依赖的重要性相悖，导致它们无法处理条件混合分布（这是反映现实世界约束的关键）。我们展示了如何通过使LLMs变得感知排列顺序来改善这些不足，从而提升其性能。**|
|**2024-06-20**|**PostMark: A Robust Blackbox Watermark for Large Language Models**|Yapei Chang et.al.|[2406.14517](http://arxiv.org/abs/2406.14517)|**[link](https://github.com/lilakk/postmark)**|**最有效的检测生成式语言模型（LLM）文本的方法是通过在解码过程中插入可识别的标记，即水印。然而，大多数现有方法依赖于获取到LLM的原始概率（logits），这使得LLM服务提供商不愿分享，因为担心模型泄露问题。因此，这些水印需要每个提供者独立开发。本文提出了一种创新的后处理水印方案，名为PostMark。它是一种模块化的、生成后插入的水印策略，无需触及logits，适合第三方实施。PostMark表现出更强的对抗同义句攻击能力：我们在实验中涵盖了八个基础算法、五个基线LLM和三个数据集。此外，我们还评估了PostMark对文本质量的影响，包括自动化和人工评估，探讨了质量和抗改写攻击之间的权衡。研究代码、输出和注释已公开在https://github.com/lilakk/PostMark。**|
|**2024-06-18**|**DrVideo: Document Retrieval Based Long Video Understanding**|Ziyu Ma et.al.|[2406.12846](http://arxiv.org/abs/2406.12846)|null|当前的长视频理解方法主要关注时长仅十几秒的视频，对处理更长视频的技术探索有限。长视频中的大量帧数带来了两个主要挑战：难以定位关键信息和进行长期推理。因此，我们提出DrVideo，一个基于文档检索的系统，专为长视频理解设计。我们的核心思想是将长视频理解问题转化为长文档理解任务，以充分利用大型语言模型的强大能力。具体来说，DrVideo将长视频转换为文本形式的长文档，首先检索关键帧并增强这些帧的信息，作为系统的起点。然后，它采用基于代理的迭代循环，持续搜索缺失信息、补充相关数据，并在收集到足够的与问题相关的信息后，以链式思考的方式给出最终预测。在多个长视频基准上的实验验证了我们方法的有效性。DrVideo在EgoSchema（3分钟）测试中比现有最先进的方法高出3.8个百分点，在MovieChat-1K（10分钟）的break模式和global模式中分别提高17.9和38.0分，以及在LLama-Vid QA（超过60分钟）数据集上提升30.2分。|
|**2024-06-18**|**Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts**|Haoxiang Wang et.al.|[2406.12845](http://arxiv.org/abs/2406.12845)|**[link](https://github.com/RLHFlow/RLHF-Reward-Modeling)**|**强化学习从人类反馈（RLHF）已经成为大型语言模型（LLMs）与人类偏好对齐的主要方法。传统上，通过使用人类偏好数据训练奖励模型（RM），过程通常从比较同一用户请求的响应开始，相对评分指示人类更喜欢哪个响应。然而，由于RM的黑盒特性，其输出缺乏可解释性，人们难以理解为什么RM认为某个回复是好的。鉴于RM作为人类偏好的代理，我们提议采用两阶段方法来创建可解释的RM：首先，使用多维绝对评分数据训练绝对评级多目标奖励模型（ArmoRM），每个维度对应于人类可理解的目标（如诚实、详尽、安全）；其次，利用混合专家（MoE）策略，结合一个门控网络，根据上下文自动选择最合适的奖励目标。我们成功地使用Llama-3 8B训练了ArmoRM，并在顶部添加了一个浅层MLP作为门控网络，形成了ArmoRM-Llama3-8B。我们的模型在评估RM的语言建模性能的RewardBench基准上实现了最先进的成绩。值得注意的是，我们的模型在性能上超过了使用GPT-4法官的LLM作为评判者的方法，并接近于规模更大的Nemotron-4 340B奖励模型的水平。**|
|**2024-06-18**|**Synergizing Foundation Models and Federated Learning: A Survey**|Shenghui Li et.al.|[2406.12844](http://arxiv.org/abs/2406.12844)|null|近期，大型语言模型、视觉Transformer和多模态模型等基础模型（FMs）的发展在学术界和工业界产生了显著影响。与小型模型相比，FMs在预训练阶段对大量数据的需求更大。尽管通用FMs可以使用互联网上的公开数据进行预训练，但针对特定领域的FMs需要专有数据，这在实际应用中因隐私问题而面临数据可用性挑战。联邦学习（FL）作为一种协作学习范式，打破了数据共享的障碍，为利用分布式数据定制和适应各种领域特定任务的FMs提供了前景，同时保护了数据隐私。这篇综述论文探讨了FL与FMs融合的潜力与挑战，总结了核心技术、未来发展方向以及应用场景。关于FM-FL的定期更新论文集合可在<https://github.com/lishenghui/awesome-fm-fl>获取。|
|**2024-06-18**|**LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation**|Seyedarmin Azizi et.al.|[2406.12832](http://arxiv.org/abs/2406.12832)|**[link](https://github.com/arminazizi98/lamda)**|**在大语言模型微调领域，低秩适应（LoRA）已经成为标准方法，因为它显著减少了可训练参数。然而，随着模型嵌入维度的增加，LoRA所需的可训练参数量也随之上升，导致计算成本较高。此外，其后向更新需要存储高维中间激活和优化器状态，对GPU内存需求较大。为此，本文提出了一种新的大语言模型微调方法——基于谱分解的低维适应（LaMDA）。LaMDA通过冻结第一投影矩阵（PMA），同时引入一个低维可训练的平方矩阵，实现了可训练参数和峰值GPU内存使用的大幅减少。在早期的微调阶段，LaMDA逐步冻结第二投影矩阵（PMB），进一步降低权重更新的计算成本，提高参数效率。  我们还引入了增强版LaMDA++，它通过规范化预训练模型权重的谱分析，实现轻量级的LoRA路径自适应秩分配。我们在多个任务上进行了评估，包括GLUE自然语言理解基准、文本摘要、自然语言生成以及复杂推理，应用于不同类型的大型语言模型。实验结果显示，LaMDA在性能上与现有方法相当或超越，且在微调期间可减少高达17.7倍的参数更新次数，以及1.32倍的峰值GPU内存使用。我们将公开代码。**|
|**2024-06-18**|**Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?**|Pinzhen Chen et.al.|[2406.12822](http://arxiv.org/abs/2406.12822)|null|## 背景 大型多语言模型旨在服务不同语种的母语使用者。我们推测，当前针对这些模型的微调和评估方法可能与其初衷不符，原因在于过度依赖翻译，可能导致翻译中的瑕疵。尚不清楚指令数据的性质如何影响模型输出，同时，用翻译测试集来捕捉这些细微差别是否有效。由于训练和评估阶段常常结合使用翻译数据，这些潜在问题可能被忽视。本研究通过在指令调优和评估阶段使用控制性的母语或翻译数据，来探究这些问题，并观察模型表现。我们在八种基础模型和八个不同基准上进行实验，结果显示，对于母语或生成性基准，使用母语或翻译指令数据时，模型性能高时，两者之间的差异尤为明显，而在其他类型的测试集上则不然。最后，我们发现正则化对于结构化任务有益，但对于生成性任务则不然。|
|**2024-06-18**|**Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?**|Zhe Yang et.al.|[2406.12809](http://arxiv.org/abs/2406.12809)|null|大型语言模型（LLMs）展现了令人印象深刻的性能，但它们仍存在不一致的问题，例如对重述或微小顺序变化的反应不一致。除了这些不稳定性，我们还观察到尽管LLMs能够解决难题，但在相对简单的任务上却可能失败。为了评估这种从难到易的不一致性，我们创建了ConsisEval基准，其中每个条目包含两个难度有序的问题。我们还引入了一致性分数的概念，以量化这种不一致性，并分析通过相对一致性分数改进一致性潜力。通过对现有模型的广泛实验，我们得出以下发现：(1) GPT-4获得92.2%的最高一致性分数，但仍因冗余信息的干扰、问题误解等问题对特定问题不一致；(2) 能力更强的模型通常表现出更高的一致性，但也存在例外情况；(3) 对于 Fine-tuning 和上下文学习而言，硬数据可以提高一致性。我们的数据和代码将在GitHub上公开提供。|
|**2024-06-18**|**Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents**|Zehao Wang et.al.|[2406.12806](http://arxiv.org/abs/2406.12806)|null|**背景**：配置设置对于调整软件行为以满足特定性能需求至关重要，但错误配置普遍存在。由于配置项众多且复杂，识别影响系统性能的配置是一项挑战。本研究提出PerfSense，这是一个轻量级框架，利用大型语言模型（LLMs）高效地识别性能关键配置，同时保持低开销。PerfSense利用LLM代理模拟开发者和性能工程师之间的交互，采用先进的提示链技术和检索增强生成（RAG）等技术。  **方法与成果**：我们在七个开源Java系统上的评估显示，PerfSense在分类性能敏感配置方面的平均准确率为64.77%，优于基于LLM的基线（50.36%）和先前的最佳方法（61.75%）。特别是，我们的提示链技术提高了召回率10%至30%，而保持了相似的精确度。进一步的手动分析362个误分类案例，发现常见问题包括LLMs对需求的理解偏差（占26.8%）。  **结论**：PerfSense显著减少了手动分类性能关键配置的工作量，并为未来的LLM基于代码分析研究提供了有价值的观点。|
|**2024-06-18**|**Supporting Human Raters with the Detection of Harmful Content using Large Language Models**|Kurt Thomas et.al.|[2406.12800](http://arxiv.org/abs/2406.12800)|null|本文探讨了利用大型语言模型（LLMs）自动或辅助人类审阅者检测有害内容的可能性，如仇恨言论、骚扰、极端主义和选举误导。通过50,000条评论的数据集，我们发现LLMs在与人类判断相比时能达到90%的准确率。我们提出五种设计模式，以整合LLMs与人工评级，例如预筛选非暴力内容、检测人类评级可能的错误，或者提供关键上下文以支持人工评级。我们展示了如何使用一个优化的提示来支持这些设计模式。在实际应用的试点中，我们的方法在优化人力资源效率方面实现了41.5%的提升，同时在检测违规内容的精确度和召回率上分别提高了9%至11%。|
|**2024-06-18**|**ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools**|Team GLM et.al.|[2406.12793](http://arxiv.org/abs/2406.12793)|**[link](https://github.com/thudm/chatglm-6b)**|我们介绍ChatGLM，这是一个随时间不断发展的大型语言模型系列。本报告主要关注GLM-4语言系列，包括GLM-4、GLM-4-Air和GLM-4-9B，它们代表了我们当前最强大的模型，集成了前三代ChatGLM的所有经验和教训。这些模型经过了十万亿次训练，主要涵盖中文和英语，以及少量来自24种语言的语料库，侧重于中英文的对齐。高质量的对齐是通过多阶段的后训练过程实现的，包括监督微调和学习人类反馈。评估显示，GLM-4在通用指标如MMLU、GSM8K、MATH、BBH、GPQA和HumanEval上接近或优于GPT-4；在IFEval指令跟随任务中的表现接近GPT-4 Turbo；在长文本任务上与GPT-4 Turbo（128K）和Claude 3相当；在中文对齐方面，GLM-4优于GPT-4，根据AlignBench衡量。GLM-4 All Tools模型进一步进行了对齐，以理解用户意图并能自主决定何时使用哪种工具，如Web浏览器、Python解释器、文本转图像模型和自定义函数，以有效地完成复杂任务。在实际应用中，它在诸如通过网络浏览获取信息和使用Python解释器解题等任务上与GPT-4 All Tools相匹配甚至超越。到目前为止，我们已经开源了一系列模型，包括ChatGLM-6B（三代）、GLM-4-9B（128K、1M）、GLM-4V-9B、WebGLM和CodeGeeX，在2023年仅Hugging Face上就有超过1000万次下载。这些开源模型可通过<https://github.com/THUDM>和<https://huggingface.co/THUDM>访问。|
|**2024-06-18**|**UBENCH: Benchmarking Uncertainty in Large Language Models with Multiple Choice Questions**|Xunzhi Wang et.al.|[2406.12784](http://arxiv.org/abs/2406.12784)|**[link](https://github.com/Cyno2232/UBENCH)**|随着大型语言模型（LLMs）的迅速发展，它们在实际应用中展现出显著的效果。然而，由于低可解释性，这些模型在未预见情况下常会出现错误，限制了其价值。尽管已有许多研究致力于构建全面的评估体系，但先前的基准测试主要关注问题解决能力，对响应的不确定性评估不足，可能导致不稳定性。当前的方法在衡量LLM可靠性时资源消耗大，且难以测试黑盒模型。  为解决这些问题，我们提出了UBENCH，一个全面的LLM可靠性评估基准。它包含3,978个涵盖知识、语言理解、推理能力的多选题。实验结果显示，UBENCH达到了最先进的性能，并且其单次采样方法显著节省了计算资源，相较于需要多次采样的基线方法更为高效。此外，我们利用UBENCH评估了15种流行LLM的可靠性，发现GLM4表现出色，紧随其后的是GPT-4。我们还探究了Chain-of-Thought提示、角色扮演提示、选项顺序和温度对LLM可靠性的影响，分析了它们对不同模型的不同作用。|
|**2024-06-17**|**LLaNA: Large Language and NeRF Assistant**|Andrea Amaduzzi et.al.|[2406.11840](http://arxiv.org/abs/2406.11840)|null|多模态大型语言模型（MLLM）在理解和处理图像和3D数据方面表现出色，但它们在全面捕捉物体的外观和几何特性上存在局限。近期，神经辐射场（Neural Radiance Fields，简称NeRF）作为一种新兴的表示方式，通过一个简单的多层感知器（Multi-Layer Perceptron，MLP）的权重编码了物体的几何结构和高度逼真的外观，引起了广泛关注。本文探讨了将NeRF整合到MLLM中的可行性和效果。我们开发了LLaNA，这是首个通用的NeRF-语言助手，能够执行新任务，如NeRF描述和问答。我们的方法直接处理NeRF MLP的权重，无需渲染图像或构建3D数据结构，就能提取有关代表对象的信息。此外，我们创建了一个无须人工干预的NeRF文本标注数据集，用于各种NeRF-语言任务，并据此建立了一个评估方法来衡量我们的模型对NeRF理解能力。实验结果表明，处理NeRF权重的方法在与从NeRF中提取2D或3D表示进行比较时表现更优。|
|**2024-06-17**|**mDPO: Conditional Preference Optimization for Multimodal Large Language Models**|Fei Wang et.al.|[2406.11839](http://arxiv.org/abs/2406.11839)|null|### 背景  直接偏好优化（DPO）已被证明是大型语言模型（LLM）校准的有效手段。最近的研究尝试将DPO应用于多模态场景，但发现实现持续改进颇具挑战。通过对比实验，我们发现了多模态偏好优化中的无条件偏好问题，即模型忽视了图像条件。为解决这个问题，我们提出了mDPO，一个旨在防止语言偏好过度优先的多模态DPO目标，同时优化图像偏好。此外，我们引入了奖励锚点，确保选择的响应奖励保持正向，从而避免相对偏好优化固有的可能性降低问题。  ### 任务  我们在两个不同规模的多模态LLM以及三个常用基准上进行了实验，结果显示，mDPO有效解决了多模态偏好优化中的无条件偏好问题，并显著提高了模型性能，特别是在减少幻觉方面。|
|**2024-06-17**|**Unveiling Encoder-Free Vision-Language Models**|Haiwen Diao et.al.|[2406.11832](http://arxiv.org/abs/2406.11832)|**[link](https://github.com/baaivision/eve)**|**当前的视觉语言模型（VLM）主要依赖于视觉编码器来提取视觉特征，然后利用大型语言模型（LLMs）处理视觉语言任务。然而，视觉编码器在抽象视觉表示方面设定了强烈的先验，如分辨率、比例和语义倾向，这可能限制了VLM的灵活性和效率。直接训练无编码器的纯VLM仍然具有挑战性，且鲜有探索。实证研究显示，这种直接训练方法会导致收敛缓慢和性能差距较大。本文旨在弥合编码器依赖型和无编码器模型之间的差距，提出了一种简单而有效的纯VLM训练策略。具体来说，我们通过深入实验揭示了高效训练无编码器VLM的关键要素：（1）在统一的解码器内融合视觉与语言表示；（2）通过额外监督提升视觉识别能力。基于这些策略，我们开发了EVE，一个无编码器的视觉语言模型，既能高效训练也能快速推理。值得注意的是，仅使用3500万公开可用的数据，EVE就能在多个视觉语言基准上与类似容量的编码器依赖型VLM匹敌，甚至超越了训练过程神秘、数据未公开的Fuyu-8B模型。我们相信，EVE为跨模态开发纯粹的解码器架构提供了一个透明且高效的路径。我们的代码和模型已公开在：https://github.com/baaivision/EVE。**|
|**2024-06-17**|**Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models**|Bingqi Ma et.al.|[2406.11831](http://arxiv.org/abs/2406.11831)|null|大型语言模型（LLMs）基于解码器-only变压器在文本理解方面表现出色，但如何将这些先进的LLMs应用于文本到图像的扩散模型仍是一个待探索的问题。我们发现直接使用LLM作为提示编码器会显著降低生成图像时的提示跟随能力。主要存在两个问题：一是LLM的下一个词预测训练与扩散模型对区分性提示特征的需求不匹配；二是解码器架构固有的位置偏见。为解决这些问题，我们提出了一种新框架，通过精心设计的使用指南，增强LLM的文本表示能力，消除其内在的定位偏见，从而灵活地将最先进的LLMs融入文本到图像生成模型。此外，我们还提供了一种融合多个LLMs的方法。鉴于Transformer架构的卓越性能和扩展能力，我们进一步设计了基于该框架的LLM-Infused Diffusion Transformer（LI-DiT）。我们进行了广泛的实验，验证了LI-DiT在不同模型规模和数据量下的性能。得益于LLMs的内在能力及我们的创新设计，LI-DiT的提示理解性能轻松超越开源的最新模型，以及包括Stable Diffusion 3、DALL-E 3和Midjourney V6在内的主流闭源商业模型。强大的LI-DiT-10B将在进一步优化和安全检查后提供。|
|**2024-06-17**|**WPO: Enhancing RLHF with Weighted Preference Optimization**|Wenxuan Zhou et.al.|[2406.11827](http://arxiv.org/abs/2406.11827)|**[link](https://github.com/wzhouad/wpo)**|**强化学习从人类反馈（RLHF）是调整大型语言模型（LLMs）以更好地符合人类价值观的有前景方法。由于成本效益和可扩展性，离线偏好优化——通过其他模型获取偏好数据——被广泛采用。然而，离线偏好优化常受采样策略与目标策略之间分布差异的影响，导致优化效果不理想。为此，我们提出了一种创新策略——加权偏好优化（WPO），旨在通过调整偏好评分对，使离线数据更接近于当前策略，从而缓解这一问题。这种方法不仅解决了分布差距难题，还提升了优化过程，无需额外成本。  我们在Alpaca Eval 2和MT-bench等指令跟随基准上验证了我们的方法。WPO在Alpaca Eval 2上的性能比直接偏好优化（DPO）提高了5.6%。基于Llama-3-8B-Instruct，WPO甚至建立了显著的长度控制胜率，达到48.6%，在80亿参数模型排行榜上成为最强劲的模型。我们将在<https://github.com/wzhouad/WPO>上开源代码和模型。**|
|**2024-06-17**|**Embodied Instruction Following in Unknown Environments**|Zhenyu Wu et.al.|[2406.11818](http://arxiv.org/abs/2406.11818)|null|在自主家庭服务系统中，使实体代理能根据自然语言完成复杂的人类指令至关重要。传统方法仅能在所有互动对象都提供给代理的已知环境中执行指令，直接将现有方法应用于未知环境通常会产生操作不存在物体的不可行计划。相反，我们提出了一种针对未知环境的复杂任务实体指令跟随（Embodied Instruction Following，EIF）方法，该方法使代理能够有效地探索环境，利用现有物体生成可执行计划，以达成抽象指令。具体来说，我们构建了一个包括高层任务规划器和低层探索控制器的多模态大语言模型的层次化实体指令跟随框架。然后，我们通过动态区域注意力构建场景的语义表示地图，以展示已知的视觉线索，使任务规划和场景探索与人类指令目标保持一致。对于任务规划器，根据任务完成过程和已知视觉线索，我们生成步骤式的可行计划。对于探索控制器，根据生成的步骤计划和已知视觉线索预测最优的导航或物体交互策略。实验结果表明，我们的方法在大型房屋级场景中的204个复杂人类指令（如做早餐和整理房间）上实现了45.09%的成功率。|
|**2024-06-17**|**VideoLLM-online: Online Video Large Language Model for Streaming Video**|Joya Chen et.al.|[2406.11816](http://arxiv.org/abs/2406.11816)|null|## 翻译  近期的大型语言模型已经增强了视觉功能，能够理解图像、视频和融合了视觉与语言的内容。然而，这些大模odels的训练方法通常将视频视为预先剪辑好的片段，这使得它们在处理连续视频流时效果不佳且效率低下。为此，我们在本文中提出了一种新颖的“Learning-In-Video-Stream”（LIVE）框架，旨在实现实时、长序列、与视频流同步的对话，适用于连续视频输入。LIVE框架包括以下三个方面：（1）一个设计用于处理连续流式输入的语言建模目标；（2）一种数据生成策略，将离线时间标注转换为适合流式对话的格式；（3）一个优化的推理管道，以提高在实际视频流中的响应速度。基于Llama-2/Llama-3，我们构建了VideoLLM-online模型，并通过它展示了在处理视频流对话方面的显著优势，例如，在A100 GPU上，该模型能在5分钟视频片段中实现超过10帧每秒的流式对话。此外，VideoLLM-online还在公开的离线视频基准测试（如识别、captioning和预测）上展现出最先进的性能。我们已将代码、模型、数据和演示发布在https://showlab.github.io/videollm-online供人使用。|
|**2024-06-17**|**How Do Large Language Models Acquire Factual Knowledge During Pretraining?**|Hoyeon Chang et.al.|[2406.11813](http://arxiv.org/abs/2406.11813)|null|尽管近期研究表明大型语言模型（LLMs）能够存储大量事实知识，但它们如何在预训练过程中获取这些知识的机制尚不明确。本研究针对这一缺口，探讨了LLMs在预训练期间如何获取和保持事实知识。研究发现了一些关键洞见：首先，出乎意料的是，更多的训练数据对模型获取和保持事实知识的能力并无显著提升。其次，训练步数与记忆遗忘和事实知识泛化之间存在幂律关系，使用重复训练数据的模型遗忘速度更快。第三，增大批量大小可以提高模型抵抗遗忘的能力。总的来说，我们的观察表明，LLMs在预训练中的事实知识获取是通过逐步增加每一步中预训练数据中事实知识出现的概率。然而，这种增加随后会因遗忘而稀释。基于这种理解，我们能够解释一些最近观察到的LLM行为，如长尾知识上的性能不佳，以及去重预训练语料库的好处。|
|**2024-06-17**|**RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference Content**|Joao Monteiro et.al.|[2406.11811](http://arxiv.org/abs/2406.11811)|null|## 背景  大型语言模型（LLMs）在训练过程中大量依赖自动从互联网抓取的数据，其中包括包含大量通用知识的百科全书（如维基百科），也可能与用于评估LLMs的基准数据集重叠。因此，如果测试集可能已泄露到训练集中，对模型的评估可能会产生误导性的结论。为了推动语言模型的公正评估，我们提出了一种新的测试数据集——RepLiQA，适用于问答和主题检索任务。RepLiQA是一个包含五个分片的测试集，其中四个在本论文发布前未公开或通过LLM API提供。RepLiQA的每个样本由以下四部分组成：（1）由人类标注员创作的虚构场景描述文档（例如新闻文章），这些内容不会出现在互联网上；（2）关于文档主题的问题；（3）直接源自文档信息的正确答案；（4）包含答案的文档段落。这意味着只有当模型能在提供的文档中找到相关内容时，才能生成准确的答案。  我们进行了一项大规模基准测试，包括多个最先进的LLM，以揭示不同类型的和规模的模型在条件语言建模设置下的性能差异。RepLiQA的已发布分片可在以下链接找到：https://huggingface.co/datasets/ServiceNow/repliqa。|
|**2024-06-17**|**Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations**|Rima Hazra et.al.|[2406.11801](http://arxiv.org/abs/2406.11801)|**[link](https://github.com/declare-lab/safety-arithmetic)**|**随着大型语言模型（LLMs）在翻译和问答等应用中的日益重要，确保它们与人类价值观的正确导向变得至关重要。然而，当前的对齐方法在处理动态用户意图和复杂目标时存在困难，使得模型容易生成有害内容。为此，我们提出了一种无需训练的框架——安全算术（Safety Arithmetic），旨在提升LLMs在不同场景下的安全性，包括基础模型、监督微调模型（SFT）和编辑后的模型。安全算术包含两部分：有害内容消除（Harm Direction Removal）以避免不良输出，以及安全对齐（Safety Alignment）以促进安全响应。此外，我们还发布了NoIntentEdit数据集，它揭示了可能导致模型安全风险的编辑实例。实验结果显示，安全算术显著增强了安全措施，减少了过度安全的问题，同时保持了模型的实用性，相较于现有方法在保障内容生成的安全性方面表现出色。**|
|**2024-06-14**|**Quantifying Variance in Evaluation Benchmarks**|Lovish Madaan et.al.|[2406.10229](http://arxiv.org/abs/2406.10229)|null|评价基准是衡量大型语言模型（LLMs）能力的关键，也是推动这些能力进步的驱动力。最初设计用于评估预训练模型的性能（或缺乏），现在它们也被广泛用于决定不同的训练选择之间。然而，尽管被广泛应用，我们很少量化评价基准的方差，这决定了性能差异的含义。本文定义并测量了一系列旨在衡量评价基准方差的指标，包括初始化时的随机种子方差和训练过程中的单调性。通过对大量模型（包括公开可用的和从头训练的模型）进行研究，我们提供了各种方差度量的实证估计，并为实践者提供了考虑和建议。我们还评估了连续和离散性能度量的实用性和权衡，并探索了更好地理解和减少方差的方法。我们发现，对于较小规模（约70亿参数）的模型，如将多模态多任务学习（MMLU）任务框架为完成任务，可以常常降低方差；而受到人类测试文献启发的更复杂方法（如项目分析和项目反应理论）在显著减少方差方面效果有限。总的来说，我们的工作揭示了评价基准的方差特性，提出了针对LLMs的特定技术来减少方差，并普遍鼓励实践者在比较模型时仔细考虑方差因素。|
|**2024-06-14**|**Semantic Membership Inference Attack against Large Language Models**|Hamid Mozaffari et.al.|[2406.10218](http://arxiv.org/abs/2406.10218)|null|## 背景 成员身份泄露攻击（Membership Inference Attacks，MIA）的目标是识别特定数据点是否被纳入了目标模型的训练集。本文提出了一种新颖的方法——语义成员身份泄露攻击（Semantic Membership Inference Attack，SMIA），通过利用输入的语义内容及其扰动，提升MIA的性能。SMIA训练一个神经网络来分析目标模型对扰动输入的行为，从而捕捉成员样本与非成员样本之间输出概率分布的差异。我们在Pythia和GPT-Neo模型家族，以及Wikipedia数据集上进行了全面的评估。实验结果显示，SMIA明显优于现有攻击手段，例如在Pythia-12B上的AUC-ROC值达到了67.39%，而第二好的攻击方法仅为58.90%。|
|**2024-06-14**|**Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs**|Rui Yang et.al.|[2406.10216](http://arxiv.org/abs/2406.10216)|null|在强化学习从人类反馈（RLHF）框架中，利用基于人类偏好数据的奖励模型已证实能有效调整大型语言模型（LLMs）以符合人类意图。然而，当前奖励模型对未见过的提示和响应的泛化能力有限，可能导致所谓的过度优化问题，即奖励优化过度导致实际性能下降。尽管先前的研究倾向于约束策略优化，我们的研究提出了一种新方法，通过正则化隐藏状态来增强奖励模型应对分布变化的泛化能力。具体来说，我们保留基础模型的语言模型头，并结合一系列文本生成损失，旨在保持隐藏状态的文本生成能力，同时在相同的隐藏状态后学习一个奖励头。实验结果表明，引入的正则化技术显著提高了在各种泛化任务中的奖励模型准确性，并有效缓解了RLHF中的过度优化问题，提供了一个更可靠、更稳健的偏好学习范式。|
|**2024-06-14**|**Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs**|Abhimanyu Hans et.al.|[2406.10209](http://arxiv.org/abs/2406.10209)|**[link](https://github.com/ahans30/goldfish-loss)**|**## 背景 大型语言模型能够记住并重复其训练数据，这带来了隐私和版权问题。为了减轻这种记忆，我们提出了一种对下一步 token 训练目标的微妙修改，称为“金鱼损失”。在训练过程中，随机选择一部分令牌不参与损失计算。模型不会记住这些被丢弃的令牌，从而防止了完整训练序列的逐字复制。我们在数十亿规模的 Llama-2 模型上进行了大量实验，包括预训练和从头开始训练，结果显示，我们的方法显著减少了可提取的记忆，而对下游基准的影响微乎其微。**|
|**2024-06-14**|**TRIP-PAL: Travel Planning with Guarantees by Combining Large Language Models and Automated Planners**|Tomas de la Rosa et.al.|[2406.10196](http://arxiv.org/abs/2406.10196)|null|**摘要：**  旅行规划是一个复杂的任务，它涉及根据约束条件生成一系列与访问地点相关的行动，同时最大化用户的满意度。传统方法通常会将问题转化为特定形式的语言表达，从网络资源中提取相关信息，并使用合适的求解器来生成有效解决方案。然而，近期的基于大型语言模型（LLMs）的方法直接从用户请求中输出计划，利用丰富的旅行领域知识提供景点和可能路线等高层次信息。尽管如此，当前最先进的模型往往产生不连贯、未能完全满足约束的计划，且无法保证生成高质量方案。我们提出TRIP-PAL，一种融合LLMs和自动化规划器的混合方法：（1）LLMs获取并转换旅行信息和用户需求，将其转化为可输入规划器的数据结构；（2）自动化规划器负责生成满足约束并优化用户效用的旅行计划。我们在不同旅行场景中的实验表明，TRIP-PAL在生成旅行计划方面优于纯LLM方法。|
|**2024-06-14**|**Detecting and Evaluating Medical Hallucinations in Large Vision Language Models**|Jiawei Chen et.al.|[2406.10185](http://arxiv.org/abs/2406.10185)|null|随着大型视觉语言模型（LVLM）在医疗领域的应用日益增长，如医学图像问答和报告生成，它们从基础大语言模型（LLMs）那里继承了强大的功能，但同时也带来了令人担忧的幻觉问题，这在医疗这样对错误容限极低的环境中尤为重要。然而，目前尚无专门针对医疗领域的幻觉检测和评估方法或基准。为了填补这一空白，我们推出了Med-HallMark，这是首个专为医疗多模态领域设计的幻觉检测和评估基准。Med-HallMark支持多任务幻觉检测，提供多元化的幻觉数据，并采用分级幻觉分类。此外，我们提出了MediHall Score，这是一种新的医疗评估指标，通过分层评分系统评估LVLM的幻觉，考虑其严重程度和类型，从而实现对潜在临床影响的细致评估。我们还展示了MediHallDetector，一种专为精确幻觉检测设计的医疗LVLM，它采用了多任务训练方法。通过广泛的实验，我们在我们的基准上为流行的LVLM设立了基线。实验结果表明，MediHall Score提供了比传统指标更深入理解幻觉影响的能力，并显示了MediHallDetector的提升性能。我们期望这项工作能显著提高LVLM在医疗应用中的可靠性。所有相关资源将在不久后发布。|
|**2024-06-14**|**Practical offloading for fine-tuning LLM on commodity GPU via learned subspace projectors**|Siyuan Chen et.al.|[2406.10181](http://arxiv.org/abs/2406.10181)|null|在大语言模型（LLMs）的微调过程中，由于内存需求通常超过单个GPU的容量，解决这一内存挑战的一个常见方法是将计算和数据从GPU迁移到CPU。然而，这受到普通硬件带宽限制的制约，影响了CPU与GPU之间的通信效率。本文提出了一种名为LSP_Offload的框架，通过学习式的子空间投影器，实现在 commodity 硬件上接近原生速度的大规模语言模型微调。我们的数据驱动方法涉及学习一个高效的稀疏压缩器，以最小化通信并保持最小精度损失。此外，我们引入了一种创新的层级通信调度策略，以最大化通信与计算之间的并行性。因此，我们的框架能够在4GB笔记本GPU上微调13亿参数的模型，在配备24GB内存的NVIDIA RTX 4090 GPU上微调70亿参数的模型，仅比无内存限制的微调慢31%。与最先进的离线框架相比，我们的方法提高了微调吞吐量，最高可达3.33倍，当达到相同准确度时，减少了端到端微调时间的33.1%至62.5%。|
|**2024-06-14**|**Datasets for Multilingual Answer Sentence Selection**|Matteo Gabburo et.al.|[2406.10172](http://arxiv.org/abs/2406.10172)|null|**摘要：**  在设计高效的检索式问答（Question Answering，QA）系统中，答案句子选择（Answer Sentence Selection，AS2）是一个关键任务。然而，由于缺乏标注数据，大多数AS2领域的进展主要集中在英语上。这导致了非英语环境下QA系统的性能与英语系统之间的差距。本论文针对这一问题，我们开发了新的高质量多语言（法语、德语、意大利语、葡萄牙语和西班牙语）AS2数据集，通过使用大型语言模型（Large Language Model，LLM）对现有的英文AS2数据集（如ASNQ、WikiQA和TREC-QA）进行监督自动机器翻译（Automatic Machine Translation，AMT）。我们通过多种实验和不同Transformer架构的评估，验证了我们的方法以及翻译数据集的质量。结果显示，我们的数据集对于构建健壮的多语言AS2模型至关重要，显著缩小了非英语与英语环境下的性能差距。|
|**2024-06-14**|**Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models**|Carson Denison et.al.|[2406.10162](http://arxiv.org/abs/2406.10162)|**[link](https://github.com/anthropics/sycophancy-to-subterfuge-paper)**|**在强化学习中，当人工智能系统学会因训练目标不明确而获得不期望的行为时，就会出现规格游戏现象。这种行为可能从简单的奉承行为发展到更复杂且危险的奖励篡改，即模型直接修改其自身的奖励机制。然而，发现这些复杂行为可能超出探索的范畴。本论文探讨大型语言模型（LLMs）是否会在学习常见规格游戏策略后，泛化到执行更为罕见和明显的行为，包括奖励篡改。我们构建了一个逐步升级的可游戏环境系列，并发现针对早期阶段环境的训练会导致在后续环境中出现更多的规格游戏。令人惊讶的是，一小部分但非零的LLMs，在经历了完整训练课程后，能够零样本地直接修改其奖励函数。重新训练LLMs以避免早期阶段的游戏行为可以减轻但不能完全消除后期环境中的奖励篡改。此外，对可游戏环境进行无害性训练并不能阻止奖励篡改。这些结果表明，LLMs能够从常见的规格游戏策略中泛化到更恶劣的奖励篡改行为，并且要消除这种行为可能并非易事。**|
|**2024-06-14**|**BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack**|Yuri Kuratov et.al.|[2406.10149](http://arxiv.org/abs/2406.10149)|**[link](https://github.com/booydar/babilong)**|近年来，大型语言模型（LLMs）的输入上下文长度显著增加。然而，现有的评估方法未能充分衡量模型处理长篇文本中的事实推理能力。为此，我们提出了BABILong基准测试，旨在测试模型在分布式长文档中跨事实推理的能力。BABILong包括20个多样化的推理任务，如事实链、简单归纳、演绎、计数以及处理列表/集合等。这些任务本身就具有挑战性，而当所需事实分散在长篇自然文本中时，难度进一步提升。我们的评估显示，流行的LLMs实际上只利用了10%-20%的上下文信息，且随着推理复杂性的提高，性能急剧下降。对于替代的上下文推理方法，检索增强生成策略在单事实问题回答上的准确率仅为60%，与上下文长度无关。在上下文扩展方法中，循环记忆Transformer展现出最高性能，可处理长达1100万个令牌的长度。BABILong基准测试可以扩展到任意长度，以支持评估具有更强能力的新模型，并提供了长达100万令牌的分隔。|
|**2024-06-13**|**VideoGPT+: Integrating Image and Video Encoders for Enhanced Video Understanding**|Muhammad Maaz et.al.|[2406.09418](http://arxiv.org/abs/2406.09418)|**[link](https://github.com/mbzuai-oryx/videogpt-plus)**|**在基于语言模型的进展基础上，大型多模态模型（LMMs）在视频理解方面取得了显著进步。然而，现有的视频LMMs依赖于图像或视频编码器处理视觉输入，这些编码器各自存在局限性。图像编码器擅长捕捉帧序列中的丰富空间细节，但缺乏明确的时间上下文；而视频编码器提供时间上下文，但常常受限于计算资源，导致只能处理低分辨率的稀疏帧，从而影响了对空间和上下文的理解。因此，我们提出VideoGPT+，它结合了图像编码器（用于详细的空间理解）和视频编码器（用于全局时序上下文建模）的优势。该模型通过将视频划分为小段，并对来自两者特征的提取应用自适应池化策略，以提高性能。我们的架构在多个视频基准上表现出色，包括VCGBench、MVBench和零样本问答任务。此外，我们开发了一个112K的视频指令集，通过新颖的半自动标注管道进一步提升模型性能。为了全面评估视频LMMs，我们还提出了VCGBench-Diverse，它涵盖了18个广泛视频类别，如生活方式、体育、科学、游戏和监控视频，共4,354个问题-答案对。这个基准测试评估现有LMMs在密集视频描述、空间和时间理解以及复杂推理方面的泛化能力，确保在各种视频类型和动态下的全面评估。代码可在https://github.com/mbzuai-oryx/VideoGPT-plus找到。**|
|**2024-06-13**|**Explore the Limits of Omni-modal Pretraining at Scale**|Yiyuan Zhang et.al.|[2406.09412](http://arxiv.org/abs/2406.09412)|**[link](https://github.com/invictus717/MiCo)**|**我们提议构建全模态智能，旨在理解各种模态并学习通用表示。为此，我们提出了一种可扩展的预训练范式，称为多模态上下文（MiCo）。这种方法能够在预训练过程中同时增加模态数量、数据量以及模型参数的数量。通过MiCo，预训练模型在多项任务上展现出显著的多模态学习能力：一是针对10种不同模态的单模态感知基准，二是包括检索、问答和captioning在内的25项跨模态理解任务，三是18个多模态大语言模型基准。我们的模型创造了37项最新的最高性能记录。我们期望这项研究能推动全模态智能的发展。相关代码和模型已在<https://github.com/invictus717/MiCo>开源。**|
|**2024-06-13**|**Aligning Vision Models with Human Aesthetics in Retrieval: Benchmarks and Algorithms**|Miaosen Zhang et.al.|[2406.09397](http://arxiv.org/abs/2406.09397)|null|现代视觉模型在大规模嘈杂数据集上进行训练，虽然展现出强大能力，但在遵循用户意图、如视觉美感、特定风格和责任输出方面可能存在问题。本文关注视觉美学领域，目标是使视觉模型与人类审美标准在检索系统中保持一致。高级检索系统通常采用基于低级特征（如饱和度）的审美模型作为重排器或过滤器，但面对风格、文化或知识背景时性能有限。我们发现利用大型语言模型（LLM）的推理能力，通过改写搜索查询并扩展审美期望，可以弥补这一不足。  因此，我们提出了一种基于偏好的强化学习方法，该方法针对视觉模型进行微调，以提取LLM推理和审美模型的知识，从而更好地使视觉模型符合人类审美。由于缺乏专门用于评估检索系统的基准，我们利用强大的多模态大模型（LMM）来评价美感表现。考虑到美感评估的主观性，我们还提出了一个名为HPIR的新数据集，用于衡量与人类审美的契合度。实验结果显示，我们的方法显著提升了视觉模型的美感行为，从多个指标来看。我们相信，提出的算法可以作为一种通用实践，用于使视觉模型与人类价值观相一致。|
|**2024-06-13**|**Too Many Frames, not all Useful:Efficient Strategies for Long-Form Video QA**|Jongwoo Park et.al.|[2406.09396](http://arxiv.org/abs/2406.09396)|**[link](https://github.com/jongwoopark7978/LVNet)**|长期视频通常包含大量冗余信息，跨越较长的时间间隔，且包含多个松散关联的事件或实体。因此，在进行长视频问答（LVQA）时，生成正确答案所需的所有信息往往只需一小部分帧就足以提供。近期的研究试图利用大型语言模型（LLMs）在LVQA基准上取得卓越性能，但这些模型依赖于视觉语言模型（VLMs）将视频中的所有视觉内容转换成自然语言。传统做法通常是均匀采样大量帧并独立为其生成描述，这既不高效也不免有冗余。针对这一问题，我们探索了关键帧选择和顺序感知的描述方法，以显著减少这些冗余。  为此，我们提出了两个创新方法：层次关键帧选择器和顺序视觉语言模型。我们的最终框架称为LVNet，在三个基准LVQA数据集上实现了最先进的性能。我们将公开我们的代码。|
|**2024-06-13**|**Needle In A Video Haystack: A Scalable Synthetic Framework for Benchmarking Video MLLMs**|Zijia Zhao et.al.|[2406.09367](http://arxiv.org/abs/2406.09367)|**[link](https://github.com/joez17/videoniah)**|**视频理解是大规模多模态语言模型（MLLMs）的关键下一步。为了检验视频理解的特定方面，现有的视频基准通常需要精心选择与目标能力匹配的视频，并对查询-响应对进行繁琐的标注，以匹配视频内容。这个过程既具有挑战性又资源密集。本文提出VideoNIAH（视频针 haystack），一个通过合成视频生成的基准构建框架。VideoNIAH通过将不相关的图像/文本“针”插入原始视频中，将测试视频内容与它们的查询-响应分离。它仅基于这些针生成注释，确保视频来源的多样性和查询-响应的丰富性。此外，通过插入多个针，VideoNIAH严格评估模型的时序理解能力。我们利用VideoNIAH构建了视频基准VNBench，包括检索、排序和计数等任务。VNBench能够高效地评估视频模型的精细理解能力和时空建模能力，同时支持长距离依赖性的评估。我们还对近期的视频为中心的多模态大型语言模型进行了评估，包括开源和专有模型，提供了全面的分析。尽管专有模型相对于开源模型具有显著优势，但所有现有视频模型在长距离依赖任务上的性能仍然不佳。VideoNIAH是一个简单且高度可扩展的基准构建框架，我们相信它将激发未来视频基准工作的创新。代码和数据已在https://github.com/joez17/VideoNIAH上提供。**|
|**2024-06-13**|**ElicitationGPT: Text Elicitation Mechanisms via Language Models**|Yifan Wu et.al.|[2406.09363](http://arxiv.org/abs/2406.09363)|null|该论文探讨了如何利用无需领域知识的查询来大型语言模型（如ChatGPT）对获取的文本预测进行评分，以评估其与实际状态的一致性。这种方法是激励信息收集和机器学习模型训练的关键组成部分。研究通过在同行评审数据集上进行实验，比较自动的模型评分与人工导师给出的评分，旨在实证评估这些机制与人类偏好的一致性。|
|**2024-06-13**|**DiscreteSLU: A Large Language Model with Self-Supervised Discrete Speech Units for Spoken Language Understanding**|Suwon Shon et.al.|[2406.09345](http://arxiv.org/abs/2406.09345)|null|## 背景  将预训练的文本型大型语言模型（LLMs）与语音输入相结合，已经赋予了这些模型执行多样化语音任务的能力，包括指令跟随。这种整合需要结合语音编码器、语音适配器和LLM，它们分别针对不同的任务进行训练。我们提议使用离散语音单元（DSU），而非连续值的语音编码输出，通过语音适配器将DSU转换到LLM的嵌入空间。我们通过无监督的语音编码器生成DSU，然后运用k-means聚类方法。提出的模型在处理来自见/未见过领域以及口语问答中的指令跟随任务时表现出稳健性能。我们还研究了来自不同自监督语音编码器层的DSU类型，以及梅尔频率倒谱系数（MFCC）。实验结果表明，在口语问答的指令调优任务中，ASR任务和数据集的重要性可能较低。|
|**2024-06-13**|**REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space**|Tomer Ashuach et.al.|[2406.09325](http://arxiv.org/abs/2406.09325)|null|大型语言模型（LLMs）可能无意中记住并泄露训练数据中的敏感或个人识别信息（PII），引发隐私问题。当前的解决方案包括昂贵的数据清洗，或者通过遗忘和模型编辑来过滤模型，但这些方法可能被提取攻击绕过。我们提出了一种新颖的模型编辑方法，名为REVS，用于从LLMs中消除敏感信息。REVS识别并修改与每条敏感信息相关的少量神经元。通过将这些神经元投影到词汇空间（去嵌入），我们定位驱动其生成的关键部分。然后，我们根据去嵌入矩阵的伪逆计算模型编辑，并应用它来降低目标敏感数据的生成概率。为了充分评估我们的方法在真正敏感信息上的效果，我们创建了两个数据集：一个是GPT-J固有的电子邮件数据集，另一个是我们调整模型使其记忆的合成社会保障号码数据集。与最先进的模型编辑方法相比，REVS在消除敏感信息和抵抗提取攻击方面表现出色，同时保持模型的完整性。代码和演示笔记本可在<https://technion-cs-nlp.github.io/REVS>获取。|
|**2024-06-13**|**Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs**|Zhao Xu et.al.|[2406.09324](http://arxiv.org/abs/2406.09324)|**[link](https://github.com/usail-hkust/bag_of_tricks_for_llm_jailbreaking)**|**尽管大型语言模型（LLMs）在零样本任务执行方面展现出显著能力，但它们易受破解攻击，可能被操纵产生有害输出。近期的研究开始将破解攻击分为令牌级和提示级。然而，先前的工作主要忽视了破解攻击的多样关键因素，大部分研究聚焦于LLM的漏洞，而对防御增强的LLMs探索不足。为了改进这一状况，我们评估了不同攻击设置对LLM性能的影响，并提议建立一个基准测试框架，以促进标准化评估。我们从目标级和攻击级两个角度，详细考察了实施针对LLMs的破解攻击的八个关键因素。我们在两个常用数据集上对六种防御方法进行了七种代表性的破解攻击，总计约320个实验，使用A800-80G GPU耗时大约5万小时。实验结果强调了对防御增强的LLMs进行标准化评估的必要性。我们的代码已开源：https://github.com/usail-hkust/Bag_of_Tricks_for_LLM_Jailbreaking。**|
|**2024-06-13**|**JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts Against Large Language Models**|Delong Ran et.al.|[2406.09321](http://arxiv.org/abs/2406.09321)|**[link](https://github.com/thuccslab/jailbreakeval)**|**本文探讨了针对大型语言模型（LLMs）的越狱攻击研究中的评估难题。目前，对于攻击是否成功缺乏统一标准，不同的评估方法如人工标注或特定方式提示GPT-4存在，各有优缺点，对人类价值观的体现和研究成本产生影响。我们的研究分析了近九十项2023年5月至2024年4月期间发布的越狱攻击相关研究，提出了一种详细的评估方法分类体系，深入剖析了各种评估器的优缺点及其应用现状。为了推动后续研究，我们开发并推出了JailbreakEval工具包，它是一个用户友好的平台，集成了多种知名的评估器，用户只需一个命令即可获取结果。此外，JailbreakEval支持用户在统一框架内定制自定义评估流程，简化了开发和比较过程。总之，我们期望JailbreakEval能促进越狱攻击评价的标准化，成为社区内越狱研究评估的催化剂。**|
|**2024-06-12**|**Improving LLMs for Recommendation with Out-Of-Vocabulary Tokens**|Ting-Ji Huang et.al.|[2406.08477](http://arxiv.org/abs/2406.08477)|null|在推荐系统中，通过向量表示用户和项目对于多种任务至关重要。最近的研究尝试将大型语言模型（LLMs）应用于问答形式的推荐，使用词汇表内的标记（如“item”、“20”、“24”）来表示实际的用户和项目。然而，由于LLMs通常是在自然语言任务上预训练的，这些词汇表内的标记在表达独特用户和项目方面能力有限，即使经过推荐任务的微调，也会削弱推荐性能。本文探讨如何有效在LLM基的推荐系统中处理用户和项目的标记。  我们强调了出词汇表（OOV）标记的作用，它们除了词汇表内的标记外，还能捕捉用户/项目之间的关联性和多样性。通过分析历史用户-项目交互的表示学习，我们使具有相似特性的用户/项目组合共享相同的OOV标记。此外，将这些OOV标记整合到LLM的词汇表中，有助于更好地区分用户和项目，增强在下游任务微调时对用户-项目关系的捕捉。  我们的提出的框架在各种下游推荐任务上超越了现有最先进的方法。|
|**2024-06-12**|**Real2Code: Reconstruct Articulated Objects via Code Generation**|Zhao Mandi et.al.|[2406.08474](http://arxiv.org/abs/2406.08474)|null|我们提出了一种新颖的方法——Real2Code，旨在通过代码生成来重建可动物体。给定物体的视觉观测，我们首先利用图像分割模型和形状补全模型重构其部件几何结构。接着，我们将物体部件表示为带有方向的边界框，然后输入到一个经过微调的大语言模型（LLM）中，预测关节活动的代码表示。通过利用预训练的视觉和语言模型，我们的方法能够优雅地扩展到具有更多可动部件的对象，并能从合成训练数据中泛化到现实世界中的不规则环境物体。实验结果表明，Real2Code在重建精度上显著优于现有最先进的方法，并且是首个能够超越训练集中对象结构复杂性的方法，能够重建多达10个可动部件的物体。当与立体重建模型结合时，Real2Code还能从少量多视图RGB图像中泛化到现实世界的物体，无需深度或相机信息。|
|**2024-06-12**|**Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing**|Zhangchen Xu et.al.|[2406.08464](http://arxiv.org/abs/2406.08464)|**[link](https://github.com/magpie-align/magpie)**|高质量的指令数据对于调整大型语言模型至关重要。尽管像Llama-3-Instruct这样的模型公开了权重，但它们的对齐数据仍然保密，这限制了人工智能的普及。现有的开源数据生成方法受限于高昂的人力成本和有限的提示范围，难以有效扩展，可能影响公共对齐数据集的多样性和质量。能否通过直接从已对齐的大型语言模型中提取，大规模合成高质指令数据呢？我们提出了一种自我合成方法，称为Magpie。我们的关键观察是，由于Llama-3-Instruct等已对齐的模型具有自回归特性，当我们仅输入左侧模板到用户消息预留位置时，它们可以生成用户查询。我们利用这种方法提示Llama-3-Instruct，生成了400万个指令及其对应的响应。我们对提取的数据进行了全面分析，并选择了30万个高质量实例。为了比较Magpie数据与其他公共指令数据集，我们分别使用每个数据集对Llama-3-8B-Base进行微调，并评估微调后模型的性能。结果显示，在某些任务中，仅使用Magpie进行微调的模型在性能上与官方经过1000万个数据点监督微调（SFT）和后续反馈学习增强的Llama-3-8B-Instruct相当。我们还展示了仅使用Magpie进行SFT可以超越先前用于SFT和偏好优化（如UltraFeedback的直接偏好优化）的公共数据集。这种优势在AlpacaEval、ArenaHard和WildBench等对齐基准测试中表现明显。|
|**2024-06-12**|**TasTe: Teaching Large Language Models to Translate through Self-Reflection**|Yutong Wang et.al.|[2406.08434](http://arxiv.org/abs/2406.08434)|**[link](https://github.com/yutongwang1216/reflectionllmmt)**|**大型语言模型在自然语言处理任务中展现出卓越性能，特别是通过指令调优后，在机器翻译（Machine Translation, MT）等下游任务中的表现有所提升。然而，这些方法未能达到与监督神经机器翻译（Supervised Neural Machine Translation, NMT）系统相当的翻译质量。原因可能是当前使用的简单提示无法充分利用模型的指令跟随能力。为此，我们提出了TasTe框架，即“通过自我反思进行翻译”。该框架包括两个推理阶段：第一阶段，模型被引导生成初步翻译并同时对其自身进行评估；第二阶段，模型根据评估结果对初步翻译进行细化。在WMT22基准的四种语言方向上，我们的方法显示出与现有技术相比的有效性。这项工作展示了一种有前景的方法，能够释放大型语言模型的潜力，并增强其在机器翻译领域的性能。相关代码和数据已在https://github.com/YutongWang1216/ReflectionLLMMT上开源。**|
|**2024-06-12**|**Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL**|Zijin Hong et.al.|[2406.08426](http://arxiv.org/abs/2406.08426)|null|文本转SQL生成准确的SQL查询以响应自然语言问题是一个长期存在的挑战，它涉及用户问题理解、数据库模式理解以及SQL生成等多个复杂环节。传统的文本转SQL系统依赖于人工工程和深度神经网络。随着预训练语言模型（PLMs）的发展和在该任务中的应用，性能得到了显著提升。然而，随着数据库复杂度增加和用户问题难度增大，PLMs有限的理解能力可能导致错误的SQL生成，这促使研究人员寻求更高级和定制化的优化方法，限制了PLM基础系统的广泛应用。最近，大型语言模型（LLMs）因其在自然语言理解上的强大能力而备受瞩目。因此，整合LLM的实现为文本转SQL研究带来了独特的机遇、挑战和解决方案。本综述全面概述了基于LLM的文本转SQL。首先，我们概述当前面临的挑战和文本转SQL的发展历程。接着，详细介绍用于评估文本转SQL系统的数据集和评价指标。然后，我们系统分析了近期在LLM支持下的文本转SQL进展。最后，我们讨论了该领域尚存的挑战，并对未来研究方向提出期待。|
|**2024-06-12**|**OmniCorpus: An Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text**|Qingyun Li et.al.|[2406.08418](http://arxiv.org/abs/2406.08418)|**[link](https://github.com/opengvlab/omnicorpus)**|**该论文介绍了一种名为OmniCorpus的大型图像-文本交错数据集，规模达到100亿级别。这个数据集通过高效的引擎筛选和提取了大量高质量文档，包含86亿张图片和1,696万亿个文本令牌，相较于同类数据（如MMC4、OBELICS），OmniCorpus具有以下优势：1）规模扩大15倍，同时保持了良好的数据质量；2）来源更为多样，包括英文和非英文网站，以及视频为主的网站；3）灵活性更强，可以从图像-文本交错格式轻松转换为纯文本语料库或图像-文本对。通过全面分析和实验，论文验证了OmniCorpus的数据质量、可用性和有效性，旨在为未来的多模态模型研究提供坚实的数据基础。相关的代码和数据已在https://github.com/OpenGVLab/OmniCorpus上公开。**|
|**2024-06-12**|**Discovering Preference Optimization Algorithms with and for Large Language Models**|Chris Lu et.al.|[2406.08414](http://arxiv.org/abs/2406.08414)|**[link](https://github.com/luchris429/DiscoPOP)**|****中文翻译：**  离线偏好优化是提升和控制大型语言模型（LLM）输出质量的重要方法。传统上，偏好优化被视为基于人工设计的凸损失函数的离线监督学习任务。然而，这些方法受限于人类创造力，未能充分探索可能的损失函数的巨大搜索空间。为此，我们提出了一种利用LLM进行目标发现的方法，以自动发现新的最先进的偏好优化算法，无需（专家）人工干预。具体来说，我们通过迭代地提示LLM，根据先前的性能评估提出并实现新的偏好优化损失函数。这个过程导致了未知且高效的优化算法的发现。其中最好的一个被命名为“发现偏好优化”（DiscoPOP），这是一种新颖的算法，它巧妙地融合了逻辑和指数损失。实验结果表明，DiscoPOP在性能上达到了最新水平，并成功地应用于未见过的任务上。**|
|**2024-06-12**|**Memory Is All You Need: An Overview of Compute-in-Memory Architectures for Accelerating Large Language Model Inference**|Christopher Wolters et.al.|[2406.08413](http://arxiv.org/abs/2406.08413)|null|## 背景  大型语言模型（LLMs）近期在自然语言处理领域取得了显著进步，使得机器能够生成逼真的文本并进行有意义的对话。然而，随着计算和内存需求的急剧增长，尤其是当LLMs超越单个GPU的处理能力时，对速度、效率和可访问性的需求也随之增加。同时，计算机性能和内存能力的发展并未跟上步伐，尤其是在摩尔定律放缓的背景下。内存访问成本远高于计算，这给大规模扩展带来了挑战，即所谓的“内存墙”。在这个时候，计算在内存（Compute-in-Memory, CIM）技术为AI推理提供了加速可能，通过在内存中直接执行模拟计算，有望降低延迟和功耗。通过紧密集成内存和计算元件，CIM消除了冯诺依曼瓶颈，减少了数据传输，提高了能源效率。  本综述论文概述了基于变压器的模型，探讨了各种CIM架构，并研究了它们如何应对现代人工智能计算系统面临的紧迫挑战。我们详细讨论了与变压器相关的运算及其硬件加速策略，同时指出相关CIM设计中的挑战、趋势和洞察。|
|**2024-06-12**|**Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models**|Chun-Yi Kuan et.al.|[2406.08402](http://arxiv.org/abs/2406.08402)|**[link](https://github.com/kuan2jiu99/audio-hallucination)**|**## 背景 大型音频语言模型（LALMs）通过整合音频感知能力，增强了传统的大规模语言模型，使其能够处理音频相关任务。先前的研究主要集中在评估LALMs在各种任务上的性能，但对它们的可靠性，特别是关于对象幻觉等问题的关注不足。我们的研究中，我们提出方法来评估公开可用的LALMs在对象幻觉方面的程度。结果表明，LALMs在理解音频内容方面与专门的音频captioning模型相当，但在回答区分性问题时表现不佳，尤其是那些需要识别音频片段中特定物体声音的问题。这揭示了当前LALMs的一个关键弱点：它们对区分性查询的理解不足。此外，我们还探讨了提示工程如何提升LALMs在区分性问题上的性能。**|
|**2024-06-12**|**cPAPERS: A Dataset of Situated and Multimodal Interactive Conversations in Scientific Papers**|Anirudh Sundar et.al.|[2406.08398](http://arxiv.org/abs/2406.08398)|null|## 背景 在情境化和多模态交互对话（SIMMC）的新兴研究领域中，科学论文的互动是一个重要方向。由于科学论文主要由文本、公式、图表和表格构成，SIMMC方法需要针对这些组成部分进行专门设计，以支持科研人员所需的深度探究和互动。本研究提出了一种名为“对话式论文”（cPAPERS）的数据集，它包含了来自arXiv上可用的科学文档的学术论文评论中的问答对，这些问答与论文组件及其引用相关。我们介绍了数据收集策略，通过OpenReview收集这些问题-答案对，并与LaTeX源文件中的上下文信息关联起来。此外，我们展示了使用大型语言模型（LLMs）的一系列基线方法，包括零样本和微调配置，来处理cPAPERS数据集。|
|**2024-06-11**|**Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena**|Aidar Myrzakhan et.al.|[2406.07545](http://arxiv.org/abs/2406.07545)|**[link](https://github.com/vila-lab/open-llm-leaderboard)**|**### 背景  多项选择题（MCQ）常用于评估大型语言模型（LLMs）。通常，LLM会根据调整后的概率，如长度因素，选择最可能的答案。然而，LLMs可能存在固有的偏见，例如对A、B、C、D等选项ID的偏好，这可能影响答案预测。先前的研究通过在少数测试样本上随机打乱选项，并将其应用到新样本上，试图减少这种“选择偏差”。此外，MCQ的另一个问题是“彩票式猜测”，即LLM并未真正学习知识，而是凭运气猜对答案，这对小型LLMs尤为严重。  为解决这些问题，一个更全面的方法是转向开放式问题，这能从根本上消除选择偏差和随机猜测。但转向开放式问题也带来了挑战：一是如何识别合适的开放性问题，二是如何验证LLM对开放式问题的回答与人类标注的真实答案之间的准确性。本研究旨在解决这些难题，并建立一个新的LLM评估基准，通过完全的开放式问题来衡量模型性能，例如GPT-4o/4/3.5、Claude 3、Gemini等。  ### 任务  我们创建了Open-LLM-Leaderboard，这是一个新的评价平台，旨在跟踪各种LLM的表现，揭示它们的真实能力。我们的代码和数据集已开源，可在此链接获取：https://github.com/VILA-Lab/Open-LLM-Leaderboard。**|
|**2024-06-11**|**QuickLLaMA: Query-aware Inference Acceleration for Large Language Models**|Jingyao Li et.al.|[2406.07528](http://arxiv.org/abs/2406.07528)|**[link](https://github.com/dvlab-research/q-llm)**|**大型语言模型（LLMs）在理解和处理长序列方面的能力对于各领域的发展至关重要。然而，它们在捕捉序列中的长期依赖关系以深入理解语义方面仍然存在挑战。为此，我们提出了Query-aware Inference for LLMs（Q-LLM），这是一种旨在模仿人类认知处理大规模序列的系统。通过聚焦于与给定查询相关的内存数据，Q-LLM能够在固定窗口大小内准确捕捉相关信息，并为查询提供精确的答案，无需额外训练，可无缝集成到任何LLMs中。使用LLaMA3（QuickLLaMA）的Q-LLM能在30秒内阅读《哈利·波特》，并能准确回答问题。相较于当前最先进的LLaMA3，Q-LLM的性能提升了7.17%，而在Mistral上，它在 $\infty$ -bench上的表现提升了3.26%。在“针锋相对”任务中，Q-LLM在广泛认可的基准上，相对于当前最佳成绩，Mistral上的提升达到了7.0%，在LLaMA3上实现了100%的准确率。我们的代码已在https://github.com/dvlab-research/Q-LLM上开源。**|
|**2024-06-11**|**Beyond Model Collapse: Scaling Up with Synthesized Data Requires Reinforcement**|Yunzhen Feng et.al.|[2406.07515](http://arxiv.org/abs/2406.07515)|null|随着生成模型合成数据的兴起，越来越多地被用于大型语言模型的微调，这引发了对模型崩溃（即微调性能下降）的关注。由于人类和机器都较容易分辨好样本和坏样本，而非生成高质量样本，我们探讨了如何利用反馈来防止模型在合成数据上出现崩溃。我们理论分析了一个高斯混合分类模型在基于反馈增强的合成数据训练下的最优性能，并提供了有限样本情况下的实验证据。我们在两个实际问题上展示了这些理论预测：使用变压器计算矩阵特征值和利用大型语言模型进行新闻摘要，这两种情况下模型在生成数据上都会经历崩溃。我们发现，通过从反馈增强的合成数据中训练，无论是修剪错误预测还是选择最佳猜测，都能防止模型崩溃，证实了像RLHF（Reinforcement Learning with Human Feedback）这样的流行方法的有效性。|
|**2024-06-11**|**THaLLE: Text Hyperlocally Augmented Large Language Extension -- Technical Report**|KBTG Labs et.al.|[2406.07505](http://arxiv.org/abs/2406.07505)|null|## 背景  近期大型语言模型（LLMs）的进步在科技领域展现了新功能和机遇。然而，非常大的LLMs的实际应用受到其高计算成本的制约，这与其相对有限的人类能力相比，收益并不明显。尽管小型、更实用的LLMs在金融分析方面展现出潜力，但它们尚未完全掌握，如它们在模拟特许金融分析师（CFA）考试中的接近通过表现所示。本文中，我们展示了Financial Analyst Extension（FAE）对我们的Text Hyperlocally Augmented Large Language Extension（THaLLE）系列的扩展，这一系列80亿参数的LLMs在模拟CFA考试中始终表现出最高性能，与同类规模的模型相比。我们详细记录了用于优化的微调技术，以供后续研究参考。此外，我们引入Flare CFA，这是一个公开可用的金融顾问评估数据集，用于检验LLMs在财务顾问角色中的能力。|
|**2024-06-11**|**Image Textualization: An Automatic Framework for Creating Accurate and Detailed Image Descriptions**|Renjie Pi et.al.|[2406.07502](http://arxiv.org/abs/2406.07502)|**[link](https://github.com/sterzhang/image-textualization)**|**## 背景  图像描述数据集对于推动图像理解、文本到图像生成和文本图像检索等应用至关重要。当前，这些数据集主要来自两个途径：一是从网络上抓取图像与文字对，但这类描述往往质量较低且存在噪声；二是人工标注，如COCO等，通常描述简洁，缺乏详细信息。尽管详细的图像描述可以通过人类标注获得，但高昂的标注成本限制了其可行性。这些局限性促使我们寻求更有效和可扩展的方法来生成准确而详尽的图像描述。  本文提出了一种创新框架，称为“图像文本化”（Image Textualization，简称IT），它通过协同利用现有的多模态大型语言模型（Multimodal Large Language Models，MLLMs）和视觉专家模型，有效地将视觉信息转化为文本，从而自动生成高质量的图像描述。针对当前缺乏详尽描述的基准问题，我们还提出了多个评价基准，以全面评估我们的框架生成的图像描述质量。  此外，我们展示了在IT精心编纂的描述训练下，LLaVA-7B模型的图像描述生成能力得到了提升，能够生成更丰富的描述，输出长度和细节显著增加，同时减少了幻觉现象。**|
|**2024-06-11**|**TextGrad: Automatic "Differentiation" via Text**|Mert Yuksekgonul et.al.|[2406.07496](http://arxiv.org/abs/2406.07496)|**[link](https://github.com/zou-group/textgrad)**|**人工智能正经历一场范式转变，通过大型语言模型（LLMs）和其他复杂组件的协同工作取得了突破。当前，为复合人工智能系统设计原则化的自动化优化方法成为一项关键新挑战。神经网络在早期面临类似问题时，通过反向传播和自动微分实现了重大革新。受此启发，我们提出了TextGrad，这是一个强大的框架，它通过文本实现自动“微分”，将LLMs提供的丰富、通用的自然语言建议回传到复合AI系统的各个组件中。TextGrad遵循PyTorch的语法和抽象，易于使用且灵活，用户仅需提供目标函数，无需调整框架组件或提示，即可无缝应用。  TextGrad适用于多种任务，从问答和分子优化到放射治疗计划设计。在无需修改框架的情况下，它显著提升了GPT-4o在Google证明性问题回答中的零-shot准确率，从51%提升至55%；在优化LeetCode难题解法上实现了20%的相对性能提升；改进了推理提示，设计出具有理想体外亲和力的新药候选分子；以及设计出具有高特异性的放射治疗方案。TextGrad为下一代AI系统的发展奠定了基础，推动了复合AI技术的加速发展。**|
|**2024-06-12**|**CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization**|Frederic Kirstein et.al.|[2406.07494](http://arxiv.org/abs/2406.07494)|null|该文章综述了2019年至2024年间发表的1262篇独特的研究论文，集中在Transformer架构在英文对话摘要生成方面的研究。文章详细探讨了对话摘要中存在的主要挑战，如语言理解、结构处理、理解能力、说话者识别、重要性判断和事实准确性，并与相应的技术，如图解方法、额外训练任务和规划策略进行了关联。尽管在某些方面（如语言）取得了显著进展，但如理解力、真实性与重要性评估等挑战仍然存在，提供了丰富的研究空间。  文章还分析了评估这些方法的方式，涵盖了对话子领域（如会议、医疗）的常用数据集，以及自动评价指标（如ROUGE）和人类评估的普遍实践。然而，发现跨领域的数据集相对有限，且报告的人类评估往往缺乏足够的内审员一致性信息和标注指南细节。此外，文章讨论了大语言模型的最新探索可能带来的影响，指出尽管它们可能会改变相关性和难度，但描述的挑战分类体系仍然具有价值。|
|**2024-06-11**|**PITCH: Productivity and Mental Well-being Coaching through Daily Conversational Interaction**|Adnan Abbas et.al.|[2406.07485](http://arxiv.org/abs/2406.07485)|null|高效的计划制定对生产力和心理健康至关重要，但人们往往难以制定实际的计划并反思自己的效率。利用人工智能的发展，对话助手作为一种有前景的工具，旨在通过对话方式将计划外化，强化决心，促进专注行动，从而正面影响生产力和心理健康。我们的研究目标是设计一个对话助手，通过自然对话的社交互动性，提供深入的问题和反思提示，以提高计划执行度。尽管先前的研究显示了这些代理的效益，但许多干预措施仍保持静态，可能导致用户参与度随时间下降。为了弥补这一不足，我们提出了一种新颖的旋转和上下文感知的提示策略，每天为用户提供多样的干预手段。我们的系统PITCH利用大语言模型（LLMs）来促进日常计划的外部化和反思。本研究旨在探究与对话代理一起外化任务对生产力和心理健康的影响，以及旋转策略在保持用户参与度方面的有效性。|
|**2024-06-11**|**Advancing Annotation of Stance in Social Media Posts: A Comparative Analysis of Large Language Models and Crowd Sourcing**|Mao Li et.al.|[2406.07483](http://arxiv.org/abs/2406.07483)|null|在快速发展的自然语言处理领域，大型语言模型（LLMs）在社交媒体帖子的自动文本标注方面展现出浓厚兴趣。本文研究了八种开源和专有LLMs在立场标注任务中的性能，将其与人类（通过众包）的判断进行基准测试。我们探究了何时LLMs可能与人类判断产生分歧的情况。研究发现，文本中表达立场的明确程度对LLMs判断与人类一致性至关重要。当人类注释者表现良好时，LLMs也表现出色；反之，LLMs的失败往往对应于人类难以达成一致的情境。因此，我们建议结合人类专业知识的精确度与LLMs预测的规模，提出一种全面的方法。这项研究强调了提高自动化立场检测准确性和全面性的必要性，旨在推动这些技术在更高效、无偏见的社会媒体分析中得到提升。|
|**2024-06-11**|**VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs**|Zesen Cheng et.al.|[2406.07476](http://arxiv.org/abs/2406.07476)|**[link](https://github.com/damo-nlp-sg/videollama2)**|**本文介绍VideoLLaMA 2，一套专为提升视频和音频定向任务中的空间-时间建模及音频理解能力而设计的视频大型语言模型（Video-LLMs）。它在前一代的基础上增添了定制的时空卷积（STC）连接器，有效地捕捉视频数据的复杂空间和时间动态。此外，我们通过联合训练融入了音频分支，增强了模型的多模态理解能力，使其能无缝融合音频线索。在多项评估中，如多选视频问答（MC-VQA）、开放性视频问答（OE-VQA）和视频captioning（VC）任务上，VideoLLaMA 2表现出与开源模型相当的竞争实力，并在某些基准上接近专有模型。在音频仅用（AQA）和音频-视频问答（OE-AVQA）任务上，VideoLLaMA 2也显示出对现有模型的合理改进。这些进步凸显了VideoLLaMA 2在多模态理解方面的卓越性能，为智能视频分析系统树立了新标准。所有模型均公开以促进进一步研究。**|
|**2024-06-10**|**Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation**|Peize Sun et.al.|[2406.06525](http://arxiv.org/abs/2406.06525)|**[link](https://github.com/foundationvision/llamagen)**|**我们提出LlamaGen，这是一种全新的图像生成模型家族，它将大型语言模型的原始“下一个词预测”范式应用于视觉生成领域。这表明，如果适当扩展，未经视觉特性的先验知识增强的纯自回归模型（如Llama）也能达到最先进的图像生成性能。我们的研究探索了图像分词器的设计空间、图像生成模型的可扩展性以及训练数据质量，结果如下：(1) 一种具有16倍下采样的图像分词器，其在ImageNet基准上的重构质量为0.94，代码书利用率高达97%。(2) 一系列从111百万到31亿参数的类条件图像生成模型，在ImageNet 256x256基准上实现了2.18的FID分数，超越了流行的扩散模型，如LDM和DiT。(3) 一个7.75亿参数的文本条件图像生成模型，通过两阶段训练在LAION-COCO和高审美质量图像上，显示出良好的视觉质量和文本一致性性能。(4) 我们验证了大语言模型服务框架在优化图像生成模型推理速度方面的有效性，实现了326%至414%的速度提升。我们开源所有模型和代码，以促进视觉生成和多模态基础模型的开放源代码社区的发展。**|
|**2024-06-10**|**UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor**|Shivani Upadhyay et.al.|[2406.06519](http://arxiv.org/abs/2406.06519)|**[link](https://github.com/castorini/umbrela)**|**## 翻译  大量相关性判断对于检索系统的有效训练和精确评估至关重要。传统上，这些判断由人工评定员完成，过程昂贵且耗时。微软Bing的Thomas等人最近的一项研究表明，大型语言模型（LLMs）能够准确地进行相关性评估，提供与人类相当的判断。遗憾的是，他们的研究并未公开可供重复使用的软件工具。我们的工作介绍了一个开源工具包——UMBRELA（全称为“UMBRELA是Bing RELevance Assessor的递归缩写”），它基于OpenAI的GPT-4模型复现了Thomas等人的结果，并为原论文增添了更多细节。我们在TREC 2019年至2023年的深度学习任务中发现，LLM生成的相关性判断与高效多阶段检索系统生成的排名高度相关。该工具包设计为易于扩展，可以融入现有的多阶段检索和评估流程，为研究检索评估方法的研究者提供了宝贵的资源。UMBRELA将在TREC 2024年的RAG任务中用于辅助相关性评估，我们期望它成为该领域进一步创新的基础。UMBRELA的代码库可于https://github.com/castorini/umbrela获取。**|
|**2024-06-10**|**NarrativeBridge: Enhancing Video Captioning with Causal-Temporal Narrative**|Asmar Nadeem et.al.|[2406.06499](http://arxiv.org/abs/2406.06499)|null|当前的视频字幕基准和模型在表征因果时间叙事方面存在不足，这种叙事是通过因果关系连接的一系列事件，随时间发展，由人物或主体驱动。这种缺乏叙事性限制了模型生成捕捉视频内容内在因果和时间动态的文本描述的能力。为填补这一空白，我们提出NarrativeBridge，它包括以下两个组成部分：（1）一个由大型语言模型通过少量提示生成的新型因果时间叙事（CTN）字幕基准，该基准明确地在视频描述中编码因果关系，通过自动评估确保质量和相关性；（2）一个专门的因果网络（CEN）架构，具有独立的编码器以分别捕获因果动态，从而实现有效的学习和生成具有因果时间叙事的字幕。实验结果表明，CEN在表达视频内容的因果和时间方面比第二好的模型（GIT）更准确：在MSVD和MSR-VTT数据集上的CIDEr分数分别为17.88和17.44。提出的框架能够理解和生成具有复杂因果时间叙事结构的细微文本描述，这是视频字幕生成的一个关键局限性。有关项目详情，请访问<https://narrativebridge.github.io/>。|
|**2024-06-10**|**Towards a Personal Health Large Language Model**|Justin Cosentino et.al.|[2406.06474](http://arxiv.org/abs/2406.06474)|null|在健康领域，大部分大型语言模型（LLM）的研究集中在临床任务上。然而，移动和可穿戴设备提供的丰富、长期的个人健康监测数据往往被忽视。本文介绍了一种名为Personal Health Large Language Model（PH-LLM）的新模型，它是Gemini的定制版，专为理解和处理数值时间序列的个人健康数据而设计。我们创建并整理了三个测试集，考察了PH-LLM在以下方面的性能：1）从睡眠模式、身体活动和生理反应中生成个性化见解和建议；2）专业知识领域的专家水平；3）预测自我报告的睡眠结果。我们与领域专家合作构建了857个案例研究，以评估实际的睡眠和健身场景。通过针对特定领域的评分标准进行全面评估，我们发现Gemini Ultra 1.0和PH-LLM在健身方面与专家表现无统计差异，尽管在睡眠方面专家仍占优势，但Fine-tune后的PH-LLM在利用相关领域知识和个人化睡眠信息方面表现出显著提升。我们还通过多项选择的睡眠医学和健身考试评估了PH-LLM的专业知识，其得分分别为79%和88%，超过了人类专家样本的平均分。最后，我们训练PH-LLM预测来自可穿戴设备文本和多模态编码数据的自我报告睡眠质量结果，并证明了多模态编码对于达到专门区分模型的性能至关重要。尽管在个人健康这个关键安全领域还需要进一步发展和评估，但这些结果展示了Gemini模型的广泛知识和能力，以及将生理数据应用于个人健康应用，如PH-LLM中的做法。|
|**2024-06-10**|**AID: Adapting Image2Video Diffusion Models for Instruction-guided Video Prediction**|Zhen Xing et.al.|[2406.06465](http://arxiv.org/abs/2406.06465)|null|文本引导的视频预测（TVP）任务旨在根据初始帧和指令预测后续帧的运动，这对于虚拟现实、机器人技术和内容创作等领域具有广泛的应用。尽管先前的方法通过改编Stable Diffusion在该任务上取得了重大进展，但它们在帧一致性与时间稳定性方面仍存在问题，主要受限于视频数据集的规模。我们观察到，预训练的Image2Video扩散模型对视频动态有良好的先验知识，但缺乏文本控制。因此，将Image2Video模型转移，同时注入指令控制以生成可控制的视频，既具有意义又颇具挑战。  为了实现这一目标，我们提出了多模态大型语言模型（MLLM），用于根据初始帧和文本指令预测未来的视频状态。特别地，我们设计了双查询Transformer（DQFormer）架构，它将指令和帧信息整合到条件嵌入中，用于未来帧的预测。此外，我们开发了长短期时序适配器和空间适配器，能够在少量训练成本下快速将通用视频扩散模型适应特定场景。  实验结果表明，我们的方法在Something Something V2、Epic Kitchen-100、Bridge Data和UCF-101四个数据集上显著优于现有技术。特别是在Bridge数据集和SSv2上，AID分别实现了91.2%和55.5%的FVD改进，这证明了其在不同领域的有效性。更多示例可在我们的网站<https://chenhsing.github.io/AID>找到。|
|**2024-06-10**|**Transforming Wearable Data into Health Insights using Large Language Model Agents**|Mike A. Merrill et.al.|[2406.06464](http://arxiv.org/abs/2406.06464)|null|尽管可穿戴健康追踪器日益普及，睡眠和运动对健康的重要性不言而喻，但从这些数据中提取具有行动价值的个性化见解仍是一个挑战。这需要对大量数据进行非结构化分析。随着大型语言模型（LLM）的兴起，它们能够利用工具理解和与世界互动，为大规模个性化分析带来了希望。然而，在个人健康领域的LLM应用尚待开发。本文介绍了一种名为Personal Health Insights Agent（PHIA）的系统，它利用最新的代码生成和信息检索工具来分析和解释行为健康数据。我们构建了两个超过4000个健康洞察问题的基准问答数据集。根据650小时的人类和专家评估，PHIA能准确回答84%以上的事实性数值问题，以及超过83%的众包开放性问题。这项工作对于推动大众行为健康进步具有重要意义，可能使个人能够解读自己的可穿戴数据，开辟了一个以数据驱动洞察为指导的个性化健康方案的新时代，使得健康保健更加便捷且个性化。|
|**2024-06-11**|**Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies**|Junlin Wang et.al.|[2406.06461](http://arxiv.org/abs/2406.06461)|null|这篇论文指出，尽管已经提出了多种推理策略来评估大型语言模型的能力，但传统的评价方法仅关注性能指标，忽视了一个关键因素：额外计算资源带来的增效。这可能导致对策略效率的片面理解。为此，论文提出了一种框架，将计算预算纳入评估，以提供一个既考虑性能指标又考虑计算成本的更全面比较。通过这种预算意识的视角，研究发现复杂的推理策略在没有显著算法创新的情况下，往往由于分配了更多的计算资源而超越了简单的基线。例如，当给予链式思考自洽性（chain-of-thought self-consistency）类似级别的计算资源，它常常能优于文献中提出的推理策略。然而，在这种规模敏感的视角下，某些策略如多代理辩论或多反思在增加计算预算时可能会表现得更差。|
|**2024-06-10**|**Evaluating the Retrieval Component in LLM-Based Question Answering Systems**|Ashkan Alinejad et.al.|[2406.06458](http://arxiv.org/abs/2406.06458)|null|## 背景  大规模语言模型（LLMs）驱动的问答系统在依赖检索组件时，能够获取领域特定信息并降低产生不准确回复或错误信息的风险。尽管信息检索领域的评估方法早已存在，但如何评估LLMs驱动的聊天机器人中的检索器性能仍是一个挑战。本研究提出了一种简单的基准方法，用于评价基于检索增强生成（Retrieval-Augmented Generation，RAG）的聊天机器人中的检索器。  ## 任务  我们的研究发现，这种方法能更全面地反映检索器的性能，并与整个问答系统的整体表现更为一致。尽管传统的精确度（precision）、召回率（recall）和F1分数等指标可能无法完全揭示LLMs的能力，因为它们可能会在检索器不完美时仍提供准确答案，但我们的评估方法考虑到了LLMs的优势，即它们能够忽略无关上下文，同时也能处理可能存在的错误和虚构内容。|
|**2024-06-10**|**A Large Language Model Pipeline for Breast Cancer Oncology**|Tristen Pool et.al.|[2406.06455](http://arxiv.org/abs/2406.06455)|null|大型语言模型在众多领域展现出创新潜力，但在癌症治疗方面的应用仍需进一步开发。研究者使用一种新颖的Langchain提示工程管道，对最先进的OpenAI模型进行了微调，数据集包括临床数据和临床指南文本，专注于乳腺癌患者辅助放疗和化疗两个关键治疗因素。结果显示，模型在分类这两个治疗手段时达到了高精度（0.85+）。通过观察人类肿瘤学家的治疗质量数据，建立了一个置信区间，估计模型在预测治疗方案时必须比原始肿瘤学家表现得更好，才能在总体上成为更好的解决方案的比例为8.2%至13.3%。由于癌症治疗决策结果的不确定性，未来可能需要进行临床试验来验证这一阈值。考虑到美国85%的癌症患者在地方社区设施接受治疗，这类模型有可能显著扩大优质护理的可及性，其效果至少接近人类肿瘤学家。|
|**2024-06-10**|**Insights from Social Shaping Theory: The Appropriation of Large Language Models in an Undergraduate Programming Course**|Aadarsh Padiyath et.al.|[2406.06451](http://arxiv.org/abs/2406.06451)|null|大型语言模型（LLMs）在代码生成、调试和解释方面的性能引发了许多研究者和教育工作者对本科编程教育的关注，他们期待这些模型能革新编程教学。然而，关于如何以及为何在编程教育中使用LLMs的决策可能不仅仅基于技术评估。本研究以社会塑造技术理论为指导框架，探讨了学生对LLMs的社会感知如何影响他们的使用行为。我们通过分析一份匿名的课程结束时的调查问卷（n=158）、中期自我效能问卷（n=158）、10位学生的深度访谈、自我报告的LLM在作业中的使用情况，以及期中考试成绩，发现学生的LLM使用与其对未来职业的期望和对同伴使用的感知有关。此外，我们发现早期自我报告的LLM使用与较低的自我效能和中期考试成绩相关，而学生对过度依赖LLM的感知，而非实际使用，与课程后期的自我效能下降有关。|
|**2024-06-07**|**3D-GRAND: Towards Better Grounding and Less Hallucination for 3D-LLMs**|Jianing Yang et.al.|[2406.05132](http://arxiv.org/abs/2406.05132)|**[link](https://github.com/sled-group/3D-GRAND)**|在这个研究中，语言与三维感知的融合对于构建理解和互动于物理世界的实体代理和机器人至关重要。尽管大型语言模型（LLMs）在语言理解和生成方面表现出色，但在适应三维环境（3D-LLMs）方面仍处于初级阶段，主要挑战在于缺乏大规模的密集地将语言与三维场景关联的数据集。为此，我们提出了3D-GRAND，这是一个开创性的大型数据集，包含40,087个家庭场景，配对有620万条详尽的场景-语言指令。实验结果显示，使用3D-GRAND进行指令调优显著提高了3D-LLMs的定位能力，并减少了错误的想象。我们还设计了3D-POPE基准，用于系统性评估3D-LLMs中的幻觉问题，以促进未来模型的公平比较。  我们的实验揭示了数据集规模与3D-LLM性能之间的关联，强调了大型三维文本数据集在推动体感AI研究中的关键作用。值得注意的是，初步迹象表明，通过在大型合成数据上训练的模型可能在现实世界3D扫描中表现良好，这展示了模拟到实际的迁移学习潜力。通过3D-GRAND和3D-POPE，我们旨在为体感AI社区提供必要的资源和洞见，推动更可靠、更扎实的3D-LLMs的发展。项目网站：https://3d-grand.github.io|
|**2024-06-07**|**An Empirical Study on Parameter-Efficient Fine-Tuning for MultiModal Large Language Models**|Xiongtao Zhou et.al.|[2406.05130](http://arxiv.org/abs/2406.05130)|null|这篇论文关注的是大型多模态语言模型（MLLMs）的参数高效微调（PEFT）。由于这些模型通常具有数十亿参数，全面调整变得困难。研究目标是找出在参数受限情况下提升MLLM性能的有效方法。通过实验使用四种流行的PEFT技术对开源MLLMs的LLM组件进行微调，论文进行了详尽的分析，内容包括不同方法对模型、参数位置、微调数据规模、模型稳定性、泛化能力以及幻觉的影响。研究涵盖了两种类型的七项数据集：未见过的和已见过的。结果显示，适配器是最有效的PEFT方法，而连接器层的微调在大多数情况下能提高性能。研究代码和数据可在<https://github.com/alenai97/PEFT-MLLM.git>获取。|
|**2024-06-07**|**Towards Semantic Equivalence of Tokenization in Multimodal LLM**|Shengqiong Wu et.al.|[2406.05127](http://arxiv.org/abs/2406.05127)|null|### 背景  多模态大型语言模型（MLLMs）在处理视觉语言任务方面展现出卓越性能。MLLM的核心在于视觉 tokenization，即如何有效地将输入的视觉信号转化为对语言模型有益的特征表示。然而，现有的视觉tokenizer在保持视觉与语言的语义一致性上存在问题，它们过于碎片化视觉输入，破坏了视觉内容的语义完整性。为解决这一问题，本文提出了一种新颖的动态语义等效视觉tokenizer（SeTok），它通过动态聚类算法将视觉特征组织成语义单元，根据图像复杂性灵活决定token的数量。这种生成的视觉tokens能有效保持语义完整性，同时捕捉低频和高频视觉特征。  ### 任务  我们提出了一种名为Setokim的新型MLLM，它结合了SeTok。实验结果表明，Setokim在各种任务上表现出显著的优势。关于更多详情，可以访问项目网页：https://chocowu.github.io/SeTok-web/。|
|**2024-06-07**|**LINX: A Language Driven Generative System for Goal-Oriented Automated Data Exploration**|Tavor Lipman et.al.|[2406.05107](http://arxiv.org/abs/2406.05107)|null|## 翻译  数据探索是一个复杂的过程，用户通过逐步执行一系列查询来审视数据集。有时，用户会探索新数据以熟悉它，但更多时候，探索过程是围绕特定分析目标或问题进行的。为了帮助用户有效探索，已提出自动化数据探索（Automated Data Exploration，ADE）系统，它们旨在自动生成展示数据有趣特性的完整探索流程。然而，现有的ADE系统常受限于预定义的优化函数，导致对同一数据集始终产生相同的探索序列，这在有明确目标的探索中显得不足。为此，本文提出LINX，一个结合自然语言接口的生成式系统，专注于面向目标的数据探索。  LINX接受输入数据集和用自然语言描述的分析目标，生成与用户需求相关的个性化探索会话。系统利用大型语言模型解析输入的分析目标，并据此生成期望输出探索会话的规范。这些规范随后被传递给基于约束深度强化学习（Constrained Deep Reinforcement Learning，CDRL）的新型模块化ADE引擎，使其能根据指定指令调整输出。为了验证LINX的效果，我们创建了一个新的面向目标探索的基准数据集，并进行了深入的用户研究。实验结果表明，LINX生成的探索笔记本在相关性和实用性上显著优于现有解决方案，包括ChatGPT、无目标导向的ADE以及商业系统。|
|**2024-06-07**|**Multi-Head RAG: Solving Multi-Aspect Problems with LLMs**|Maciej Besta et.al.|[2406.05085](http://arxiv.org/abs/2406.05085)|**[link](https://github.com/spcl/mrag)**|**## 背景  **增强型检索生成（Retrieval Augmented Generation, RAG）**通过将文档内容融入大语言模型（Large Language Models, LLMs）的上下文中，提高了其响应的准确性和相关性。然而，现有的RAG方法并未充分处理那些可能需要检索包含不同内容的多文档查询。这类问题在现实中很常见，但挑战在于，这些文档的嵌入在向量空间中可能相距较远，难以一次性获取。本文提出了一种新的方案——**多头检索增强生成（Multi-Head RAG, MRAG）**，它以一种简单而强大的方式解决这个问题：利用Transformer的多头注意力层的激活作为检索键，而非解码层。这个想法的驱动力在于，不同的注意力头能够学习捕捉数据的不同方面。通过利用这些激活，我们得到的嵌入能代表数据项和查询的多种特性，从而提升复杂查询的检索精度。  **贡献**  我们提供了评估方法、度量标准、合成数据集以及实际应用案例，来展示MRAG的有效性。与标准RAG基线相比，MRAG在相关性方面的提升可高达20%。MRAG可以无缝融入现有的RAG框架，如RAGAS，以及各类数据存储系统。  总结，本文旨在改进现有RAG模型，以更好地处理涉及多角度信息检索的复杂查询任务。**|
|**2024-06-07**|**Are Large Language Models More Empathetic than Humans?**|Anuradha Welivita et.al.|[2406.05063](http://arxiv.org/abs/2406.05063)|null|随着大型语言模型（LLMs）的兴起，研究它们是否能在情感识别和共情回应方面超越人类已成为研究焦点。本论文开展了一项深入研究，对比了包括GPT-4、LLaMA-2-70B-Chat、Gemini-1.0-Pro和Mixtral-8x7B-Instruct在内的四款最先进的LLMs与人类在共情回应能力上的表现。我们通过一项涉及1,000名参与者的双盲用户研究，对2,000个精心挑选的情感对话提示进行了分析，这些提示涵盖了32种不同正负情绪的广泛范围。研究结果显示，LLMs的共情回应能力在统计学上优于人类。GPT-4表现出最强烈的共情，其“好”等级别的回复比人类基准提高了约31%。紧随其后的是LLaMA-2，提升了约24%，Mixtral-8x7B提升了约21%，Gemini-Pro提升了约10%。我们还对回复评级进行了更详细的分析，发现某些LLMs在回应特定情绪方面明显优于其他模型。提出的评估框架提供了一种可扩展且适应性强的方法，用于评估新LLMs的共情能力，避免了未来研究重复这项研究的必要性。|
|**2024-06-07**|**Robustness Assessment of Mathematical Reasoning in the Presence of Missing and Contradictory Conditions**|Shi-Yu Tian et.al.|[2406.05055](http://arxiv.org/abs/2406.05055)|null|大型语言模型在推理任务上表现出色，通过少量示例提示可以进一步提升性能。然而，当前的评估主要集中在精心构建的基准上，忽视了现实世界中存在缺失和矛盾条件的推理问题，即所谓的不明确问题。我们的观察表明，现有的少量提示方法在这种情况下效果不佳，往往给出过度自信的答案或错误推断。为了深入研究这个问题，我们创建了一个名为“带有缺失和矛盾条件的问题”（PMC）的基准，并引入了两个新指标来评估少量提示方法在处理这类问题时的表现。使用PMC基准的分析揭示了在解决明确问题的数学推理性能与识别不明确问题能力之间存在权衡。针对PMC带来的挑战，我们提出了一种新颖的少量提示方法，称为SMT-LIB提示（SLP）。这种方法利用SMT-LIB语言描述问题，而不是直接求解，然后采用双重检查求解策略验证解决方案的满足性和唯一性，从而提供最终反馈。实验结果全面展示了我们的SLP方法在处理带有缺失和矛盾条件的问题时，相较于现有方法具有显著优势。我们将开源我们的基准和代码，以促进未来的研究。|
|**2024-06-07**|**Hints-In-Browser: Benchmarking Language Models for Programming Feedback Generation**|Nachiket Kotalwar et.al.|[2406.05053](http://arxiv.org/abs/2406.05053)|null|### 概述  生成式人工智能和大型语言模型在编程教育中的潜力巨大，它们能够为学习者提供个性化的反馈和提示。当前的研究主要集中在提升生成反馈的质量，以达到人类导师的水平。然而，在实际教育部署中，除了质量外，成本、时间及数据隐私也是关键考量因素。本论文旨在对语言模型在编程反馈生成方面的性能进行全面评估，包括质量、成本、速度和数据隐私等多个维度。我们特别关注利用最新的在浏览器内推理技术，这有助于直接降低成本并保护数据隐私。  为了优化适合浏览器内运行的小型模型的反馈质量，我们开发了一种基于GPT-4生成的合成数据的微调流程。我们将展示如何使用WebLLM的浏览器内推理引擎来优化Llama3-8B和Phi3-3.8B的4位量化模型在三个不同Python编程数据集上的效果。我们承诺会公开全部实现、web应用和数据集，以促进在浏览器语言模型领域的进一步研究。|
|**2024-06-07**|**Bootstrapping Referring Multi-Object Tracking**|Yani Zhang et.al.|[2406.05039](http://arxiv.org/abs/2406.05039)|**[link](https://github.com/zyn213/temprmot)**|## 背景 当前的多对象引用跟踪（RMOT）任务通常依赖于手动标注的数据集和静态规则，这限制了多样性和实施范围。为了解决这个问题，我们的研究主要关注通过引入更多区分性语言词汇来推动RMOT任务的发展。为此，我们首先对Refer-KITTI数据集进行了扩展，创建了Refer-KITTI-V2，它从最初的2,719个手动标注开始，解决了类别不平衡问题，并增加了更多关键词，使其更贴近现实场景，相较于Refer-KITTI有所进步。我们进一步利用大型语言模型扩充这些标注，总计达到9,758个，生成了617个不同的词汇，超越了先前的RMOT基准。  此外，我们还改进了RMOT的端到端框架，采用了一个简单而优雅的时序推进策略，该策略在性能上优于先前的方法。相关源代码和数据集已可在<https://github.com/zyn213/TempRMOT>获取。|
|**2024-06-07**|**Scenarios and Approaches for Situated Natural Language Explanations**|Pengshuo Qiu et.al.|[2406.05035](http://arxiv.org/abs/2406.05035)|null|大型语言模型（LLMs）能够生成适应不同用户情境的自然语言解释（NLE）。然而，对于这种适应性的量化评估尚存空白。为此，我们创建了一个基准数据集——基于情境的解释（Situation-Based Explanation，SBE）数据集，包含100个需要解释的事物（explanandum）。每个事物都配对了针对教师、学生和专业人士等不同受众群体的解释，以便评估模型在满足这些多元化群体信息需求和背景下的解释精准度，如学生、教师和家长。每种“事例-受众”组合都附有人类撰写的参考解释，用于计算分数，以量化模型如何根据情境调整解释。我们在不同规模的预训练语言模型上测试了三种提示方法：规则基础提示、元提示和上下文学习提示。研究发现：1）模型可以通过生成提示产生更精确地符合目标情境的解释；2）明确提示“你是一个有用的助手”并非针对情境化NLE任务的必要技术；3）上下文学习提示仅能帮助模型学习演示模板，但无助于提升其推理性能。SBE数据集和我们的分析为今后生成适应情境的自然语言解释的研究提供了基础。|
|**2024-06-06**|**Verbalized Machine Learning: Revisiting Machine Learning with Language Models**|Tim Z. Xiao et.al.|[2406.04344](http://arxiv.org/abs/2406.04344)|null|受大型语言模型（LLMs）取得的巨大进展启发，我们提出了口头化机器学习（VML）框架。与传统的机器学习模型，通常在连续参数空间中优化不同，VML将参数空间限制为人可理解的自然语言。这种约束促使我们从新角度看待函数逼近问题，即将带有文本提示的LLM视为由文本提示参数化的函数。我们借此视角重新审视了经典机器学习任务，如回归和分类，发现这些问题可以通过LLM参数化的学习器和优化器来解决。VML的主要优势包括：（1）易于编码先验知识：关于问题和假设类的先验知识可以以自然语言形式编码并输入给LLM参数化的学习器；（2）自动模型选择：优化器可以根据数据和口头化先验知识自动选择具体的模型类别，并在训练过程中更新模型类别；（3）可解释的学习者更新：LLM参数化的优化器可以解释每次学习者更新的原因。我们进行了多项实验评估VML的有效性，希望它能成为增强机器学习可解释性和信任度的桥梁。|
|**2024-06-06**|**RoboMamba: Multimodal State Space Model for Efficient Robot Reasoning and Manipulation**|Jiaming Liu et.al.|[2406.04339](http://arxiv.org/abs/2406.04339)|null|在机器人操作的核心目标中，让模型理解视觉场景并执行动作是一个基本任务。尽管现有的机器人多模态大型语言模型（MLLM）能够处理一些基础任务，但它们在两个方面仍面临挑战：1）处理复杂任务的推理能力不足；2）对于MLLM的微调和推理存在高计算成本。近期提出的基于状态空间模型（SSM）的Mamba展示了在非平凡序列建模方面的潜力，具有线性推理复杂度。在此启发下，我们开发了RoboMamba，一个端到端的机器人MLLM，它利用Mamba模型结合机器人推理和动作能力，同时保持高效的微调和推理效率。  首先，我们将视觉编码器与Mamba集成，通过联合训练使视觉数据与语言嵌入对齐，赋予模型视觉常识和与机器人相关的推理能力。为了进一步提升RoboMamba的动作姿态预测能力，我们探索了一种高效的微调策略，仅使用简单的策略头。实验表明，一旦RoboMamba具备足够的推理能力，只需极少的微调参数（模型的0.1%）和时间（20分钟），就能习得操纵技能。在实验中，RoboMamba在通用和机器人评估基准上展现出卓越的推理能力。同时，我们的模型在模拟和真实世界实验中实现了姿态预测的出色表现，其推理速度比现有机器人MLLM快7倍。项目的网页链接为：<https://sites.google.com/view/robomamba-web>。|
|**2024-06-06**|**Coherent Zero-Shot Visual Instruction Generation**|Quynh Phung et.al.|[2406.04337](http://arxiv.org/abs/2406.04337)|null|尽管文本到图像合成技术取得了进步，特别是在扩散模型方面，但生成需要物体在连续步骤中保持一致表示和平滑状态转换的视觉指令仍然是一项艰巨挑战。本文提出了一种无需训练的框架，巧妙地结合了文本理解与图像生成，以确保视觉指令既美观又具有连贯性和准确性。通过测试多步骤指令，并与多个基线进行比较，我们验证了这种方法的有效性。实验结果显示，我们的方法能够生成连贯且视觉上吸引人的指令。|
|**2024-06-06**|**DeepStack: Deeply Stacking Visual Tokens is Surprisingly Simple and Effective for LMMs**|Lingchen Meng et.al.|[2406.04334](http://arxiv.org/abs/2406.04334)|null|大多数大型多模态模型（LMMs）通过将视觉令牌作为序列输入到大型语言模型（LLMs）的第一层来实现。这种方法虽然直观，但会显著增加计算和内存开销，因为模型需要处理更多的输入层令牌。本文提出了一种新的架构DeepStack，用于LMMs。在LMM的视觉和语言Transformer的N层中，我们将视觉令牌分为N组，并从底层逐层向上馈送到对应的Transformer层。令人惊讶的是，这种简单的方法极大地增强了LMM在跨层视觉令牌交互方面的建模能力，同时成本几乎不变。我们分别将DeepStack应用于LMM的语言和视觉Transformer，并通过广泛实证结果验证了DeepStack LMM的有效性。  使用相同的上下文长度，我们的DeepStack 7B和13B参数模型在9个基准测试上平均超越同类模型2.7分和2.9分。仅使用五分之一的上下文长度，DeepStack的表现接近于使用完整上下文长度的模型。这些提升在高分辨率任务中尤为明显，例如，与LLaVA-1.5-7B相比，TextVQA、DocVQA和InfoVQA上的性能分别提高了4.2分、11.0分和4.0分。此外，我们还将DeepStack应用到视觉Transformer层，这带来了与LLaVA-1.5-7B相当的平均改进，为3.8分。|
|**2024-06-06**|**PaCE: Parsimonious Concept Engineering for Large Language Models**|Jinqi Luo et.al.|[2406.04331](http://arxiv.org/abs/2406.04331)|**[link](https://github.com/peterljq/parsimonious-concept-engineering)**|**大型语言模型（LLMs）被广泛应用于各种任务，尽管它们能够生成类似人类的回复，但也会产生不良输出，如潜在有害信息、种族或性别歧视性言论以及错误的信息。为了减少这些问题，研究人员开发了对齐方法，如微调、提示工程和表示工程。然而，现有方法面临挑战：一些需要针对每个对齐任务进行昂贵的微调；一些未能充分消除不良概念，对齐效果不佳；一些则删除了良性的概念，降低了LLMs的语言能力。为此，我们提出了名为Parsimonious Concept Engineering（PaCE）的新型激活工程框架，旨在解决这些问题。  首先，我们构建了一个大规模的概念字典，它在激活空间中表示每个原子对应一个语义概念。接着，对于给定的任何对齐任务，我们会使用一个概念分区器高效地标记这些概念为良性或不良。在推理阶段，我们利用稀疏编码方法，根据概念字典分解LLM的激活，将其准确表示为良性成分和不良成分的线性组合。通过移除不良成分，我们能够调整LLMs的行为以符合对齐目标。  我们在回应净化、真实性增强和情感修订等任务上进行了实验，并发现PaCE在实现对齐性能的同时，保持了良好的语言能力，达到了当前最先进的水平。**|
|**2024-06-06**|**Step-aware Preference Optimization: Aligning Preference with Denoising Performance at Each Step**|Zhanhao Liang et.al.|[2406.04314](http://arxiv.org/abs/2406.04314)|null|## 背景  近期，Direct Preference Optimization (DPO) 已成功扩展到调整文本到图像的扩散模型，使其与人类偏好保持一致。不同于大多数现有 DPO 方法假设所有扩散步骤都与最终生成图像保持一致的偏好顺序，我们认为这种假设忽略了每个步骤特有的去噪性能，因此应该为每一步定制偏好标签。为此，我们提出了一种新颖的后训练方法——Step-aware Preference Optimization (SPO)，它独立评估并调整每个步骤的去噪性能，利用步级感知偏好模型和步级重采样器来确保准确的步级监督。  在SPO中，我们在每个去噪步骤中会创建一个图像池，寻找合适的胜者-败者对，并且关键在于，我们会从池中随机选择一个图像作为下一次去噪步骤的起点。这个步级重采样过程保证了每次胜者-败者对都来自同一原始图像，使得比较独立于前一步。为了评估每个步骤的偏好，我们训练了一个专门的步级感知偏好模型，适用于模糊和清晰的图像。在Stable Diffusion v1.5和SDXL等实验中，SPO 显著优于最新的Diffusion-DPO，尤其是在处理复杂、详细的提示时，能更好地生成图像并提升美学效果，同时在训练效率上超过20倍。代码和模型可在此链接获取：[https://rockeycoss.github.io/spo.github.io/](https://rockeycoss.github.io/spo.github.io/)。|
|**2024-06-06**|**Semantically Diverse Language Generation for Uncertainty Estimation in Language Models**|Lukas Aichberger et.al.|[2406.04306](http://arxiv.org/abs/2406.04306)|**[link](https://github.com/ml-jku/SDLG)**|**大型语言模型（LLMs）在生成文本时可能会出现幻觉，这阻碍了社会和工业中的各种应用，因为它们会降低LLMs的可信度。当前的LLMs采用自回归方式生成文本，即预测并添加文本标记。当LLMs对生成的下一个标记的语义含义不确定时，很可能会产生幻觉。因此，人们认为幻觉源于预测不确定性。我们提出了“语义多样性语言生成”（Semantically Diverse Language Generation，SDLG），用于量化LLMs的预测不确定性。SDLG引导LLM生成语义多样但又合理的初始文本替代方案，从而提供了精确的aleatoric语义不确定性测量，能够检测初始文本是否可能出现幻觉。  实验在问答任务上表明，SDLG始终优于现有方法，并且在计算效率上最为高效，为LLMs的不确定性估计设定了新的标准。**|
|**2024-06-06**|**Text-to-Drive: Diverse Driving Behavior Synthesis via Large Language Models**|Phat Nguyen et.al.|[2406.04300](http://arxiv.org/abs/2406.04300)|null|在模拟训练和评估关键安全系统，如自动驾驶车辆时，通过模拟生成各种场景至关重要。然而，模型其他车辆的轨迹以模拟复杂且有意义的近距离交互任务成本高昂。利用语言描述来生成驾驶行为是一种有前景的方法，它提供了一种可扩展且直观的人类操作方式，能够模拟广泛驾驶互动。但大型标注的语言-轨迹数据稀缺是这一方法面临的挑战。为此，我们提出了Text-to-Drive（T2D），这是一种利用大型语言模型（LLMs）合成多样化驾驶行为的技术。我们的方法采用知识驱动两阶段策略：首先，利用LLMs的内置知识生成丰富多样的驾驶行为语言描述；接着，利用其推理能力在模拟器中实现这些行为。T2D的核心是使用LLM构建状态图，将低级状态映射到高级抽象，从而简化了诸如总结低级观测、评估策略与行为描述的一致性以及设计辅助奖励等下游任务，无需人工监督。通过我们的知识驱动方法，我们证明T2D能生成比其他基准更丰富的轨迹，并提供一个自然语言界面，允许用户交互式地融入人类偏好。更多示例请访问我们的网站：<https://text-to-drive.github.io/>|
|**2024-06-07**|**What Languages are Easy to Language-Model? A Perspective from Learning Probabilistic Regular Languages**|Nadav Borenstein et.al.|[2406.04289](http://arxiv.org/abs/2406.04289)|null|## 背景  大型语言模型能够学习什么？根据定义，语言模型（LM）是字符串的分布。因此，可以将这个问题转化为评估字符串分布类的学习能力。尽管先前的研究主要关注理论限制，但我们关注的是实际可学习性。不同于以往的实证工作，我们评估神经语言模型在其“主场”——学习概率语言——上的表现，而不是作为形式语言的分类器。具体来说，我们研究递归语言模型（RLM）由循环神经网络（RNN）和Transformer LM学习的可行性。我们通过实验测试RLM的可学习性，考察其与RLM的复杂参数以及神经LM隐藏层大小的关系。实验结果显示，RLM的秩（对应于其条件分布对数似然线性空间的大小）和采样字符串的预期长度是RNN和Transformer LM可学习性的强且显著预测因素。其他一些预测指标也达到了显著性，但RNN和Transformer之间存在不同的模式。|
|**2024-06-06**|**Characterizing Similarities and Divergences in Conversational Tones in Humans and LLMs by Sampling with People**|Dun-Ming Huang et.al.|[2406.04278](http://arxiv.org/abs/2406.04278)|**[link](https://github.com/jacobyn/SamplingTonesACL)**|**## 翻译后的中文摘要  对话语气在人际交流中至关重要。随着大型语言模型（LLMs）的日益普及，研究它们与人类交流语气的差异变得尤为重要。然而，当前关于对话模式的研究往往依赖于预先存在的分类体系或文本语料库，这些可能存在实验者偏见，并可能无法充分反映研究领域中的真实世界分布。受认知科学方法的启发，我们提出一种迭代方法，通过交替进行两项任务来同时揭示语气和句子：（1）参与者判断给定句子的语气，（2）另一参与者根据该语气生成句子。我们在人类参与者和GPT-4之间进行了100轮这样的互动，从而获得了一组包含句子和常见对话语气的数据。我们还进行了额外实验，让人类和GPT-4对所有句子标注所有语气。基于1,339名人类参与者、33,370次人类评价以及29,900个GPT-4查询的数据，我们展示了如何使用这种方法创建一个可解释的几何表示，以展示人类和GPT-4之间的对话语气关系。这项工作展示了机器学习和认知科学理念如何结合，以解决人机交互中的挑战。**|
|**2024-06-05**|**Wings: Learning Multimodal LLMs without Text-only Forgetting**|Yi-Kai Zhang et.al.|[2406.03496](http://arxiv.org/abs/2406.03496)|null|## 任务  多模态大型语言模型（MLLMs）起源于预训练的通用语言模型，首先将图像与文本对齐，然后在混合模态输入上进行微调。然而，MLLM在处理仅包含文本的指令时会出现灾难性的遗忘，这些文本指令并未包含图像，这些问题在初始的语言模型阶段就已经存在。本文提出Wings，一个新型的MLLM，它在文本对话和多模态理解方面表现出色。通过分析MLLM在多模态指令中的注意力，我们发现文本遗忘与从图像前向图像后的注意力转移有关。因此，我们构建了额外模块作为增强学习器，以补偿这种注意力转移。视觉和文本学习器作为“翅膀”式的补充，平行连接在每个注意力块内，起初图像和文本输入由视觉学习器与主注意力协同工作，平衡对视觉元素的关注。随后，文本学习器通过注意力路由的方式与视觉学习器的输出协作整合。我们设计了低秩残差注意力（LoRRA）机制以保证学习器的高效运行。  实验结果表明，Wings在文本对话和视觉问答任务上优于同等规模的MLLM。在我们新构建的交错图像-文本（IIT）基准测试中，Wings在从文本为主到多模态为主的问答任务中展现出卓越性能。|
|**2024-06-06**|**Seq1F1B: Efficient Sequence-Level Pipeline Parallelism for Large Language Model Training**|Ao Sun et.al.|[2406.03488](http://arxiv.org/abs/2406.03488)|**[link](https://github.com/maydomine/seq1f1b)**|大型语言模型（LLMs）的兴起在很大程度上依赖于分布式训练策略，其中管道并行性起着关键作用。随着LLMs的训练序列长度扩展到32k甚至128k，当前的管道并行方法面临严重瓶颈，如高内存占用和显著的管道延迟，这极大地限制了模型的可扩展性和训练吞吐量。为了提高内存效率和训练效率，我们提出了一种针对长序列训练LLMs的高效序列级一次前向一次后向（1F1B）管道调度方法，称为Seq1F1B。Seq1F1B将批级别可调度单元分解为更细的序列级单元，从而减小延迟并降低内存需求。  考虑到如果均匀分割序列，Seq1F1B可能会产生轻微的额外延迟，我们设计了一种基于计算的策略来划分输入序列，以缓解这个副作用。与竞争性的管道基线方法，如Megatron的1F1B管道并行相比，我们的方法在保持更高训练吞吐量的同时，内存占用更低。值得注意的是，Seq1F1B能够在不使用重新计算策略的情况下，有效地在64个NVIDIA A100 GPU上训练一个具有300亿参数的LLM，处理长达64k的序列，这是现有方法无法实现的。我们的代码基于Megatron-LM，并已开源：https://github.com/MayDomine/Seq1F1B.git。|
|**2024-06-05**|**Analyzing LLM Behavior in Dialogue Summarization: Unveiling Circumstantial Hallucination Trends**|Sanjana Ramprasad et.al.|[2406.03487](http://arxiv.org/abs/2406.03487)|null|### 翻译  近期的大型语言模型（LLMs）的进步显著提升了摘要生成系统的性能，但它们在真实性方面的问题引起了关注。尽管之前的研究广泛评估了新闻领域的LLMs，对话摘要的评价主要集中在基于BART的模型上，这在我们理解它们的可信度方面留下了空白。本研究旨在评估LLMs在对话摘要中的真实性，通过人类标注，并着重于识别和分类句级不一致。我们特别关注GPT-4和Alpaca-13B这两款主流模型。我们的评估揭示了错误定义的微妙之处：LLMs常常生成看似合理的推断，这些推断依赖于对话中的间接证据，而缺乏直接证据，这在旧模型中较少见。我们提出了一种改进的错误分类体系，引入了“情境推理”类别来归类这些LLM行为，并公开了相关数据集。利用我们的分类体系，我们比较了LLMs与老式微调模型之间的行为差异。此外，我们系统地评估了自动错误检测方法在LLM摘要上的效果，发现它们在识别这类细微错误时表现不佳。为此，我们提出了两种基于提示的精细错误检测方法，这两种方法优于现有指标，特别是在识别“情境推理”错误时。|
|**2024-06-05**|**BIPED: Pedagogically Informed Tutoring System for ESL Education**|Soonwoo Kwon et.al.|[2406.03486](http://arxiv.org/abs/2406.03486)|null|大型语言模型（LLMs）显示出巨大的潜力，能够作为经济且易于获取的英语第二语言（L2）学习者对话式智能辅导系统（CITS）。然而，现有的CITS往往只能教授简单概念，或者在教学深度上无法满足不同学习策略的需求。为了开发一个更具教育学导向、能教授复杂概念的CITS，我们构建了一个双语教育指导对话数据集（BIPED），包含一对一的人类英语辅导互动。通过对辅导对话的后处理分析，我们提炼出一套包含34种教师行为和9种学生行为的对话动作词典，并将其用于进一步标注收集的数据。根据先预测合适的教师行为再生成相应回复的两步框架，我们利用GPT-4和SOLAR-KO分别实现了两个CITS模型。实验结果表明，这些实施的模型不仅模仿了人类教师的风格，还运用了丰富且与上下文相适应的教学策略。|
|**2024-06-05**|**Does your data spark joy? Performance gains from domain upsampling at the end of training**|Cody Blakeney et.al.|[2406.03476](http://arxiv.org/abs/2406.03476)|null|随着大型语言模型（LLMs）的预训练数据集规模增长到万亿级别的tokens，这些数据集主要由大规模的CommonCrawl网络爬虫内容以及较小的领域特定数据组成。由于在大计算量（FLOPs）下训练以揭示模型在困难和新兴基准上的显著变化成本高昂，如何在通用网络抓取的多样性和领域特定信息密度之间找到最优平衡成为一个问题。本文展示了如何利用这些较小的领域特定数据，在训练后期对其进行上采样，从而在诸如MMLU、GSM8K和HumanEval等基准上提升性能。对于一个训练了1万亿（T）令牌的70亿参数模型，这种简单方法可使其性能提高6.90分、8.26分和6.17分，与训练时间两倍的Llama-2（7B）模型相当。我们研究了在训练后期领域上采样的持续时间，从5%到30%，发现10%到20%的比例最为合适，以平衡一般语言建模能力与特定任务的优化。此外，我们还利用领域上采样来大规模分析单个数据集对不同基准的增益，通过在这一阶段移除它们进行实验。这种方法极大地降低了实验成本，使得能够以预训练运行的十分之一左右的成本探索不同预训练数据集的影响。|
|**2024-06-05**|**AD-H: Autonomous Driving with Hierarchical Agents**|Zaibin Zhang et.al.|[2406.03474](http://arxiv.org/abs/2406.03474)|null|鉴于多模态大语言模型（MLLM）的强大功能，近期的研究聚焦于使用MLLM驱动的自动驾驶系统在大规模动态环境中。然而，常见的方法直接将高级指令转化为低级车辆控制信号，这违背了MLLM的本质生成模式，未能充分利用其潜在能力。因此，这些方法的一般化能力受到训练数据集的极大限制。为解决这个问题，我们提出通过中层语言驱动命令来连接高级指令和低级控制信号，它们比高级指令更细致，但比控制信号更通用且可解释，从而有效弥合两者之间的鸿沟。我们通过一个名为AD-H的分层多代理驾驶系统实现这一理念，包括一个用于高层推理的MLLM规划器和一个轻量级控制器进行低层执行。这种分层设计使MLLM摆脱了低级控制信号解码，充分释放了其在高层感知、推理和规划方面的涌现能力。  我们构建了一个带有动作层次注释的新数据集。全面的闭环评估显示，我们的AD-H系统具有多项关键优势。首先，AD-H在驾驶性能上显著优于现有方法，甚至展现出在车辆操作过程中自我纠正的能力，这是训练数据未涵盖的场景。其次，AD-H在长程指令和新环境条件下表现出色，明显超越当前最先进的方法。我们将公开我们的数据和代码，可通过<https://github.com/zhangzaibin/AD-H>获取。|
|**2024-06-05**|**What is the Best Way for ChatGPT to Translate Poetry?**|Shanshan Wang et.al.|[2406.03450](http://arxiv.org/abs/2406.03450)|null|本文研究了大型语言模型如ChatGPT在英语-中文诗歌翻译任务中的性能，通过定向提示和小样本场景分析以优化其表现。尽管初期结果令人鼓舞，但研究发现ChatGPT的翻译存在持续问题。为此，我们提出了“解释辅助诗歌机器翻译”（EAPMT）方法，该方法利用诗歌的单语解释作为翻译过程的指导。同时，我们改进了现有的评估标准，以更好地适应现代诗歌翻译的微妙之处。我们邀请专业诗人进行评估，并结合GPT-4的评价，结果显示，我们的EAPMT方法在与传统ChatGPT翻译方法以及现有在线系统的比较中表现出色。论文验证了我们方法的有效性，并为文学翻译的机器辅助提供了新颖视角。|
|**2024-06-05**|**Pre-trained Large Language Models Use Fourier Features to Compute Addition**|Tianyi Zhou et.al.|[2406.03445](http://arxiv.org/abs/2406.03445)|null|## 翻译  预训练的大型语言模型（LLMs）在数学推理方面表现出色，但它们如何执行基本的算术运算，如加法，仍不清楚。本文揭示了预训练的LLMs通过傅里叶特征进行加法——这些是隐藏状态中的维度，通过一组在频域中稀疏分布的特征来表示数字。在模型中，多层感知器（MLP）层和注意力层以互补的方式使用傅里叶特征：MLP层主要使用低频特征近似答案的大小，而注意力层主要通过高频特征执行模运算（例如判断答案是否为偶数）。预训练对于这种机制至关重要：从头开始训练的模型仅利用低频特征，导致准确性较低。将预训练的词嵌入引入到随机初始化的模型中可以恢复其性能。总的来说，我们的分析表明，适当的预训练表示（如傅里叶特征）能够解锁Transformer学习算法任务精确机制的能力。|
|**2024-06-05**|**Cycles of Thought: Measuring LLM Confidence through Stable Explanations**|Evan Becker et.al.|[2406.03441](http://arxiv.org/abs/2406.03441)|null|在许多高风险的机器学习应用中，模型需要能够表明其对预测的不确定性至关重要。尽管大型语言模型（LLMs）在各种基准上的准确度可达到甚至超过人类水平，但它们对错误响应的过度自信仍是已知的问题。传统的方法在直接应用于LLMs时可能面临计算成本和封闭源模型的挑战。近期提出了一些黑盒方法，但它们往往依赖于诸如自我表述的信心等启发式。我们提出了一种框架，通过分析模型生成答案的解释分布来衡量LLMs的不确定性。尽管利用解释本身并非新颖，但我们将其视为测试时间分类器，通过计算最可能的分类器后验答案分布，以此进行不确定性评估。  我们展示了使用解释蕴含作为分类器似然性的一种特定框架实例，如何在五个不同的数据集上改进了信心分数指标（特别是AUROC和AURC）。我们的结果表明，该框架既具有理论依据，又是有效量化LLMs不确定性的方式。|
|**2024-06-05**|**Interactive Text-to-Image Retrieval with Large Language Models: A Plug-and-Play Approach**|Saehyung Lee et.al.|[2406.03411](http://arxiv.org/abs/2406.03411)|**[link](https://github.com/saehyung-lee/plugir)**|**该论文主要关注的是交互式文本到图像检索任务中的对话形式上下文查询问题。我们的方法论，名为PlugIR，通过两种方式有效地利用大型语言模型（LLMs）的一般指令跟随能力。首先，通过重述对话形式的上下文，我们消除了在现有视觉对话数据上微调检索模型的需求，从而能够使用任意黑盒模型。其次，我们设计了一个LLM提问者，根据当前上下文中候选图像的信息，生成关于目标图像属性的非冗余问题。这种方法减少了生成问题的噪声和冗余。除了我们的方法，我们还提出了一种新的评估指标，称为最佳对数排名积分（BRI），以全面评估交互式检索系统。PlugIR在多个基准测试中表现出优于零次设置和 Fine-tuned 基准的性能。此外， PlugIR 的两个组成部分可以根据不同情况灵活单独或结合应用。我们的代码已开源在：https://github.com/Saehyung-Lee/PlugIR。**|
|**2024-06-04**|**Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks**|Tianyu He et.al.|[2406.02550](http://arxiv.org/abs/2406.02550)|**[link](https://github.com/ablghtianyi/ICL_Modular_Arithmetic)**|**这篇工作研究了大型语言模型在一组模块化算术任务中出现的上下文学习和技能组合现象。我们关注的是有限数量的一次性模运算函数 $z = a \times x + b \times y \;(\text{mod}\; p)$，这些函数由向量 $(a, b) \in \mathbb{Z}_p^2$ 标记。部分任务被用作预训练，其余用于分布外测试。实验表明，GPT风格的Transformer随着预训练任务数量增加，其在分布内和分布外的泛化能力会经历转变。最小型能实现分布外泛化的模型需要两个Transformer块；而对于更深的模型，分布外泛化阶段是“瞬态”的，需要早期停止。最后，我们对预训练模型进行了可解释性分析，揭示了两种阶段中高度结构化的表示，并讨论了学习到的算法。**|
|**2024-06-04**|**Leveraging Visual Tokens for Extended Text Contexts in Multi-Modal Learning**|Alex Jinpeng Wang et.al.|[2406.02547](http://arxiv.org/abs/2406.02547)|**[link](https://github.com/showlab/VisInContext)**|**这段研究并未介绍最先进的多模态大语言模型（MLLM），而是提出了一种创新方法，旨在有效提升长序列在多模态模型中的处理。我们提出了“Visualized In-Context Text Processing”（VisInContext）技术，通过视觉令牌来处理长文本，从而显著降低GPU内存使用和浮点运算（FLOPs）在训练和推理阶段的需求。例如，对于一个560亿参数的混合 Experts（MOE）模型，我们的方法将预训练中的上下文文本长度扩展到了2048个tokens，而计算量几乎保持不变。实验结果显示，使用VisInContext训练的模型在常见的基于实例的少量数据评估下游任务中表现出色。此外，VisInContext与现有技术相结合，能增强对文档的理解能力，特别适用于文档问答和连续文档检索，显示出巨大的潜力。**|
|**2024-06-04**|**To Believe or Not to Believe Your LLM**|Yasin Abbasi Yadkori et.al.|[2406.02543](http://arxiv.org/abs/2406.02543)|null|我们研究大型语言模型（LLMs）中的不确定性量化，目标是识别对给定查询的响应时的不确定性程度。我们同时考虑了两种类型的不确定性：一种是知识性不确定性（例如对事实或语言真理的未知），另一种是不可消除的随机性（如可能的答案多样性）。特别是，我们提出了一种信息论指标，能够可靠地区分出只有知识性不确定性较大的情况，这时模型的输出是不可靠的。这个条件仅依赖于通过特殊迭代提示基于先前响应得到的模型输出来计算。这种量化方法可以检测单答和多答情况下是否存在虚构（即知识性不确定性高）的情况，这与许多标准的不确定性量化策略（如以响应的对数似然性作为阈值）不同，后者无法识别多答情况下的虚构。  我们进行了一系列实验，展示了我们的方法的优势。此外，我们的研究还揭示了LLM如何通过迭代提示放大对给定输出的概率分配，这可能具有独立的兴趣价值。|
|**2024-06-04**|**Loki: Low-Rank Keys for Efficient Sparse Attention**|Prajwal Singhania et.al.|[2406.02542](http://arxiv.org/abs/2406.02542)|null|针对大型语言模型的推理计算成本高昂，特别是当使用长序列时，自注意力机制是主要开销。为了解决这个问题，近期的研究提出了一些稀疏注意力近似方法。本文中，我们通过分析发现，注意力块中的键向量实际上处于一个远低于原始维度的空间。这一观察促使我们提出Loki，一种新的稀疏注意力方法。Loki根据在低维空间计算的注意力得分，对KV缓存中的令牌进行排序和选择。实验结果表明，Loki能够比其他流行近似方法更好地保持模型的效能，同时由于减少了数据移动（加载/存储）和计算成本，加速了注意力计算。|
|**2024-06-04**|**Parrot: Multilingual Visual Instruction Tuning**|Hai-Long Sun et.al.|[2406.02539](http://arxiv.org/abs/2406.02539)|null|随着GPT-4V等多模态大型语言模型的快速发展，人工智能朝着通用人工智能迈出了重要一步。当前的方法主要依赖于监督微调（SFT）来同步视觉编码器与语言模型，从而赋予它们多模态能力。然而，这种做法可能导致随着训练的进行，语言模型处理多种语言的能力逐渐减弱。我们发现，以英语为中心的不平衡SFT数据集会导致非英语语言性能显著下降，原因在于SFT过程中未能有效连接视觉编码器和多语言令牌。为此，我们提出Parrot，一种利用文本引导在语言层面驱动视觉令牌对齐的新方法。Parrot通过让视觉令牌根据不同的语言输入进行条件化，并借助混合专家（MoE）促进多语言令牌的对齐。特别是，为了增强非英语视觉令牌的对齐，我们计算初始视觉特征与文本嵌入之间的跨注意力，然后将其输入到MoE路由器，选择最相关的专家。选定的专家会将初始视觉令牌转化为特定语言的视觉令牌。鉴于目前缺乏评估多语言能力的标准基准，我们还创建并公开了一个大规模多语言多模态基准（MMMB），包括6种语言、15个类别和12,000个问题。Parrot不仅在MMMB和MMM Benchmark上展现出最先进的性能，还在广泛的多模态任务中表现出色。我们将提供Parrot的源代码和训练数据集供公众使用。|
|**2024-06-04**|**Mitigate Position Bias in Large Language Models via Scaling a Single Dimension**|Yijiong Yu et.al.|[2406.02536](http://arxiv.org/abs/2406.02536)|**[link](https://github.com/PositionalHidden/PositionalHidden)**|这篇论文主要探讨了大型语言模型（LLMs）在实际应用中的一个现象——位置偏见，也称为"迷失在中间"。这种偏见在长文本情境中尤为明显，即关键信息在提示中的不同位置会显著影响模型的准确性。研究发现，注意力权重是位置偏见的微观表现。此外，论文指出，因果注意力掩码通过创建位置特定的隐藏状态，也对位置偏见有所贡献。  基于这些洞察，作者提出了一种方法来减轻位置偏见，即调整这些位置特定的隐藏状态。实验在多个任务上进行，包括自然问题多文档问答、键值检索、LongBench和时间线重排，涉及RoPE模型、扩展上下文窗口模型和Alibi模型等多种架构。结果显示，我们的方法通过仅修改隐藏状态的一个维度，就能实现性能提升，最高可达15.2%。研究者还提供了代码供进一步使用，代码地址为：https://aka.ms/PositionalHidden。|
|**2024-06-04**|**SpecExec: Massively Parallel Speculative Decoding for Interactive LLM Inference on Consumer Devices**|Ruslan Svirschevski et.al.|[2406.02532](http://arxiv.org/abs/2406.02532)|**[link](https://github.com/yandex-research/specexec)**|随着大型语言模型的广泛应用，高效运行它们变得至关重要。近期的研究通过推测性解码实现了显著的速度提升。然而，大多数工作都是针对数据中心硬件进行设计。本研究反问：我们能在消费级设备上多快地运行LLMs？消费者级GPU已无法容纳最大的模型（500亿参数以上），因此需要将参数卸载到RAM或SSD。当使用卸载参数的方式运行时，推理引擎可以同时处理数百乃至数千个令牌的批次，使其非常适合推测性解码。我们提出SpecExec（推测性执行），这是一种简单的并行解码方法，适用于主流LLM家族，能生成每轮目标模型迭代高达20个令牌的预测。它利用现代LLMs中概率分布的高波动性和模型输出概率之间的高度一致性。SpecExec通过从草稿模型获取最可能的令牌延续，构建一个目标模型的“缓存”树，然后在一个单次遍历中验证。  使用SpecExec，我们在消费级GPU上实现了500亿参数LLM的推理，配合RAM卸载，4位量化下的速度达到4-6个令牌/秒，而16位权重下的速度为2-3个令牌/秒。|
|**2024-06-04**|**Scalable MatMul-free Language Modeling**|Rui-Jie Zhu et.al.|[2406.02528](http://arxiv.org/abs/2406.02528)|**[link](https://github.com/ridgerchu/matmulfreellm)**|**## 翻译  在大型语言模型（LLMs）中，矩阵乘法（MatMul）通常占据主要计算开销。随着LLMs的规模扩大，其嵌入维度和上下文长度也随之增加，这一问题更为显著。本文提出了一种方法，能够在保持强大性能的同时，完全移除LLMs中的MatMul操作，即使是在27亿参数量级的模型上也能实现。实验表明，我们的无MatMul模型在与内存消耗显著更多的状态-of-the-artTransformer相当的条件下表现出色。我们研究了模型的扩展性规律，并发现无MatMul模型与全精度Transformer之间的性能差距随着模型尺寸增大而减小。  此外，我们提供了一个高效的GPU实现，相较于未优化的基线，训练时能减少高达61%的内存使用。在推理阶段，通过优化的内核，我们的模型内存消耗可降低超过10倍。为了准确评估架构效率，我们在FPGA上构建了定制硬件解决方案，利用GPU无法处理的轻量级运算，实现了对十亿参数规模模型的高速处理，使其接近人脑级别的效率。  这项工作不仅展示了LLMs在减小复杂性后仍能保持高效，还指出了未来加速器应优化的运算类型，以适应下一代轻量级LLMs的需求。我们的代码实现已开源至：\url{https://github.com/ridgerchu/matmulfreellm}。**|
|**2024-06-04**|**CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks**|Maciej Besta et.al.|[2406.02524](http://arxiv.org/abs/2406.02524)|**[link](https://github.com/spcl/checkembed)**|大型语言模型（LLMs）正在各个领域带来变革，但验证其答案仍然是一个重大挑战，尤其是在处理复杂、开放性的任务，如知识整合、摘要和提取。本文提出了一种名为CheckEmbed的精确、可扩展且简便的LLM验证方法。CheckEmbed的核心理念是：通过利用如GPT文本嵌入大模型获取的答案级嵌入来比较LLM的回答。这将复杂的文本答案转化为单一的嵌入，简化了对比过程，实现快速而有意义的验证。我们构建了一个全面的验证管道，该管道实现了CheckEmbed的理念，并提供了评估LLM答案真实性的度量，如嵌入热力图及其总结。我们展示了如何利用这些指标设计实际的引擎，以决定LLM答案是否令人满意。在实际文档分析任务中，如术语提取和文档摘要，我们的方法表现出显著的准确性提升、成本效益和运行时间性能，相较于BERTScore或SelfCheckGPT等基于token、句子和事实级别的方案。|
|**2024-06-04**|**RoboCasa: Large-Scale Simulation of Everyday Tasks for Generalist Robots**|Soroush Nasiriany et.al.|[2406.02523](http://arxiv.org/abs/2406.02523)|null|## 翻译  人工智能的最新进展在很大程度上依赖于规模的扩大。然而，在机器人领域，大规模机器人数据集的获取是一个瓶颈。我们主张利用逼真的物理模拟来提升环境、任务和数据集的规模，以支持机器人学习方法。为此，我们介绍RoboCasa，这是一个大型的仿真框架，旨在训练能够在日常环境中通用的机器人。RoboCasa的特点是拥有丰富且多样化的厨房场景，包括超过150个类别的一千多件3D模型资产和数十种可交互的家具和电器。  我们通过生成式AI工具进一步增强模拟的真实性和多样性，如使用文本到3D模型的技术生成对象资产，以及通过文本到图像模型生成环境纹理。我们设计了100项任务，包括由大型语言模型指导的复合任务，用于系统性评估。为了促进学习，我们提供了高质量的人类演示，并结合自动轨迹生成方法，以最小的人力成本大幅扩充数据集。  我们的实验表明，在使用合成生成的机器人数据进行大规模模仿学习时，存在明显的规模效应，并显示出利用模拟数据在现实世界任务中的巨大潜力。相关视频和开源代码已在https://robocasa.ai/网站上提供。|
|**2024-05-31**|**Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis**|Chaoyou Fu et.al.|[2405.21075](http://arxiv.org/abs/2405.21075)|null|在人工智能的追求中，多模态大型语言模型（MLLMs）已成为近期进步的核心。然而，对它们处理序列视觉数据的能力的关注尚显不足。为此，我们在本文中提出Video-MME，这是首个全面评估MLLMs在视频分析性能的多模态评估基准。我们的工作有四个关键特性：1）视频类型多样，涵盖6个主要视觉领域和30个子领域，确保广泛的应用场景泛化能力；2）时间维度的跨度，包括短、中、长期视频，从11秒到1小时，以检验模型对复杂情境动态的适应性；3）数据模态的广度，结合视频帧以外的多种输入，如字幕和音频，揭示MLLMs的全方位能力；4）高质量的标注，由专家严格手动标记，以保证精确且可靠的模型评估。我们精心挑选并手动注解了900段视频，总时长达到256小时，生成了2,700个问题-答案对。通过Video-MME，我们对包括GPT-4系列、Gemini 1.5 Pro在内的多个最先进的MLLM，以及开源图像模型InternVL-Chat-V1.5和视频模型LLaVA-NeXT-Video进行了深入评估。实验结果显示，Gemini 1.5 Pro是表现最佳的商业模型，明显优于开源模型。我们的数据集和发现强调了改进处理更长序列和多模态数据的必要性。项目网页链接：https://video-mme.github.io|
|**2024-05-31**|**Grammar-Aligned Decoding**|Kanghee Park et.al.|[2405.21047](http://arxiv.org/abs/2405.21047)|null|大型语言模型（LLMs）在生成高度结构化的输出时面临挑战，如程序代码、数学公式或规范的标记。约束解码方法通过限制每次输出可能的令牌，确保输出符合特定规则来缓解这个问题，例如在语法约束解码（GCD）中，LLM的输出必须遵循给定的语法规则。然而，研究表明，这种约束解码可能会扭曲模型的分布，导致生成的输出虽然语法正确，但其概率并不直接反映LLM本身的概率分配，从而质量不高。我们称之为“与语法约束对齐的解码”（Grammar-Aligned Decoding，GAD），并提出了一种名为“自适应采样与近似期望未来”（Adaptive Sampling with Approximate Expected Futures，ASAp）的解码算法。  ASAp算法旨在保证输出的语法性，并理论上产生与LLM在给定语法约束条件下的条件概率相符的结果。该算法利用先前的样本输出来稳健地估算不同输出前缀的未来语法可能性。我们在代码生成和结构化自然语言处理任务上的实验表明，ASAp经常能够生成比现有GCD技术更符合LLM分布且仍遵守所需语法限制的输出，从而提高了整体质量。|
|**2024-05-31**|**Direct Alignment of Language Models via Quality-Aware Self-Refinement**|Runsheng Yu et.al.|[2405.21040](http://arxiv.org/abs/2405.21040)|null|强化学习从人类反馈（RLHF）是调整大型语言模型（LLMs）行为以符合人类偏好的常用方法。最近，直接策略优化（DPO）作为一种替代方案兴起，它不再依赖LLM奖励模型，从而减少了额外的内存和训练时间。然而，DPO忽视了正向和负向响应的相对质量，可能导致训练结果不理想。为解决这个问题，我们探讨利用LLM内部知识在即时微调过程中获取响应的质量，并优化损失函数。我们设计了一种细化函数，利用LLM的知识来估计正向和负向响应的品质。实验表明，在轻度假设下，构建的细化函数能够帮助自我调整损失函数。我们将这个细化功能整合到DPO及其变体身份策略优化（IPO）中。实验证明，这些改进后的模型在各种评估者上表现出优于DPO和IPO的性能。|
|**2024-05-31**|**Standards for Belief Representations in LLMs**|Daniel A. Herrmann et.al.|[2405.21030](http://arxiv.org/abs/2405.21030)|null|随着大型语言模型（LLMs）在各个领域展现出非凡能力，计算机科学家们正在寻求理解它们的认知过程，特别是关于LLMs如何（如果有的话）内部构建对世界的信念。然而，目前尚缺乏一个统一的理论框架来支撑对LLM中信念的研究。本文试图填补这一空白，提出了一套条件，使LLM中的表示能够被视为信念似的。我们指出，尽管在LLMs中测量信念的项目与决策理论和形式认识论中的信念测量在许多方面有相似之处，但也存在差异，这些差异应影响我们的测量方法。因此，借鉴哲学洞察和机器学习的当代实践，我们确立了四个标准：准确性、一致性、统一性和实用性。这四个标准结合了理论考量与实际限制，为全面理解LLM中的信念表示奠定了基础。我们引用实证工作的成果，揭示了单独使用某些标准时识别信念表示的局限性。|
|**2024-05-31**|**LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models**|Elias Stengel-Eskin et.al.|[2405.21028](http://arxiv.org/abs/2405.21028)|**[link](https://github.com/esteng/pragmatic_calibration)**|**当回答问题时，语言模型不仅能提供答案，还能传达对答案正确性的信心程度。这包括明确的分数标记，如给出数字，以及隐含的信心标志，如权威语气或提供额外知识。然而，当前大多数模型往往过于自信。为了校准这些信心度，我们提出了一种实用的、考虑听众的微调方法（LACIE），它不仅关注答案是否正确，还关注答案是否会被听众接受。我们将校准视为偏好优化，通过双代理游戏创建数据，让一个演讲者模型的输出接受模拟听者的评判。然后，我们使用LACIE对三个语言模型（Mistral-7B、Llama3-8B和Llama3-70B）进行微调，并显示经过微调的模型在模拟听者面前有更好的校准。重要的是，这些趋势也适用于人类听众，帮助他们更准确地预测模型的正确性：我们在人机评估中发现，经过LACIE训练的模型接受的错误答案减少了47%，而正确答案的接受率保持不变。此外，LACIE泛化到另一个数据集上，在使用TriviaQA训练后，TruthfulQA上的真实性大幅提高。我们的分析表明，LACIE导致了正确和错误示例之间的信心度更好地分离。定性上，我们发现经过LACIE训练的模型会更加谨慎，并在回答正确时通过使用权威语气或提供细节来隐性地表示确定性。最后，LACIE微调导致模型对于可能错误的答案更倾向于放弃（例如说“我不知道”）。**|
|**2024-05-31**|**Improved Techniques for Optimization-Based Jailbreaking on Large Language Models**|Xiaojun Jia et.al.|[2405.21018](http://arxiv.org/abs/2405.21018)|**[link](https://github.com/jiaxiaojunqaq/i-gcg)**|**随着大型语言模型（LLMs）的快速发展，其安全校准成为广泛应用的关键。针对这些模型的破解（即“jailbreaking”）活动日益增多，其中贪婪坐标梯度（GCG）攻击因其成效显著而受到关注。然而，GCG的攻击效率仍有提升空间。本文提出了一系列改进的优化基线破解技术，以提升GCG的性能。首先，我们注意到单个目标模板“Sure”极大地限制了GCG的攻击效果，因此我们建议采用包含有害自我暗示和/或指导的多样化目标模板，以误导模型。在优化策略上，我们建议在GCG中实施自动多坐标更新，以加速收敛，并引入从简单到复杂（easy-to-hard）的初始化技巧。将这些改进整合，我们开发出一种高效的方法—— $\mathcal{I}$ -GCG。实验在一系列基准测试，如NeurIPS 2023 红队挑战中进行，结果显示，我们的改进技术能够帮助GCG超越现有破解攻击，实现接近100%的攻击成功率。代码已发布在https://github.com/jiaxiaojunQAQ/I-GCG。**|
|**2024-05-31**|**DeCo: Decoupling Token Compression from Semantic Abstraction in Multimodal Large Language Models**|Linli Yao et.al.|[2405.20985](http://arxiv.org/abs/2405.20985)|**[link](https://github.com/yaolinli/deco)**|该研究关注于多模态语言模型（MLLMs）中的投影器模块，因为它们在连接视觉和语言模态、促进跨模态对齐方面发挥关键作用。然而，目前对于投影器在视觉-语言对齐方面的效果评估仍显不足，通常只能通过下游任务的性能间接推断。为此，本研究通过分析MLLM中的视觉-语言语义流，来解读投影器的工作机制。  具体来说，研究者追踪从生成的语言标记到原始视觉编码块以及投影器产生的中间输出之间的语义相关性流。发现压缩型投影器（如QFormer）倾向于将视觉块抽象成有限的几个概念，如物体或属性，导致“双重抽象”现象：首先，投影器参照预定义查询令牌进行视觉语义抽象，然后，基于文本指令的大语言模型进一步提取。这种双重抽象在训练过程中效率不高，并可能导致视觉语义信息的累积缺失。  为解决这个问题，研究提出“解耦压缩与抽象（DeCo）”的关键洞察，即在投影层面上将视觉令牌数量压缩，而让大语言模型完全负责视觉语义抽象。因此，研究人员采用了一种简单的压缩器——二维自适应池化，以无参数的方式降低视觉块的尺寸。实验结果显示，DeCo在性能和效率上都优于传统的压缩投影器。它在MLLM基准、视觉定位和开放性视觉问答任务中分别取得了0.9%、7.1%和2.9%的性能提升，同时拥有更少的可训练参数和更快的收敛速度。|
|**2024-05-31**|**Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training**|Feiteng Fang et.al.|[2405.20978](http://arxiv.org/abs/2405.20978)|**[link](https://github.com/calubkk/raat)**|大型语言模型（LLMs）展现出强大功能，但面临挑战，如虚构、过时知识和难以追溯的推理过程。为解决这些问题，检索增强生成（RAG）作为一种有前景的方法崭露头角，它结合外部数据库的知识。然而，不适当的检索段落可能妨碍LLMs生成全面且高质量的回答。先前关于RAG中检索噪声稳健性的研究往往局限于有限的噪声类型，这与现实世界的检索环境不符，限制了实际应用。本研究首先探讨了检索噪声，并将其分为三种不同的类别，反映真实环境。我们分析了这些不同类型的检索噪声对LLMs稳健性的影响。  接着，我们提出了一种新颖的RAG方法，称为检索增强自适应对抗训练（RAAT）。RAAT利用自适应对抗训练来动态调整模型的训练流程以应对检索噪声，并采用多任务学习确保模型能够识别嘈杂的上下文。大量的实验表明，在各种噪声条件下，使用RAAT训练的LLaMA-2 7B模型在F1和EM分数上显示出显著提升。为了便于复现，我们已在https://github.com/calubkk/RAAT上发布了我们的代码和数据。|
|**2024-05-31**|**SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales**|Tianyang Xu et.al.|[2405.20974](http://arxiv.org/abs/2405.20974)|**[link](https://github.com/xu1868/sayself)**|**大型语言模型（LLMs）常常产生不准确或虚假的信息，并且通常无法表明其信心水平，这限制了它们的广泛应用。先前的研究试图通过直接提示或自我一致性提示来提取LLMs的信心，或者构建特定数据集进行监督微调。基于提示的方法性能较差，而基于训练的方法又局限于二元或不精确的整体信心估计。本文提出了一种先进的方法——SaySelf，这是一个训练框架，旨在教导LLMs提供更精确的细粒度信心估计。  此外，SaySelf还推动LLMs生成自我反思的解释，明确指出它们在参数知识上的空白并解释不确定性。这是通过让LLM以自然语言的形式自动总结特定知识中的不确定性来实现的。这种总结是基于对多个采样推理链的不一致性分析，生成的数据用于监督微调。为了进一步校准信心估计，我们采用了精心设计的强化学习，奖励准确、高置信度的预测，同时惩罚错误输出中的过度自信。  实验结果表明，无论是在分布内还是分布外的数据集上，SaySelf都能有效减少信心校准误差，同时保持任务性能。生成的自我反思理由也被证明是合理的，能进一步促进校准。代码已公开在：\url{https://github.com/xu1868/SaySelf}。**|
|**2024-05-31**|**LCQ: Low-Rank Codebook based Quantization for Large Language Models**|Wen-Pu Cai et.al.|[2405.20973](http://arxiv.org/abs/2405.20973)|null|## 背景  大型语言模型（LLMs）在众多任务上展现出优异性能，但它们的存储和计算成本高成为部署的一大挑战。为了压缩模型并降低成本，权重量化技术被广泛应用。目前，大多数针对LLMs的量化方法使用秩一码本，然而在高压缩比下，这会导致显著的精度损失。本文提出了一种新颖的权重量化方法，称为低秩码本量化（LCQ），旨在解决这一问题。  ## 方法  LCQ采用低秩码本进行量化，其秩可以大于一。这种方法旨在通过利用更高的秩来保持或提升模型的精度，同时控制额外的存储开销几乎为零。实验表明，与现有方法相比，LCQ在保持良好准确性的前提下，能够实现更优的压缩效果。  ## 结论  综上所述，本文介绍了一种创新的低秩码本量化方法，它有望在不显著增加存储成本的情况下，提升大型语言模型在实际应用中的性能和效率，为高效部署这些模型提供了新的解决方案。|
|**2024-05-30**|**MotionLLM: Understanding Human Behaviors from Human Motions and Videos**|Ling-Hao Chen et.al.|[2405.20340](http://arxiv.org/abs/2405.20340)|null|这项研究关注于多模态（视频和动作模态）下的人类行为理解，通过大型语言模型（LLMs）的强大功能。与专为单模态（视频或动作）设计的最新LLMs不同，我们认为理解人类行为需要对视频和动作序列（如SMPL序列）进行联合建模，以有效捕捉精细的身体部位动态和语义。为此，我们提出MotionLLM，这是一个简洁而有效的框架，用于人类动作理解、描述和推理。MotionLLM采用了一体化的视频-动作训练策略，利用现有粗粒度的视频-文本数据和精细动作-文本数据的优势，以获取丰富的空间-时间洞察。此外，我们还创建了一个大规模的MoVid数据集，包含了多样化的视频、动作、caption和指令。我们还提出了MoVid-Bench，它具有精心的手动标注，以更好地评估在视频和动作上的人类行为理解能力。实验结果充分展示了MotionLLM在caption生成、空间-时间理解以及推理能力方面的优越性。|
|**2024-05-30**|**Visual Perception by Large Language Model's Weights**|Feipeng Ma et.al.|[2405.20339](http://arxiv.org/abs/2405.20339)|null|这篇论文的背景是现有的多模态大型语言模型（MLLMs）采用了一种方法，即将视觉信息与语言模型的输入空间对齐，然后将视觉令牌与文本令牌合并，形成统一的序列输入给语言模型。然而，这种方法由于增加了由视觉令牌导致的输入序列长度，计算成本较高。为此，论文提出了一种新颖的参数空间对齐范式，通过将视觉信息表示为模型权重来处理。对于每个输入图像，首先使用视觉编码器提取特征，然后将这些特征转换为感知权重，并将其与语言模型的权重融合。这样，语言模型的输入无需视觉令牌，从而缩短了输入序列，显著提高了效率。  基于这一理念，论文提出了VLoRA模型，其中包含一个感知权重生成器。该生成器设计成能够将视觉特征转化为具有低秩特性的感知权重，类似于LoRA（低秩自适应训练）。实验结果表明，尽管VLoRA在多种多模态任务的基准上表现出与现有MLLMs相当的性能，但其在训练和推理阶段的计算成本显著降低。论文承诺开源代码和模型。|
|**2024-05-30**|**Xwin-LM: Strong and Scalable Alignment Practice for LLMs**|Bolin Ni et.al.|[2405.20335](http://arxiv.org/abs/2405.20335)|**[link](https://github.com/xwin-lm/xwin-lm)**|**本文介绍Xwin-LM，一个专为大型语言模型（LLMs）设计的全面对齐方法套件。它涵盖了监督微调（SFT）、奖励建模（RM）、拒绝采样微调（RS）和直接偏好优化（DPO）等多种关键技术。主要组成部分包括：(1) 使用高质量指令数据进行初始微调的Xwin-LM-SFT；(2) 由GPT-4精心标注的大型多轮偏好数据集Xwin-Pair；(3) 在7B、13B和70B参数规模上训练的Xwin-RM奖励模型；(4) 每个提示关联64个独特响应的多wise偏好数据集Xwin-Set，这些响应由Xwin-LM-SFT生成并由Xwin-RM评分；(5) 使用Xwin-Set中最高得分响应进行微调的Xwin-LM-RS模型；(6) 通过DPO算法在Xwin-Set上进一步优化的Xwin-LM-DPO模型。我们在AlpacaEval和MT-bench上的评估显示了整个管道的稳定且显著改进，证明了Xwin-LM的强大和可扩展性。我们将在https://github.com/Xwin-LM/Xwin-LM的仓库中持续更新，以促进社区研究。**|
|**2024-05-31**|**ParSEL: Parameterized Shape Editing with Language**|Aditya Ganeshan et.al.|[2405.20319](http://arxiv.org/abs/2405.20319)|null|本文提出了一种名为ParSEL的系统，它旨在通过自然语言实现高质量3D资产的可控编辑。面对自然语言在精确操控上的局限性，ParSEL接收一个分割的3D网格和编辑请求，生成一个参数化的编辑程序。用户可以调整程序参数，精细地探索形状变化，控制编辑幅度。系统利用大型语言模型（LLMs）来理解初始编辑指令，但发现它们在推断完整编辑程序时常常不足，产生的结果可能违反形状逻辑。为此，我们设计了分析性编辑传播（Analytical Edit Propagation，AEP）算法，它从初始编辑种子开始，通过计算机代数系统进行几何分析，寻找与潜在用户编辑兼容的分析性编辑操作，以生成完整的编辑程序。实验表明，相较于其他方案，ParSEL通过自然语言请求有效地实现了对3D对象的可控编辑。|
|**2024-05-30**|**CausalQuest: Collecting Natural Causal Questions for AI Agents**|Roberto Ceraolo et.al.|[2405.20318](http://arxiv.org/abs/2405.20318)|**[link](https://github.com/roberto-ceraolo/causal-quest)**|**人类天生就有寻求因果关系的驱动力，无论是出于好奇心还是特定目标。为了开发能处理这种人类本性追求的AI代理，我们急需一个全面的自然因果问题数据集。然而，现有的数据集要么包含人工制造的问题，无法反映实际AI应用场景，要么在特定来源的问题覆盖上有限。为此，我们提出了CausalQuest，这是一个源自社交网络、搜索引擎和AI助手的13,500个自然出现的问题的数据集。我们定义了因果问题，并建立了更细致的分类体系。通过人类标注员和大型语言模型的协作，我们对数据集进行了精心标注。研究发现，42%的人类提问实际上是关于因果的，大部分是想了解给定结果背后的原因。利用这个数据集，我们训练了高效的二分类器（高达28.5亿参数），用于识别因果问题，实现了高性能，F1分数高达0.877。最后，我们提出了一系列丰富的未来研究方向，这些都可以基于我们的数据和模型进行扩展。**|
|**2024-05-30**|**ANAH: Analytical Annotation of Hallucinations in Large Language Models**|Ziwei Ji et.al.|[2405.20315](http://arxiv.org/abs/2405.20315)|**[link](https://github.com/open-compass/anah)**|**### 背景  大型语言模型（LLMs）的“幻觉”问题对于其广泛应用至关重要。然而，对这一问题的细致测量在社区中并未得到充分探索。为此，我们提出了一项名为 $\textbf{ANAH}$ 的双语数据集，专注于生成式问答中的LLM幻觉分析。ANAH中的每个答案句子都经过严谨标注，包括参考片段检索、幻觉类型的判断以及错误内容的修正。该数据集包含约12,000个句级注释，涵盖了大约4,300个LLM响应，涉及超过700个主题，通过人机交互式流程构建而成。由于幻觉注释的精细粒度，我们可以定量确认LLMs的幻觉问题随着答案的扩展而逐渐增加，并利用ANAH来训练和评估幻觉标注器。  ### 任务  我们构建了大约12,000条句子级别的注释，针对约4,300个LLM生成的回答，涵盖了超过700个主题。这个名为ANAH的数据集通过人类参与的流程精心设计，旨在提供关于生成式问答中LLMs幻觉的详尽分析。通过细致的幻觉标注，我们能够量化地验证LLMs在生成答案时幻觉问题的累积，并利用ANAH来训练和评估幻觉识别能力。我们的实验深入研究了生成式和区分性标注器，并发现尽管开源LLMs在精细幻觉标注方面面临挑战，但使用ANAH训练的生成式标注器能够超越所有开源模型，甚至接近GPT-3.5的表现，并展现出在未见过问题上的良好泛化能力。**|
|**2024-05-30**|**Sequence-Augmented SE(3)-Flow Matching For Conditional Protein Backbone Generation**|Guillaume Huguet et.al.|[2405.20313](http://arxiv.org/abs/2405.20313)|null|蛋白质在几乎所有的生物过程中发挥关键作用，其多样化的功能源于复杂的三维结构，而这些结构又由氨基酸序列决定。在这篇论文中，我们利用氨基酸序列丰富的生物学归纳偏置，提出了一种新的序列条件的SE(3)等变流匹配模型——FoldFlow-2，用于蛋白质结构生成。与FoldFlow家族的先前模型相比，FoldFlow-2引入了新颖的架构特性，包括用于编码序列的蛋白质大语言模型、结合结构和序列表示的新多模态融合主干，以及基于几何变换器的解码器。为了增加生成样本的多样性和新颖性——这对新药设计至关重要——我们在比先前工作使用的PDB数据集大一个数量级的新数据集上大规模训练FoldFlow-2，该数据集包含了已知的PDB蛋白质和通过过滤获得的高质量合成结构。此外，我们展示了如何通过引入强化微调（Reinforced Finetuning，简称ReFT）目标，使FoldFlow-2能够适应任意奖励，如提高二级结构多样性。  实验结果表明，FoldFlow-2超越了现有基于蛋白质结构的生成模型的状态，无论在无条件生成还是在设计性、多样性和新颖性方面，都优于RFDiffusion，且在蛋白质长度的各类任务上表现出良好的泛化能力，特别是在等温构象采样任务上。最后，我们展示了一个经过微调的FoldFlow-2在诸如VHH纳米抗体骨架设计等具有挑战性的条件设计任务上取得了进展。|
|**2024-05-30**|**Large Language Models Can Self-Improve At Web Agent Tasks**|Ajay Patel et.al.|[2405.20309](http://arxiv.org/abs/2405.20309)|**[link](https://github.com/AjayP13/webdreamer)**|在复杂的环境中，如网络浏览器，训练模型作为能够有效导航和执行动作的代理通常具有挑战性，主要受限于缺乏训练数据。近年来，大型语言模型（LLMs）显示出通过自然语言提示以零样本或少量样本来在新环境中导航的能力。研究还表明，LLMs可以通过自我改进（即在其自身生成的数据上微调）来超越基础性能。本研究旨在探究LLMs在长时序任务的复杂环境——WebArena基准中，通过自我改进能否提升其表现。WebArena要求代理自主浏览网页并执行操作以达成特定目标。我们使用三种不同的合成训练数据混合进行微调，并发现经过自我改进后，模型在WebArena基准上的任务完成率提高了31%。此外，我们还提出了新的评估指标，用于更全面地评估我们的微调代理模型的行为性能、鲁棒性、能力以及轨迹质量，这些指标超越了当前仅依赖于整体基准分数的评估方式。|
|**2024-05-30**|**Group Robust Preference Optimization in Reward-free RLHF**|Shyam Sundhar Ramesh et.al.|[2405.20304](http://arxiv.org/abs/2405.20304)|**[link](https://github.com/rsshyam/Group-robust-preference-optimization)**|**## 翻译  针对大型语言模型（LLMs）的特定任务进行适应时，通常需要通过基于人类反馈的强化学习（RLHF）和多元标签者群体（如不同性别、种族、公司团队等）的偏好数据进行微调。然而，传统方法倾向于采用“一刀切”的策略，即假设并优化单一的偏好模型，对各群体的独特特性和需求不够敏感。为此，我们提出了一种新颖的群体鲁棒偏好优化（GRPO）方法，旨在稳健地使LLMs适应各个群体的偏好。GRPO方法基于无奖励直接偏好优化，但区别于以往，它目标是寻找一个能最大化最差群体性能的鲁棒策略。为了实现这一目标，GRPO会动态且逐次调整不同群体的权重，优先关注累积损失较高的群体。我们在理论上探讨了GRPO的可行性，并分析了其在对数线性策略类别下的收敛性。通过使用来自不同群体的全局意见数据对LLMs进行GRPO微调，我们显著提高了最差群体的表现，减少了群体间损失的不平衡，同时提高了概率准确性，相较于非鲁棒基线，这些改进效果显著。**|
|**2024-05-30**|**Who Writes the Review, Human or AI?**|Panagiotis C. Theocharopoulos et.al.|[2405.20285](http://arxiv.org/abs/2405.20285)|null|随着人工智能在自然语言处理中的广泛应用，人们关注如何识别不同领域的AI生成文本。本研究旨在探讨这个问题，通过提出一种方法来准确区分人工智能生成的和人类撰写的书评。我们的方法利用迁移学习，让模型能够在不同主题间识别生成文本，同时提高其识别写作风格和词汇变化的能力。我们构建了一个数据集，包含真实的书评和使用Vicuna开源语言模型生成的模拟评论，以评估所提方法的有效性。实验结果显示，识别文本原创来源是可行的，准确率达到96.86%。我们的工作聚焦于大型语言模型在文本识别方面的性能与局限性研究，这对于未来有效管理此类模型以及确保人类创作内容的完整性和真实性具有重要意义。|
|**2024-05-29**|**X-VILA: Cross-Modality Alignment for Large Language Model**|Hanrong Ye et.al.|[2405.19335](http://arxiv.org/abs/2405.19335)|null|我们提出X-VILA，一种旨在增强大型语言模型（LLMs）功能的多模态模型，它融合了图像、视频和音频模态。通过将各模态特定的编码器与LLM输入对齐，并将扩散解码器与LLM输出对齐，X-VILA实现了跨模态理解、推理和生成。为了支持这种跨模态对齐，我们开发了一个有效的任意模态指令跟随数据集。然而，我们发现当前的跨模态对齐方法存在一个关键问题，导致视觉信息丢失。为此，我们设计了视觉对齐机制，包括一个视觉嵌入高速公路模块，以解决这一问题。此外，我们还提供了一种资源高效的训练策略，使得X-VILA在任意模态对话任务上表现出色，大幅超越先前的方法。令人惊讶的是，即使在缺乏类似训练数据的情况下，X-VILA在不同模态间也展现出涌现特性。该项目将开源。|
|**2024-05-29**|**LLMs Meet Multimodal Generation and Editing: A Survey**|Yingqing He et.al.|[2405.19334](http://arxiv.org/abs/2405.19334)|**[link](https://github.com/yingqinghe/awesome-llms-meet-multimodal-generation)**|**随着大型语言模型（LLMs）的最新进展，人们越来越关注将它们与多模态学习相结合。当前的多模态大语言模型（MLLMs）调查主要集中在理解上。这篇综述详细探讨了跨图像、视频、3D和音频等领域的多模态生成，特别强调了这些领域中的里程碑式工作及其技术进步。我们深入研究了这些方法的关键技术组件，以及在相关研究中使用的多模态数据集。此外，我们还剖析了借助现有生成模型进行人类-计算机交互的工具增强型多模态代理。最后，我们全面讨论了人工智能安全的进步，并探索了新兴应用和未来前景。我们的工作提供了一个系统而深入的多模态生成概述，有望推动生成内容的人工智能（AIGC）和世界模型的发展。所有相关的论文列表可在<https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation>找到。**|
|**2024-05-29**|**Multi-Modal Generative Embedding Model**|Feipeng Ma et.al.|[2405.19333](http://arxiv.org/abs/2405.19333)|null|在大多数多模态任务中，问题可以归结为生成或嵌入。现有的模型通常通过将语言模块分解为一个用于生成的文本解码器和一个用于嵌入的文本编码器来处理这两种问题。为了探索多模态方法的简约性，本工作试图仅使用一个模型来处理每种模态。为此，我们提出了一种多模态生成嵌入模型（MM-GEM），它将生成和嵌入目标整合到一个大型语言模型中。同时，我们设计了PoolAggregator，以提高效率并实现细粒度的嵌入和生成能力。  令人惊讶的是，这两个目标之间并没有显著冲突。例如，基于ViT-Large和TinyLlama的MM-GEM在诸如跨模态检索和零样本分类等多模态嵌入模型基准上表现出良好的性能，同时具备良好的图像描述能力。此外，MM-GEM能够无缝执行区域级别的图像描述生成和检索任务。另外，MM-GEM中的先进文本模型对于长文本和图像检索的Recall@1指标带来了超过5%的提升。|
|**2024-05-29**|**Self-Exploring Language Models: Active Preference Elicitation for Online Alignment**|Shenao Zhang et.al.|[2405.19332](http://arxiv.org/abs/2405.19332)|**[link](https://github.com/shenao-zhang/selm)**|****摘要：**  偏好优化，特别是在人类反馈强化学习（RLHF）的驱动下，已经在使大型语言模型（LLMs）遵循人类意愿方面取得了显著成就。相较于使用固定数据集的离线对齐，通过人或人工智能对模型生成的反馈通常能够通过迭代过程提升奖励模型的能力和LLMs的一致性。然而，要实现全局准确的奖励模型，需要系统地探索生成各种各样的响应，以涵盖自然语言的广阔空间。仅依赖标准奖励最大化LLMs的随机采样是不足以满足这一需求的。  为解决这个问题，我们提出了一种双层目标，乐观地倾向于可能具有高奖励的响应，以此来主动探索分布外区域。通过解决内层问题，利用重新参数化的奖励函数，我们提出了名为Self-Exploring Language Models（SELM）的算法。它消除了对单独奖励模型（RM）的需求，并通过一个直观的目标对LLMs进行迭代更新。与直接偏好优化（DPO）相比，SELM的目标降低了对未见过的过度延伸的无差别偏好，提高了探索效率。  我们的实验结果显示，在Zephyr-7B-SFT和Llama-3-8B-Instruct模型上进行微调后，SELM在MT-Bench和AlpacaEval 2.0等指令跟随基准以及不同设置下的各种标准学术基准上表现出显著的性能提升。我们的代码和模型已可在<https://github.com/shenao-zhang/SELM>获取。**|
|**2024-05-29**|**Normative Modules: A Generative Agent Architecture for Learning Norms that Supports Multi-Agent Cooperation**|Atrisha Sarkar et.al.|[2405.19328](http://arxiv.org/abs/2405.19328)|null|本文提出了一种名为“规范模块”的架构，它针对生成性代理在面对包含现有规范的社会结构时的协作挑战。这些代理通过大型语言模型理解和评估环境，但在处理复杂社会任务时，如何识别并适应规范基础设施成为关键问题。规范模块的核心在于促进均衡选择，借鉴分类机构实现相关均衡的概念，使代理能够通过同伴互动学习环境中不同候选机构中的权威性。通过提升规范能力，代理可以协调制裁行为，进而影响社交环境中的基本行为，从而提高整体福祉。  我们设计了一个支持机构的新环境，并根据两个主要标准来评估该框架：一是代理能否忽略非权威机构，二是代理在多个选项中识别权威机构的能力。实验结果显示，配备了规范模块的代理相比基础代理能实现更稳定的合作效果，这为研究设计考虑规范基础设施的环境和代理开辟了新途径。|
|**2024-05-29**|**MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series**|Ge Zhang et.al.|[2405.19327](http://arxiv.org/abs/2405.19327)|**[link](https://github.com/multimodal-art-projection/map-neo)**|近年来，大型语言模型（LLMs）在各种任务上取得了显著进步。然而，出于商业利益，像GPT、Gemini和Claude这样的最先进模型被封闭在专有接口后，其训练详情并未公开。近期，一些机构开源了类似性能的LLMs，如LLaMA-3，但大多数细节（如中间检查点、预训练语料库和训练代码等）仍未披露。为了提高LLMs的透明度，研究界正在推动真正开放的模型，如Pythia、Amber和OLMo，这些模型提供了更多的信息，促进了对大模型性能、局限性、偏见和风险的科学研究。然而，现有的开放模型在推理、知识和编程任务上的表现仍逊于同等规模的封闭源码模型。  因此，我们开源了MAP-Neo，一个拥有70亿参数的双语语言模型，从头开始在4.5万亿高质量令牌上进行训练。MAP-Neo是首个与现有顶级LLMs性能相当的完全开源的双语模型。此外，我们还公开了所有细节，包括清理后的预训练语料库、数据清洗流程、检查点以及优化的训练和评估框架，以供重现。我们期望MAP-Neo能推动开放研究社区的发展，激发更多创新，促进LLMs的进一步提升。|
|**2024-05-29**|**Reasoning3D -- Grounding and Reasoning in 3D: Fine-Grained Zero-Shot Open-Vocabulary 3D Reasoning Part Segmentation via Large Vision-Language Models**|Tianrun Chen et.al.|[2405.19326](http://arxiv.org/abs/2405.19326)|null|本文提出了一项新的任务：零样本3D推理分割，目标是针对物体的部件搜索和定位，这是一种超越了先前类别特定的3D语义分割、3D实例分割和开放词汇3D分割局限的新范式。我们设计了一个名为Reasoning3D的简单基线方法，它能够理解和执行复杂的命令，对3D网格进行（细致）部分分割，同时具备上下文感知和推理答案的交互式分割能力。特别地，Reasoning3D利用预训练的2D分割网络，该网络由大型语言模型（LLMs）驱动，在零样本情况下解析用户输入查询。已有研究表明，大规模预训练赋予基础模型世界知识的先验，使其能够理解复杂指令，这使得我们在依赖有限3D数据集的情况下也能“分割任何东西”（源效率高）。实验表明，我们的方法具有泛化性，能有效根据隐性文本查询在3D对象（3D网格）中定位和突出显示部分，包括可动3D对象和真实世界的扫描数据。此外，我们的无监督方法便于快速部署，并为未来3D（语义）对象理解领域的研究，如机器人、物体操作、部件组装、自动驾驶应用、增强现实和虚拟现实（AR/VR）、以及医疗应用，提供了一个可行的通用基准。代码、模型权重、部署指南和评估协议可在以下链接获取：http://tianrun-chen.github.io/Reason3D/。|
|**2024-05-29**|**Nearest Neighbor Speculative Decoding for LLM Generation and Attribution**|Minghan Li et.al.|[2405.19325](http://arxiv.org/abs/2405.19325)|null|大型语言模型（LLMs）常常会产生虚构内容且缺乏对生成文本的来源标注。为解决这些问题，半参数化语言模型如kNN-LM通过在非参数数据存储中寻找与给定提示最接近的邻居来改进LM输出。然而，这类模型的推理速度通常较慢，生成的文本流畅度不高。本文提出了一种新颖的半参数化语言建模方法——Nearest Neighbor Speculative Decoding（NEST），它能够将现实世界中的任意长度文本片段融入生成过程，并提供其源头的标注。NEST在每次推理步骤中进行基于令牌的检索，计算出一个半参数混合分布，并从语料库中识别出可能的连续文本段落扩展。它采用一种近似推测解码策略，接受检索到的片段前缀或生成新的令牌。NEST显著提高了基础LM在各种知识密集型任务中的生成质量和来源标注率，超越了传统的kNN-LM方法，并在基于上下文的检索增强方面表现出竞争力。此外，NEST大幅提升了生成速度，当应用于Llama-2-Chat 70B时，推理时间提高了1.8倍。|
|**2024-05-29**|**Are Large Language Models Chameleons?**|Mingmeng Geng et.al.|[2405.19323](http://arxiv.org/abs/2405.19323)|null|大语言模型（LLMs）是否拥有自己的世界观和人格倾向？研究人员进行了超过一百万次的实验，让LLMs回答主观问题。通过将这些模型的响应与欧洲社会调查（ESS）的实际数据进行比较，结果显示提示对偏见和变异性有显著影响，揭示了重大的文化、年龄和性别偏差。文中讨论了评估LLMs与调查数据差异的方法，如计算加权平均值以及一个新提出的基于Jaccard相似性的测量指标。研究者强调，在利用LLMs模拟个体决策或集体行为之前，分析提示的稳健性和变异性至关重要，因为它们的模仿能力充其量只能说是近似的。|
|**2024-05-29**|**Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF**|Shicong Cen et.al.|[2405.19320](http://arxiv.org/abs/2405.19320)|null|**摘要：**  强化学习从人类反馈（RLHF）在调整大型语言模型（LLMs）以符合人类偏好方面展现出巨大潜力。在线和离线RLHF都处于活跃的研究阶段，但关键挑战之一是如何在处理从偏好数据中学习的奖励函数不确定性时。尽管标准强化学习（RL）中乐观主义或悲观主义的原则已广为人知，但在大型语言模型中实现既实用又基于理论的方法尚不成熟，因为构建置信区间的标准技术在处理任意策略参数化时变得难以处理。  本文提出了一种统一的在线和离线RLHF方法——价值激励的偏好优化（VPO）。VPO通过在最大似然估计的奖励函数中添加相应的值函数的正则化，以指示选择乐观主义还是悲观主义，实现了这一目标。此外，VPO直接优化策略，并利用隐式奖励建模，因此其RLHF管道与直接偏好优化更为简单。对于在线和离线设置，VPO提供了理论保证，其收敛速度与标准RL相当。实验在文本摘要和对话任务上验证了VPO的实用性与有效性。|
|**2024-05-28**|**Don't Forget to Connect! Improving RAG with Graph-based Reranking**|Jialin Dong et.al.|[2405.18414](http://arxiv.org/abs/2405.18414)|null|## 背景  检索增强生成（Retrieval Augmented Generation，RAG）通过结合现有文档的上下文显著提升了大语言模型（Large Language Model，LLM）的响应性能。然而，当文档与问题上下文的相关性不明显或存在部分信息时，RAG的效果如何？又该如何处理文档之间的关联性呢？本研究旨在解答RAG生成中的这两个核心问题。我们提出了一种名为G-RAG的方法，它是一个基于图神经网络（Graph Neural Networks，GNNs）的重排器，介于RAG的检索器和阅读器之间。G-RAG结合了文档之间的连接性和语义信息（通过抽象意义表示图），为RAG提供了一个具有上下文感知的排名器。实验结果表明，G-RAG超越了现有的领先方法，同时计算开销更小。此外，我们评估了PaLM 2作为重排器的表现，发现其明显逊色于G-RAG，这强调了即使使用大型语言模型，重排在RAG中的重要性。|
|**2024-05-28**|**Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning**|Yixiao Zhang et.al.|[2405.18386](http://arxiv.org/abs/2405.18386)|**[link](https://github.com/ldzhangyx/instruct-MusicGen)**|**在文本到音乐编辑领域，近期的进步依赖于文本查询来改变音乐风格或调整乐器元素。然而，现有方法要么需要从头训练特定的编辑模型，耗时且资源密集，要么使用大型语言模型预测编辑后的音乐，导致音频重建不够精确。为了结合优点并解决这些问题，我们提出了Instruct-MusicGen，这是一种新颖的方法，它针对预训练的MusicGen模型进行微调，以高效地执行编辑指令，如添加、删除或分离音轨。我们的方法修改了原始MusicGen架构，引入了文本融合模块和音频融合模块，使模型能够同时处理指令文本和音频输入，生成所需的编辑音乐。令人惊讶的是，Instruct-MusicGen仅向原始模型增加了8%的新参数，并在5000步的训练后，其性能超越现有基准，且表现出与专门针对任务训练的模型相当的能力。这一进展不仅提高了文本到音乐编辑的效率，还拓宽了音乐语言模型在动态音乐制作环境中的应用范围。**|
|**2024-05-28**|**OwLore: Outlier-weighed Layerwise Sampled Low-Rank Projection for Memory-Efficient LLM Fine-tuning**|Pengxiang Li et.al.|[2405.18380](http://arxiv.org/abs/2405.18380)|**[link](https://github.com/pixeli99/owlore)**|**随着大型语言模型（LLMs）的快速发展，它们在自然语言处理任务中带来了革命性变化。然而，大模型的训练或微调带来了巨大挑战。针对这一问题，低秩适应（LoRA）等参数高效方法崭露头角，但往往牺牲性能。本文提出了一种新的内存高效微调方法——Outlier-weighed Layerwise Sampled Low-Rank Projection（OwLore），它受到LLMs层间异常分布的启发，通过动态采样预训练层而非添加额外适配器来进行微调。我们首先通过Heavy-Tailed Self-Regularization理论（HT-SR）解读异常现象，发现具有更多异常值的层更倾向于呈现长尾分布，训练效果更好。因此，OwLore策略性地为异常值较多的层分配更高的采样概率，以更好地利用预训练模型的知识。  为了进一步减少微调时的内存需求，我们结合梯度低秩投影，使得每一层能以低秩方式高效训练。通过融合低秩优势和最优层别采样策略，OwLore显著优化了LLM剪枝中的内存-性能权衡。我们在多个架构，如LLaMa2、LLaMa3和Mistral上的广泛实验表明，OwLore持续优于基础方法，包括全量微调。例如，在常识推理基准上，OwLore可实现平均1.1%的精度提升，MMLU上提高3.0%，而在MT-Bench上更是有显著的10%提升，同时内存效率更高。特别地，OwLore仅需21GB内存即可对LLaMa2-7B进行微调。**|
|**2024-05-28**|**LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models**|Anthony Sarah et.al.|[2405.18377](http://arxiv.org/abs/2405.18377)|null|现代大型语言模型（LLMs）在自然语言处理、复杂推理、情感分析等任务中的卓越表现推动了它们的广泛应用。然而，这些强大的功能伴随着巨大的内存和计算成本，限制了在大多数硬件平台上的使用。为解决这一问题，我们提出了一种有效的方法，基于LLaMA2-7B进行单次微调后，通过遗传算法搜索找到更小、计算复杂度更低的网络架构。实验表明，对于某些标准基准任务，预训练的LLaMA2-7B模型实际上过于庞大且复杂。我们实现了1.5倍的模型大小缩减和1.3倍的吞吐量提升，同时保持了几乎无损的准确性。相较于某些剪枝或稀疏化技术，我们的方法在效率和效果上更为优越。最后，我们展示了量化与我们的方法相结合的效果，进一步通过量化减少了找到的网络的大小和复杂性。我们相信，本工作提供了一种自动创建可在更廉价和广泛可用硬件平台上使用的LLMs的方法。|
|**2024-05-28**|**Empowering Source-Free Domain Adaptation with MLLM-driven Curriculum Learning**|Dongjie Chen et.al.|[2405.18376](http://arxiv.org/abs/2405.18376)|**[link](https://github.com/Dong-Jie-Chen/RCL)**|**### 背景  源免费领域适应（SFDA）的目标是仅使用未标记的靶域数据来调整预训练的源模型。当前的SFDA方法在有效利用预训练知识和挖掘靶域数据潜力方面面临挑战。多模态大型语言模型（MLLMs）在理解视觉和文本信息方面表现出色，但它们应用于SFDA时存在问题，如指令执行失败、计算需求高以及在适应前性能评估困难。为了缓解这些问题，我们提出了一种新颖的框架——可靠性基于课程学习（RCL），它通过伪标签化整合多个MLLM以促进知识利用，应用于SFDA。  ### 方法  我们的框架包括：1) 可靠知识转移，2) 自我纠正，3) MLLM引导的知识扩展，以及4) 多热掩码精炼，这些方法协同作用，逐步发掘靶域未标记数据的价值。RCL在多个SFDA基准上实现了最先进的（SOTA）性能，例如在DomainNet上提升显著，达到 $\textbf{+9.4\%}$ ，证明了其在增强适应性和鲁棒性方面的有效性，同时无需访问源数据。代码可在https://github.com/Dong-Jie-Chen/RCL获取。**|
|**2024-05-28**|**Thai Winograd Schemas: A Benchmark for Thai Commonsense Reasoning**|Phakphum Artkaew et.al.|[2405.18375](http://arxiv.org/abs/2405.18375)|**[link](https://github.com/PhakphumAdev/Thai-Winograd)**|常识推理是自然语言理解的重要组成部分，为此已开发出多个评估基准。然而，这些基准大多仅限于英语。创建平行基准有助于跨语言评估，从而更好地理解不同语言。本研究介绍了一个泰语版的Winograd Schema集合，这是一个专为测试泰语中的常识推理能力而设计的新数据集。我们通过邀请母语者、专业翻译和严格验证的方法，确保该系列题库能准确反映泰国语言的独特性、习语和文化引用，同时保持模糊性和常识挑战。我们对大型语言模型（如GPT-4和Claude-3-Opus）在这项基准上的性能进行了评估，结果显示尽管在英语上表现优异，但它们在泰语中的性能明显下降，这表明在多语言常识推理方面仍有待进步。|
|**2024-05-28**|**PromptWizard: Task-Aware Agent-driven Prompt Optimization Framework**|Eshaan Agarwal et.al.|[2405.18369](http://arxiv.org/abs/2405.18369)|null|大型语言模型（LLMs）已经在各个领域带来了革命性的变化，展现出卓越的能力。它们成功的关键在于提示的概念，即指导模型生成输出。然而，手动创建提示既耗时又局限于特定领域，因此需要自动化的解决方案。本文介绍PromptWizard，一个新颖的框架，它利用LLMs迭代地合成和优化针对特定任务的提示。与现有方法不同，PromptWizard同时优化提示指令和上下文示例，以最大化模型性能。该框架通过变异指令并引入负例，逐步深化理解并保证多样性。借助一个评判者，PromptWizard进一步改进指令和示例，融入详细的推理步骤，以实现最佳表现。PromptWizard具有计算效率高、适应不同训练数据量场景以及在小型LLM上同样有效的特点。通过对8个数据集的35个任务进行严谨评估，结果显示PromptWizard明显优于现有的提示策略，证明了其在提示优化方面的高效性和可扩展性。|
|**2024-05-28**|**Is a 3D-Tokenized LLM the Key to Reliable Autonomous Driving?**|Yifan Bai et.al.|[2405.18361](http://arxiv.org/abs/2405.18361)|null|随着自动驾驶（AD）任务的快速发展，基于端到端的方法，特别是视觉语言模型（VLM）的应用变得尤为重要。这些模型试图融合强大的逻辑推理和认知能力，以实现全面的端到端规划。然而，现有的VLM方法往往依赖于2D视觉分词器和大型语言模型（LLM），在处理三维几何信息方面存在不足，这对于可靠的规划至关重要。研究表明，2D分词的LLM并不能准确感知三维环境，这引发了关于VLM在自动驾驶中可靠性的质疑。  针对这一问题，我们提出了一种名为Atlas的新方法，它结合了DETR风格的3D感知器作为3D分词器，与单层线性投影器相连，巧妙地利用了三维物理世界的固有特性。这种方法允许高分辨率多视角图像的同时处理和时空建模。尽管简单，但Atlas在NuScenes数据集上的3D检测和自主驾驶规划任务中表现出色，证明了3D分词的LLM对于实现可靠自动驾驶至关重要。我们将开源代码和数据集，以供进一步研究。|
|**2024-05-28**|**Bridging the Gap: Dynamic Learning Strategies for Improving Multilingual Performance in LLMs**|Somnath Kumar et.al.|[2405.18359](http://arxiv.org/abs/2405.18359)|null|大型语言模型（LLMs）正在全球范围内重塑众多领域，但它们在处理非拉丁字母和低资源语言时的包容性和效果仍有待提升。本文针对这一关键挑战，提出了一种无需大量训练或微调的方法来增强多语言LLMs的表现。通过系统地研究和评估各种语言在流行的问题解答（QA）数据集上的性能，我们提出了一系列新颖技术，以释放LLMs在多元语言环境中的真正潜力。我们的方法包括三个核心策略，极大地提高了多语言能力：首先，精心优化适用于多语言LLM的提示，挖掘其潜在能力，显著提升了各语言的表现。其次，我们引入了一种新的混合方法，结合了多语言嵌入的LLM检索增强生成（RAG），实现了更好的多任务性能。最后，我们开发了一种动态学习策略，实现实时根据查询动态选择最合适的提示策略、LLM模型和嵌入模型，从而最大化LLM在不同语言上的效率，超越了最佳静态和随机策略。此外，我们的方法既适用于离线配置调整，也支持在线适应，能够无缝适应新语言和数据集，显著推动了多语言理解和生成在各种语言中的进步。|
|**2024-05-28**|**MMCTAgent: Multi-modal Critical Thinking Agent Framework for Complex Visual Reasoning**|Somnath Kumar et.al.|[2405.18358](http://arxiv.org/abs/2405.18358)|null|## 背景  近期的多模态大型语言模型（MLLM）在视觉与语言融合任务上取得了显著进步。然而，它们在细致的多模态理解、复杂任务解析以及多模态信息推理方面仍存在挑战。本文提出MMCTAgent，一个旨在解决当前MLLM在复杂视觉推理任务中固有局限性的新型多模态批判性思维代理框架。MMCTAgent借鉴了人类认知过程和批判性思考的特点，通过迭代分析多模态信息、拆解问题、规划策略，并实现动态推理。  此外，MMCTAgent还融入了批判性思考元素，如对最终答案的验证和自我反思。它通过一种新颖的方法定义基于视觉的评判者，并确定特定任务的评估标准，从而提升决策能力。在多个图像理解和视频理解基准测试中，我们严谨地评估了MMCTAgent（包括带评判者的版本）的表现，结果表明它在超越基础MLLM和其他工具增强的管道方面表现出色。|
|**2024-05-27**|**Matryoshka Multimodal Models**|Mu Cai et.al.|[2405.17430](http://arxiv.org/abs/2405.17430)|null|## 背景  大型多模态模型（如LLaVA）在视觉-语言推理方面表现出色。这些模型首先将图像嵌入到大量的固定视觉令牌中，然后将它们输入到大型语言模型（LLM）。然而，这种设计在处理高分辨率图像和视频等密集视觉场景时会导致大量令牌，从而导致效率低下。尽管存在令牌剪枝/合并方法，但它们为每个图像生成单个长度的输出，无法在信息密度与效率之间灵活权衡。受到套娃玩偶概念的启发，我们提出了M3：套娃多模态模型，它学习将视觉内容表示为捕捉不同粗细粒度信息的嵌套视觉令牌集合。  ## 任务  我们的方法为LMMs带来了几个独特的优势：(1) 在测试实例中，用户可以明确控制视觉粒度，例如，根据内容的复杂性或简洁性调整用于表示图像的令牌数量；(2) M3提供了一个分析现有数据集所需粒度的框架，我们发现像COCO这样的基准只需要大约~9个视觉令牌就能获得与使用所有576个令牌相当的准确性；(3) 我们的方法为探索性能与视觉令牌长度之间的最佳权衡提供了基础，研究显示当前固定规模表示与理想上限之间存在显著差距。|
|**2024-05-27**|**NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models**|Chankyu Lee et.al.|[2405.17428](http://arxiv.org/abs/2405.17428)|null|本文介绍了一种名为NV-Embed的新型大语言模型，专门设计用于提升基于解码器的大型语言模型在文本嵌入任务中的性能，包括密集向量检索。NV-Embed通过多种架构设计和训练策略显著增强模型的灵活性和表现，同时保持其简洁性和可复现性。  在架构方面，我们引入了隐式注意力层来获取池化嵌入，这在检索和下游任务准确性上均优于平均池化或使用LLMs的最后一个<EOS> token嵌入。为了改进表示学习，我们移除了LLMs的自回归注意力掩码，在对比性训练中允许更全面的信息交互。  在训练策略上，我们采用两阶段的对比性指令调优方法。第一阶段在检索数据集上进行指令训练，利用批次内负样本和精心挑选的难例。第二阶段将各种非检索任务的数据融入指令调优，不仅提高非检索任务的准确性，还提升了检索性能。  凭借这些创新，NV-Embed仅使用公开数据就实现了前所未有的高分，达到69.32，荣登大规模文本嵌入基准（MTEB）（截至2024年5月24日）榜首，涵盖56项任务，包括检索、重排、分类、聚类和语义文本相似度。尤其值得注意的是，我们的模型在BEIR的15项检索任务中取得了最高的59.36分。NV-Embed模型的源代码将在以下网址开源：https://huggingface.co/nvidia/NV-Embed-v1。|
|**2024-05-27**|**Reason3D: Searching and Reasoning 3D Segmentation via Large Language Model**|Kuan-Chih Huang et.al.|[2405.17427](http://arxiv.org/abs/2405.17427)|**[link](https://github.com/kuanchihhuang/reason3d)**|**随着多模态大型语言模型（LLMs）的最新进展，它们在概念推理等领域展现出巨大潜力。然而，在理解三维环境方面的应用仍相对有限。本文提出Reason3D，这是一种专为全面3D理解设计的新颖LLM。Reason3D接受点云数据和文本提示作为输入，生成文本响应和分割掩码，支持高级任务，如3D推理分割、层次搜索、表达式指代和详细掩码输出的问答。特别是，我们设计了一种分层掩码解码器，能够精确定位广阔场景中的小物体。该解码器首先生成一个粗略的位置估计，覆盖物体的大致区域，然后采用逐步细化的策略，显著提高对象识别和分割的精度。实验结果显示，Reason3D在ScanNet和Matterport3D等大规模数据集上，在3D表达式指代、3D问答和3D推理分割任务上表现出卓越性能。代码和模型已在以下链接提供：https://github.com/KuanchihHuang/Reason3D。**|
|**2024-05-27**|**LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence**|Zhuoling Li et.al.|[2405.17424](http://arxiv.org/abs/2405.17424)|null|由于实体代理需要与现实世界互动，它们必须具备全面的先验知识、长远规划能力以及快速响应速度。尽管近期基于大型语言模型（LLM）的代理表现出色，但它们仍存在一些局限性。例如，LLM的输出通常是描述性的句子，在确定具体动作时可能存在歧义。为了克服这些问题，我们提出了大型自回归模型（LARM）。LARM利用文本和多视角图像作为输入，并以自回归方式预测后续动作。为了训练LARM，我们开发了一种新颖的数据格式，称为自回归节点传输结构，并构建了相应的数据集。通过两阶段训练，LARM成功在《我的世界》（Minecraft）中收集魔法装备，这比先前最佳方法所能达到的成就需要更复杂的决策链。此外，LARM的速度是最快的，比以前快6.8倍。|
|**2024-05-27**|**Self-Corrected Multimodal Large Language Model for End-to-End Robot Manipulation**|Jiaming Liu et.al.|[2405.17418](http://arxiv.org/abs/2405.17418)|null|当机器人操作策略面对新任务或物体实例时，其动作性能往往不尽人意。因此，自动检测和自我纠正失败动作的能力对于实际的机器人系统至关重要。近期，多模态大型语言模型（Multimodal Large Language Models，MLLM）在视觉指令跟随方面展现出前景，并在多种任务中展现出强大的推理能力。为了将通用MLLM作为端到端的机器人代理，我们提出了Self-Corrected (SC)-MLLM，不仅使其能够预测末端执行器位置，还赋予其自主识别并纠正错误动作的能力。首先，我们通过参数效率高的微调，使MLLM具备姿态预测功能，将其转化为一个语言建模问题。在遇到执行失败时，模型能识别低层次动作错误的原因（如位置和旋转误差），并主动寻求专家的提示。根据反馈，SC-MLLM会重新思考当前失败场景，生成修正后的动作。此外，我们设计了一种连续策略学习方法，针对成功纠正的样本，提升模型对当前场景配置的适应性，减少专家干预的频率。  为了评估我们的SC-MLLM，我们在模拟和真实世界环境中进行了广泛实验。结果表明，与先前最先进的机器人MLLM（ManipLLM）相比，SC-MLLM显著提高了操作精度：在已知物体类别上从57%提升至79%，在未知新类别上从47%提升至69%。|
|**2024-05-27**|**THREAD: Thinking Deeper with Recursive Spawning**|Philip Schroeder et.al.|[2405.17402](http://arxiv.org/abs/2405.17402)|**[link](https://github.com/philipmit/thread)**|大型语言模型（LLMs）在各种场景中展现出卓越的能力，但随着上下文的长度和复杂度增加，它们仍面临挑战。为此，我们提出了Thinking Recursively and Dynamically（ThReaD）方法。ThReaD将模型生成过程构想为一个执行流程，根据上下文可以完整运行或动态地创建新线程。通过子线程，模型可以分发任务（如思考、获取信息），子线程只返回父线程所需的令牌，从而让模型能够根据需要调整产生令牌时使用的中间工作量。我们在任务解决和问答等场景中应用ThReaD，使其能递归地将给定的任务或问题分解为逐步简化的小子问题，由单独的子线程解决。我们使用少量样本学习的方式实现ThReaD，并在包括ALFWorld、TextCraft、WebShop在内的多个基准测试上评估GPT-4和GPT-3.5的表现，以及两个新基准：DataCommons QA和MIMIC-III ICU QA。实验结果显示，ThReaD在这些基准上实现了最先进的性能，相对于现有框架，即使是小型模型（如Llama-3-8b和CodeLlama-7b）也能提升10%到50%的绝对分数。|
|**2024-05-27**|**MindMerger: Efficient Boosting LLM Reasoning in non-English Languages**|Zixian Huang et.al.|[2405.17386](http://arxiv.org/abs/2405.17386)|**[link](https://github.com/cone-mt/mindmerger)**|## 任务  推理能力对于大型语言模型（LLMs）至关重要，但英语与其他非英语语言之间的差距明显。一些研究通过微调LLMs以重新学习非英语的推理能力，而另一些方法则使用外部模型（如英语翻译文本）的输出来替换非英语输入，以应对LLM理解非英语的挑战。然而，这些方法往往未能充分利用LLMs内在的推理和语言理解能力。为了更好地利用LLMs的思维和语言理解能力，我们提出了一种新方法，称为MindMerger，它将LLMs与多语言模型的外部语言理解能力相结合，以提升多语言推理性能。我们还引入了两步训练策略，首先将外部能力嵌入LLMs，然后训练外部能力和内置能力的协作使用。在三个多语言推理数据集和一个语言理解数据集上的实验表明，MindMerger始终优于所有基线，特别是在低资源语言上。在不更新LLMs参数的情况下，MGSM数据集上所有语言的平均准确率提高了6.7%，低资源语言提高了8.0%。|
|**2024-05-27**|**ReMoDetect: Reward Models Recognize Aligned LLM's Generations**|Hyunseok Lee et.al.|[2405.17382](http://arxiv.org/abs/2405.17382)|null|随着大型语言模型（LLMs）的卓越性能和易用性提升，它们带来的社会风险，如假新闻生成，促使开发出能检测LLM生成文本（LGT）的方法以确保安全使用。然而，由于大量LLM的存在，逐个识别它们的特点变得不切实际。因此，研究关注的是这些强大模型共有的特性，即“对齐训练”，即训练LLMs生成更符合人类偏好的文本。我们的关键发现是，随着这些对齐训练的LLMs致力于最大化人类偏好，它们生成的文本甚至比人类撰写的文本在估计偏好上更高，这使得利用偏好模型（一个训练来模拟人类偏好分布的LLM）轻易就能检测到这些文本。  基于这一发现，我们提出两种进一步增强偏好模型检测能力的训练策略：（1）持续偏好微调，使模型更偏向于识别对齐的LLG；（2）奖励模型对人/LLM混合文本的学习，即使用对齐LLM重述的人类原创文本，这是一种介于LGT和人类文本之间的偏好基准，有助于更好地学习决策边界。我们在六个文本领域和十二种对齐LLM上进行了广泛评估，结果显示我们的方法表现出最先进的性能。相关代码已在https://github.com/hyunseoklee-ai/reward_llm_detect上提供。|
|**2024-05-27**|**RTL-Repo: A Benchmark for Evaluating LLMs on Large-Scale RTL Design Projects**|Ahmed Allam et.al.|[2405.17378](http://arxiv.org/abs/2405.17378)|**[link](https://github.com/AUCOHL/RTL-Repo)**|大型语言模型在辅助进行寄存器传输级（Register Transfer Level, RTL）设计任务上展现出潜力。然而，现有的基准测试在反映真实世界RTL项目复杂性方面存在显著差距。为此，该论文提出了一项新的基准——RTL-Repo，专为评估大型语言模型在大规模RTL设计项目中的性能而设计。RTL-Repo包含了从GitHub公共仓库提取的超过4000个Verilog代码样本，每个样本都提供了对应仓库的完整上下文。我们对包括GPT-4、GPT-3.5、Starcoder2以及像VeriGen和RTLCoder这样的Verilog专用模型在内的多款最先进的模型在RTL-Repo基准上的性能进行了评估，比较它们在生成复杂项目的Verilog代码方面的表现。RTL-Repo为硬件设计社区提供了一个宝贵的资源，用于评估和比较语言模型在实际RTL设计场景中的性能，并针对复杂的多文件RTL项目专门训练Verilog代码生成。RTL-Repo是开源的，已在GitHub上公开可用。|
|**2024-05-28**|**Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models**|ShengYun Peng et.al.|[2405.17374](http://arxiv.org/abs/2405.17374)|null|### 背景  安全校准是确保大型语言模型（LLMs）的行为符合人类偏好并避免有害行为的关键，但近期研究显示，仅使用少量精心设计的训练样本来微调模型可能导致安全性被轻易破坏。我们致力于通过探索LLM的安全景观来评估微调过程中的风险。我们发现了一个普遍存在于流行开源LLM模型参数空间中的新现象，称为“安全盆地”：随机扰动模型权重能使模型在局部区域保持原始校准模型的安全性。  ### 发现与贡献  我们的发现启发我们提出了一种新的安全度量方法——VISAGE，它通过探测模型的安全景观来评估LLM微调过程中的安全性。可视化校准模型的安全景观有助于理解微调如何使模型偏离安全盆地，从而损害安全性。此外，我们观察到系统提示在保护模型方面的重要性，这种保护甚至会传递给处于安全盆地内的扰动版本。这些从安全景观研究中得出的见解为未来LLM安全领域的研究提供了新的洞见。|
|**2024-05-24**|**Scaling Laws for Discriminative Classification in Large Language Models**|Dean Wyatte et.al.|[2405.15765](http://arxiv.org/abs/2405.15765)|null|## 背景  现代大型语言模型（LLMs）标志着机器学习模型能力的一个重大飞跃。这些模型能够对各种查询生成合理的回答，这表明它们在客户服务应用中具有潜力。然而，LLMs已被观察到存在胡言乱语的问题，这在短期内限制了它们在客户服务中的应用。为了解决这个问题，我们提出了一种系统，将语言建模任务重新构想为分类任务，以帮助客户服务代表选择最佳的模板回复。我们的目标是为客服代表提供最合适的前K个候选回复。  ## 任务描述  我们展示了离线和在线实验的结果，证明了实验系统的有效性，离线实验显示出改进，而在线实验则带来了统计显著的效果提升。此外，我们分享了通过模型参数调整进行的验证损失和前K精度的度量曲线。最后，我们讨论了模型大小、延迟和准确性之间的权衡，并展望了未来可能的应用领域。|
|**2024-05-24**|**Large Language Models Reflect Human Citation Patterns with a Heightened Citation Bias**|Andres Algaba et.al.|[2405.15739](http://arxiv.org/abs/2405.15739)|**[link](https://github.com/andresalgaba/llm_citation_patterns)**|论文摘要： 引用实践对于构建科学知识结构至关重要，但往往受到当代规范和偏见的影响。随着大型语言模型（如GPT-4）的出现，这一领域出现了新的动态。研究者首次探索了完全依赖参数知识而非基于搜索或检索增强生成的推荐引用的特性及其潜在偏见。实验使用了一组包含166篇来自AAAI、NeurIPS、ICML和ICLR的论文，这些论文在GPT-4的知识截止日期后发表，涉及3,066个引用。实验让GPT-4为匿名文本中的引用提供学术参考。结果揭示了人类和语言模型（如GPT-4）的引用模式惊人相似，但GPT-4显示出更强的高引用偏见，即使在控制了出版年份、标题长度、作者数量和会议等因素后依然存在。此外，我们发现GPT-4生成的既有和不存在引用的特性高度一致，表明模型内化了引用模式。通过分析引用图谱，显示GPT-4推荐的引用嵌入在相关引用网络中，暗示其对概念的深入理解。尽管语言模型可以辅助引用生成，但它们也可能放大现有偏见并引入新偏见，可能影响科学知识的传播。我们的结果强调了识别模型偏见的必要性，并开发平衡的方法与语言模型互动的重要性。|
|**2024-05-24**|**LM4LV: A Frozen Large Language Model for Low-level Vision Tasks**|Boyang Zheng et.al.|[2405.15734](http://arxiv.org/abs/2405.15734)|**[link](https://github.com/bytetriper/lm4lv)**|大型语言模型（LLMs）的成功催生了多模态大型语言模型（MLLMs）的研究热潮，它们正在改变计算机视觉领域的多个研究范式。尽管MLLMs在诸如视觉问答（VQA）和文本到图像等高级视觉和 Vision-and-Language 任务上表现出色，但尚无研究探讨过低级视觉任务如何从这些模型中受益。我们发现，当前大多数MLLM的设计使其对低级特征视而不见，因此在解决低级视觉任务方面存在固有限制。为此，我们提出 $\textbf{LM4LV}$ ，这是一个框架，它允许一个冻结的LLM无需任何多模态数据或先验知识就能解决一系列低级视觉任务。这突显了LLMs在低级视觉领域的强大潜力，并弥合了MLLMs与低级视觉任务之间的鸿沟。我们期望这项工作能激发对LLMs的新视角，加深对其工作机制的理解。|
|**2024-05-24**|**Optimizing Large Language Models for OpenAPI Code Completion**|Bohdan Petryshyn et.al.|[2405.15729](http://arxiv.org/abs/2405.15729)|**[link](https://github.com/BohdanPetryshyn/openapi-completion-benchmark)**|近期，大型语言模型（LLMs）在代码生成任务中的进步极大地改变了软件开发领域。尽管主流编程语言的代码补全解决方案表现出色，但它们在处理较少见的格式，如OpenAPI定义时性能欠佳。本研究评估了GitHub Copilot，一个流行的商业代码补全工具，在OpenAPI完成任务中的表现，并针对Meta开源的Code Llama模型提出了一系列针对该任务的优化策略。研究中设计了一个语义感知的OpenAPI完成基准，通过实验分析了不同提示工程和微调技术对Code Llama模型性能的影响。经过微调的Code Llama模型在正确性上达到了比GitHub Copilot高出55.2%的峰值，同时其参数数量仅为商业解决方案（基于Codex模型）的1/25。此外，研究还改进了一种广泛使用的代码填充训练方法，解决了模型在接收到小于训练时使用的上下文长度提示时的性能不足问题。|
|**2024-05-24**|**Prompt-Aware Adapter: Towards Learning Adaptive Visual Tokens for Multimodal Large Language Models**|Yue Zhang et.al.|[2405.15684](http://arxiv.org/abs/2405.15684)|null|为了弥合视觉和语言模态之间的鸿沟，多模态大型语言模型（Multimodal Large Language Models，MLLMs）通常会学习一个适配器，将视觉输入转化为大语言模型（LLMs）能理解的令牌。然而，大多数适配器生成的视觉令牌相对固定，不考虑提示中提及的具体对象。由于这些适配器对图像中的每个细节分配同等关注，且倾向于处理整个场景，这可能会增加大语言模型在处理复杂场景时的认知负荷。为此，我们提出了提示感知适配器。这类适配器设计有根据提示特定关注点动态嵌入视觉输入的能力。具体来说，提示感知适配器利用全局和局部文本特征，在粗粒度和细粒度层次上捕捉与提示最相关的视觉线索。这种方法显著提升了大语言模型理解和解释视觉内容的能力。在各种视觉问答任务中，如计数和位置推理实验中，提示感知适配器的效果得到了验证。|
|**2024-05-24**|**What Do You See? Enhancing Zero-Shot Image Classification with Multimodal Large Language Models**|Abdelrahman Abdelhamed et.al.|[2405.15668](http://arxiv.org/abs/2405.15668)|null|这篇论文探讨了如何利用大型语言模型（LLMs）进行零样本图像分类。作者提出了一种简单但有效的方法，通过将多模态LLMs应用于图像输入，生成详尽的文本表示。这些文本表示被转化为跨模态嵌入空间中的固定维特征，并结合使用于零样本分类，无需为每个数据集设计复杂的提示。研究者采用通用提示策略，而非针对每个数据集单独调整。实验结果显示，这种方法在多个数据集上表现出色，比先前方法的准确性有所提升。平均而言，在十个基准测试中，该方法比传统方法提高了4.1个百分点，尤其在ImageNet数据集上的提升达到了6.8个百分点。这表明，多模态LLMs有潜力显著增强如零样本图像分类之类的计算机视觉任务，为现有技术带来了显著的进步。|
|**2024-05-24**|**Class Machine Unlearning for Complex Data via Concepts Inference and Data Poisoning**|Wenhan Chang et.al.|[2405.15662](http://arxiv.org/abs/2405.15662)|null|在人工智能时代，用户可能因隐私顾虑要求AI公司从训练数据集中删除他们的信息。作为模型所有者，重新训练模型会消耗大量计算资源，因此机器遗忘（machine unlearning）技术应运而生，以允许删除请求的训练数据或类别，同时尽量减少对模型性能的影响。然而，对于大规模复杂数据，如图像或文本，从模型中“遗忘”一个类别可能导致性能下降，因为难以确定类别与模型之间的关联。为此，我们提出使用概念（Concept）而非图像特征或文本数据中的令牌来表示要删除类别的语义信息，这有助于切断模型与类别的联系，实现彻底消除影响。  为了分析复杂数据中的概念影响，我们采用了后处理概念瓶颈模型和集成梯度技术，精确识别不同类别中的概念。然后，我们利用随机标签和目标标签的数据污染策略，提出遗忘方法。我们在图像分类模型和大型语言模型（LLMs）上测试了我们的方法，结果一致显示，提出的策略能准确地从模型中抹除目标信息，同时保持模型性能的大部分。|
|**2024-05-24**|**$$\mathbf{L^2\cdot M = C^2}$$ Large Language Models as Covert Channels... a Systematic Analysis**|Simen Gaure et.al.|[2405.15652](http://arxiv.org/abs/2405.15652)|null|近年来，大型语言模型（LLMs）因其在翻译、预测和内容生成等任务中的出色表现而备受瞩目。同时，研究界发现LLMs易受攻击，但也能增强系统的安全性。然而，这些开源的LLMs在作为掩蔽通信媒介，如支持抗审查通信方面的能力如何呢？本论文从实验角度出发，通过实证测量开源LLM模型（Llama-7B）的安全性与容量，以评估其作为掩蔽通信的有效性。尽管结果显示，基于这种模型的通道不太可能实现高实际比特率，这取决于消息长度和模型熵，但我们发现对手发现隐秘通信的可能性较低。为了使结果易于广泛参考，我们采用了一个简单且直观的方案，并假设模型是公开可用的。|
|**2024-05-24**|**LLM-based Robot Task Planning with Exceptional Handling for General Purpose Service Robots**|Ruoyu Wang et.al.|[2405.15646](http://arxiv.org/abs/2405.15646)|null|在日常生活中开发通用服务机器人的需求促使机器人必须能恰当地执行多种基础行为。近期，大规模语言模型（LLMs）的训练进步使得可以直接根据自然语言指令生成任务序列，无需额外的领域知识。然而，尽管LLMs的输出在语义上是正确的，但生成的任务计划可能并不精确地对应于可接受的动作，并且可能存在各种语言模糊性。LLM的幻觉问题对机器人任务规划构成挑战，可能导致生成的内容与现实世界事实或用户输入不符。为此，我们提出了一种基于约束LLM提示的任务规划方法，该方法可以从命令中生成可执行的动作序列。此外，我们还设计了一个异常处理模块来应对LLM幻觉问题，确保生成的结果在当前环境中是可接纳的。我们在RoboCup@Home命令生成器生成的命令上测试了我们的方法，结果显示机器人在理解和执行任务方面表现出色。|
|**2024-05-24**|**GECKO: Generative Language Model for English, Code and Korean**|Sungwoo Oh et.al.|[2405.15640](http://arxiv.org/abs/2405.15640)|null|我们介绍GECKO，一个专为韩语和英语（包括编程语言）设计的双语大语言模型（LLM）。它基于LLaMA架构，使用平衡且高质量的韩英语数据集进行预训练。本报告详述了我们在构建数据管道和训练模型过程中的一些努力。尽管GECKO的词汇量较小，但其在生成韩语和英语令牌时表现出高效性能。我们在代表性的基准测试上评估了其性能，特别是在韩国MMMLU（韩国多模态多语言理解）任务上表现优异，而在英语和代码方面则显示出适度的能力，尽管其训练的令牌数量少于专注于英语的LLMs。GECKO以宽松的许可协议对开源社区开放，我们希望它能为韩语LLM研究提供研究基线和实用见解。您可以在以下链接找到该模型：https://huggingface.co/kifai/GECKO-7B。|
|**2024-05-23**|**A Nurse is Blue and Elephant is Rugby: Cross Domain Alignment in Large Language Models Reveal Human-like Patterns**|Asaf Yehudai et.al.|[2405.14863](http://arxiv.org/abs/2405.14863)|null|跨领域对齐是指将一个概念从一个领域映射到另一个领域的任务。例如，询问“如果\textit{医生}是一种\textit{颜色}，它会是什么颜色？”这个看似奇特的课题旨在研究人们如何通过类别映射和对这些映射的推理来表征具体和抽象的概念。在这篇论文中，我们借鉴认知科学中的这一任务，通过行为研究评估大型语言模型（LLMs）在概念化和推理能力上的表现。我们通过提示LLMs执行跨域映射任务，并在群体和个体层面分析它们的响应。此外，我们还评估了模型对其预测进行推理的能力，通过分析和分类它们对这些映射的解释。结果显示，人类和模型的映射以及解释存在显著相似性，表明模型以与人类类似的方式表征概念。这种相似性不仅体现在模型的表示上，也体现在它们的行为中。而且，模型大多给出有效的解释，并采用与人类类似的推理路径。|
|**2024-05-23**|**Bitune: Bidirectional Instruction-Tuning**|Dawid J. Kopiczko et.al.|[2405.14862](http://arxiv.org/abs/2405.14862)|null|我们提出了一种名为Bitune的方法，该方法提升了预训练的解码器型大语言模型在指令调优方面的性能，从而在多个下游任务上实现了显著的提升。Bitune通过同时应用自回归和双向注意力到提示上，以获取更精确的查询或指令表示。我们为此引入了两组参数，并采用了参数高效微调技术来处理。这两种特征随后被组合成一个加权平均，其中权重由可训练系数决定，用于生成新的令牌。实验结果表明，Bitune在零样本设置下在常识推理、算术和语言理解任务上表现出色。大量的消融研究验证了每个组件的作用，并显示了该方法对不同PEFT技术的鲁棒性。|
|**2024-05-23**|**PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression**|Vladimir Malinovskii et.al.|[2405.14852](http://arxiv.org/abs/2405.14852)|**[link](https://github.com/vahe1994/aqlm)**|## 背景  对于大型语言模型（LLMs）的“极端”压缩，即将其参数压缩至1-2位每参数，以适应资源受限设备上的高效执行，引起了广泛关注。现有研究主要集中在改进一次性量化技术和权重表示上；然而，纯后训练方法在精度与位宽权衡方面的收益正在减少。当前最先进的量化方法，如QuIP#和AQLM，包含对部分压缩参数的小规模校准数据微调；然而，这些针对压缩权重的微调通常仅使用直通估计器（STE），STE在这种场景下的性能尚不明确。  本工作质疑在极端LLM压缩中使用STE的有效性，并系统地研究了量化感知微调策略。我们提出PV-Tuning，一个无特定架构限制的框架，它扩展并改进了现有的微调策略，并在某些受限情况下提供收敛保证。在实际应用中，当用于1-2位矢量量化时，PV-Tuning在高性能模型如Llama和Mistral上优于先前的技术。通过使用PV-Tuning，我们在2位参数的情况下首次实现了Llama 2家族模型的帕累托最优量化。|
|**2024-05-23**|**HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models**|Bernal Jiménez Gutiérrez et.al.|[2405.14831](http://arxiv.org/abs/2405.14831)|**[link](https://github.com/osu-nlp-group/hipporag)**|为了在恶劣多变的自然环境中生存，哺乳动物的大脑发展出存储大量世界知识并不断整合新信息的能力，同时避免灾难性遗忘。尽管大型语言模型（LLMs）如带有检索增强生成（RAG）的方法在处理此类任务上已取得显著成就，但它们在大规模新经验融合方面仍面临挑战。本研究中，我们提出HippoRAG，一个受人类长期记忆海马回索引理论启发的新型检索框架，旨在促进对新经验的更深、更有效集成。HippoRAG巧妙地协同LLMs、知识图谱以及个性化PageRank算法，模拟人脑皮层和海马体在记忆中的不同作用。  我们将HippoRAG与现有RAG方法在多轮问答任务中进行比较，结果显示HippoRAG显著优于当前最先进的方法，性能提升高达20%。单步检索时，HippoRAG表现出与迭代检索方法如IRCoT相当或更好的性能，同时成本节省10-30倍，速度提升6-13倍。当将HippoRAG融入IRCoT后，还能带来额外的显著增益。最后，我们展示HippoRAG能够应对现有方法难以触及的新场景。代码和数据已在<https://github.com/OSU-NLP-Group/HippoRAG>上开源。|
|**2024-05-23**|**Can LLMs Solve longer Math Word Problems Better?**|Xin Xu et.al.|[2405.14804](http://arxiv.org/abs/2405.14804)|null|### 翻译  数学应用题（MWPs）是衡量大型语言模型（LLMs）能力的关键，但现有研究主要集中在简短背景的题目上。然而，现实生活中的数学问题往往涉及复杂情境，因此LLMs解决长篇数学应用题的能力对于其在实际场景的应用至关重要，但这一方面尚未得到充分探索。本研究首次关注Context Length Generalizability（CoLeG），即LLMs处理长篇数学应用题的能力。我们创建了Extended Grade-School Math（E-GSM）数据集，其中包含带有详细叙述的问题。为此，我们提出了两个新指标来评估LLMs在这类任务上的效能和鲁棒性。  通过对现有零样本提示方法以及商业和开源模型的考察，我们发现它们在CoLeG方面普遍存在不足。针对不同类型的LLMs，我们提出针对性的解决方案：对于专有模型，我们设计了一种新的指导性提示以减轻长文本的影响；对于开源模型，我们开发了一种数据增强任务以提升模型的适应性。我们的全面实验结果显示，我们的方法不仅在E-GSM上表现出色，而且在其他多个数学应用题基准上也展现出良好的泛化能力。  本研究的结果为未来利用LLMs处理复杂现实问题的研究提供了方向，为当前限制提出了实用解决方案，并为进一步探索模型泛化性和训练策略开辟了道路。|
|**2024-05-23**|**Lessons from the Trenches on Reproducible Evaluation of Language Models**|Stella Biderman et.al.|[2405.14782](http://arxiv.org/abs/2405.14782)|null|在自然语言处理（NLP）领域，有效评估语言模型仍然是一项未解的挑战。研究人员和工程师面临诸多方法论难题，例如模型对评估设置的敏感性、不同方法之间的比较困难，以及可重复性和透明度的缺失。本文基于三年的大型语言模型评估经验，为研究者提供指导和教训。首先，我们概述了语言模型评估中常见的问题。其次，我们阐述了应对或减轻这些问题的最佳实践。第三，我们介绍了Language Model Evaluation Harness（lm-eval）：一个开源库，旨在独立、可重复和扩展地评估语言模型，以解决这些问题。我们将介绍库的功能，并通过案例研究展示如何使用该库来缓解这些方法论关注点。|
|**2024-05-23**|**WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models**|Peng Wang et.al.|[2405.14768](http://arxiv.org/abs/2405.14768)|**[link](https://github.com/zjunlp/easyedit)**|**在大型语言模型（LLMs）中，随着世界事实的不断增长和纠正错误响应的需求，模型编辑的方法需要不断更新知识。论文的核心问题是：在编辑过程中，知识应存储在模型的哪个记忆层次更为合适。研究发现，直接修改长期记忆（模型参数）或利用工作记忆（通过检索的神经网络激活）都会导致不可逾越的三角困境——可靠性、泛化能力和局部性无法同时实现于终身编辑场景中。直接修改参数会与无关的预训练知识或先前编辑产生冲突（可靠性差、局部性不足）；而基于检索的工作记忆难以使模型理解并泛化编辑（泛化能力弱）。因此，作者提出了一个名为WISE的新方法，旨在弥合记忆之间的鸿沟。  在WISE中，设计了一种双参数内存机制，包括主内存用于存储预训练知识，侧内存用于存放编辑后的知识。仅对侧内存中的知识进行编辑，并训练一个路由器，以便根据查询决定从哪个内存中获取信息。对于持续编辑，采用了知识切片机制，将不同的编辑分布在参数的不同子空间中，然后合并到共享内存中，以避免冲突。实验结果表明，WISE在问答、幻觉生成和跨不同趋势的LLM架构（如GPT、LLaMA和Mistral）的终身模型编辑任务中表现出色，超越了先前的模型编辑方法，成功克服了上述困境。代码将在https://github.com/zjunlp/EasyEdit上发布。**|
|**2024-05-23**|**FinRobot: An Open-Source AI Agent Platform for Financial Applications using Large Language Models**|Hongyang Yang et.al.|[2405.14767](http://arxiv.org/abs/2405.14767)|**[link](https://github.com/ai4finance-foundation/finrobot)**|**随着金融机构和专业人士越来越多地将大型语言模型（LLMs）融入工作流程，金融行业与AI社区之间仍存在显著障碍，如专有数据和专业知识。这些挑战限制了AI在提升金融任务效率方面的潜力。鉴于金融分析的重要性，我们旨在开发专门针对金融的LLM驱动工具链，并通过开源项目推动其普及，促进AI在金融决策中的广泛应用。本文介绍FinRobot，一个创新的开源AI代理平台，支持多个金融专业AI代理，每个都由LLM驱动。平台主要分为四层：1）金融AI代理层，通过构建金融Chain-of-Thought（CoT）将复杂的金融问题分解为逻辑序列；2）金融LLM算法层，根据特定任务动态配置合适的模型应用策略；3）LLMOps和DataOps层，通过训练/微调技术以及使用与任务相关的数据生成精确模型；4）多源LLM基础模型层，整合各种LLM，使上述各层可以直接访问。FinRobot旨在为专业分析师和非专业人士提供实践操作，让他们能够利用强大的AI技术进行高级金融分析。FinRobot的开源代码可在此获取：\url{https://github.com/AI4Finance-Foundation/FinRobot}。**|
|**2024-05-23**|**Evaluating Large Language Models for Public Health Classification and Extraction Tasks**|Joshua Harris et.al.|[2405.14766](http://arxiv.org/abs/2405.14766)|null|随着大型语言模型（LLMs）的快速发展，人们对其在公共卫生领域支持专家工作的潜力产生了浓厚兴趣。本研究通过结合六个外部标注的和七个内部标注的数据集，评估了LLMs在处理与健康负担、流行病学风险因素和公共卫生干预相关的文本分类和提取任务上的性能。我们首先对五个开源大模型（参数量从7亿到70亿不等）进行了零样本的上下文学习测试。结果显示，Llama-3-70B-Instruct表现出色，微-F1得分在17个任务中的15项中最高。各任务间的性能差异显著，例如，有些模型如Contact Classification的得分低于60%，而像GI疾病分类这样的任务，所有模型都能达到80%以上的微-F1。对于12个任务的子集，我们还评估了GPT-4，发现其与Llama-3-70B-Instruct的结果相当，Llama-3-70B-Instruct在其中6个任务上得分更高或持平。总体而言，根据初步结果，我们发现LLMs有可能成为公共卫生专家从各种自由文本源提取信息的有效工具，有助于公共卫生监测、研究和干预措施。|
|**2024-05-23**|**Large language models can be zero-shot anomaly detectors for time series?**|Sarah Alnegheimish et.al.|[2405.14755](http://arxiv.org/abs/2405.14755)|null|近期的研究表明，大型语言模型能够执行多种任务，包括时间序列预测。这些模型的灵活性使其适用于众多应用。本文提出一项新颖的研究，探讨大型语言模型在复杂的时间序列异常检测任务中的性能。对于语言模型而言，这涉及识别输入序列（或多个部分）中的异常点，以及处理时间序列数据而非传统的文本输入。我们介绍了sigllm，一个专为时间序列异常检测设计的大型语言模型框架。该框架包含将时间序列转换为文本的模块，以及端到端的流程，用于引导语言模型进行异常检测。我们试验了两种测试大型语言模型能力的方法：一是直接提示模型指出输入中的异常元素；二是利用语言模型的预测能力来辅助检测过程。  我们在11个来自不同来源的数据集上评估了我们的框架，使用了10种不同的管道。结果显示，预测方法在所有11个数据集中都显著优于提示方法，尤其是在F1分数上。尽管大型语言模型能够发现异常，但目前的深度学习模型在性能上仍占优，其表现比大型语言模型高出30%。|
|**2024-05-21**|**Reducing Transformer Key-Value Cache Size with Cross-Layer Attention**|William Brandon et.al.|[2405.12981](http://arxiv.org/abs/2405.12981)|null|## 翻译  键值缓存对于加速Transformer架构的自回归大型语言模型（LLMs）的解码至关重要。然而，随着序列长度增加和批量大小增大，存储键值缓存所需的内存可能会变得难以承受。自从Transformer诞生以来，两个最有效的内存减小策略是多查询注意力（MQA）及其推广，群组查询注意力（GQA）。MQA和GQA通过让多个查询头共享单个键/值头，显著减少了不同键/值头的数量，同时对准确性影响较小。本文展示了如何进一步发展MQA，即在相邻层之间也共享键和值头，我们将其称为跨层注意力（CLA）。实验表明，使用CLA，可以在保持接近原始MQA精度的同时，将键值缓存的大小再减少2倍。我们在从头训练10亿参数和30亿参数模型的实验中验证了这一点，结果表明，CLA在内存与准确性之间的权衡上提供了优于传统MQA的帕累托改进，使得更长的序列长度和更大的批量大小下的推理成为可能。|
|**2024-05-21**|**Energy Rank Alignment: Using Preference Optimization to Search Chemical Space at Scale**|Shriram Chennakesavalu et.al.|[2405.12961](http://arxiv.org/abs/2405.12961)|**[link](https://github.com/rotskoff-group/llm-era)**|在化学空间中的搜索是一个极具挑战性的问题，因为可能的分子数量随着原子数量呈组合级增长。大型自回归模型通过学习化学化合物数据库已经产生了强大的生成器，但我们仍然缺乏有效策略来生成具有特定性质的分子。这个问题与大型语言模型的“对齐”问题相似，尽管在许多化学任务中，我们有一个明确且易于评估的奖励函数。本文介绍了一种名为能量排名对齐（ERA）的算法，它利用明确的奖励函数构建了一个梯度优化目标，用于调整自回归策略。理论上，我们发现该算法与Proximal Policy Optimization（PPO）和Direct Preference Optimization（DPO）密切相关，但其最小化器收敛于一个理想的吉布斯-玻尔兹曼分布，奖励函数扮演了能量角色。此外，该算法具有高度可扩展性，无需强化学习，并且在每对样本的偏好观察次数较少时，相对于DPO表现出色。  我们将这种方法应用于分子变压器的对齐，以生成具有外部指定属性的分子，并发现它能稳健地进行搜索，探索化学空间的多样化部分。虽然我们的重点在于化学搜索，但我们在一个AI监督的任务上也取得了优秀结果，表明该方法是可扩展且通用的。|
|**2024-05-21**|**Aggregation of Reasoning: A Hierarchical Framework for Enhancing Answer Selection in Large Language Models**|Zhangyue Yin et.al.|[2405.12939](http://arxiv.org/abs/2405.12939)|**[link](https://github.com/yinzhangyue/AoR)**|## 背景 近期，Chain-of-Thought提示的进展极大地推动了大型语言模型（LLMs）在复杂推理任务中的突破。当前研究通过采样多种推理路径并根据答案频率进行ensemble，提高了LLMs的推理性能。然而，这种方法在正确答案处于少数的情况时失效。我们发现这是制约LLMs推理能力的关键因素，仅凭预测答案无法解决这个问题。为此，我们提出了一个层次化的推理聚合框架AoR（推理聚合），它依据推理链条的评估来选择答案。此外，AoR引入了动态采样策略，根据任务复杂度调整推理链条的数量。  ## 任务 一系列复杂推理任务的实验结果显示，AoR相较于主流ensemble方法表现出色。进一步分析表明，AoR不仅适用于各种LLMs，而且在与现有方法的性能天花板比较中，达到了更优秀的水平。|
|**2024-05-21**|**Skin-in-the-Game: Decision Making via Multi-Stakeholder Alignment in LLMs**|Bilgehan Sel et.al.|[2405.12933](http://arxiv.org/abs/2405.12933)|null|大型语言模型在诸如总结、算术推理和问答等任务上表现出色。然而，在道德推理和伦理决策方面，尤其是在涉及多个利益相关者的复杂情景中，它们面临严峻挑战。本文提出了一种名为Skin-in-the-Game（SKIG）的框架，旨在通过从不同利益相关者角度审视决策的后果，提升语言模型在道德推理中的能力。SKIG的核心机制是模拟行动的责任感，结合同理心练习和风险评估，对提高其有效性至关重要。我们使用专有和开源语言模型在各种道德推理基准上验证SKIG的表现，并通过深入的消融分析探究其关键组件。|
|**2024-05-21**|**Code-mixed Sentiment and Hate-speech Prediction**|Anjali Yadav et.al.|[2405.12929](http://arxiv.org/abs/2405.12929)|null|在多语言环境中，混合代码（code-mixed discourse）指的是单文本中融合多种语言的现象，尤其是在官方语言多元的国家的非正式交流中常见。随着大型语言模型在自然语言处理任务中的主导地位提升，我们针对代码混合语境的研究也随之展开。首先，我们特别设计了四款新的英语-印地语和英语-斯洛文尼亚双语预训练遮罩语言模型，以适应非正式语言。接着，我们对各种类型的模型——包括单语、双语、少量语言和大规模多语言模型——在社交媒体文本的情感分析和攻击性语言检测等任务上的性能进行了评估。结果显示，最有效的分类器是针对社交媒体文本的专业化双语和多语言模型，随后是非专业的大规模多语言和单语模型，而大型生成模型的表现并不突出。对于涉及情感的问题，模型在处理代码混合数据时总体上略优于非代码混合数据。|
|**2024-05-21**|**Streamlining Software Reviews: Efficient Predictive Modeling with Minimal Examples**|Tim Menzies et.al.|[2405.12920](http://arxiv.org/abs/2405.12920)|**[link](https://github.com/timm/ez)**|该论文提出了一项新的软件分析挑战任务。在这个被称为“软件审查”的过程中，一组SME（主题专家）会评审软件行为示例，以建议如何改进软件的运行。由于SME的时间通常非常有限，理想的状况是，该团队仅通过查看少量具有高度信息价值的示例就能完成优化任务。为了支持这个审查过程，研究探索了训练预测模型的方法，该模型能够预测某个专家是否会喜欢或不喜欢下一个示例。这种预测模型可以与SME合作，引导他们探索所有示例，同时在专家离开后，模型也可以作为代理，处理新出现的案例，以应对专家们的忙碌。  在31个案例研究中（涵盖了从软件流程的高层决策到视频编码软件配置的低层决策），我们展示了仅使用12到30个标签就能建立这样的预测模型。据我们所知，仅凭少数示例（不依赖大型语言模型）就能取得这样的成果，在当前尚属罕见。遵循开放科学的原则，我们将在<https://github.com/timm/ez/tree/Stable-EMSE-paper>提供所有的代码和数据，以便他人能复制、验证或在此基础上进一步改进这些结果。|
|**2024-05-21**|**G-DIG: Towards Gradient-based DIverse and hiGh-quality Instruction Data Selection for Machine Translation**|Xingyuan Pan et.al.|[2405.12915](http://arxiv.org/abs/2405.12915)|**[link](https://github.com/xypan0/G-DIG)**|大型语言模型（LLMs）在通用场景中展现出显著能力，通过指令微调，它们能够与人类在多种任务上协同。然而，指令数据的多样性和质量是指令微调面临的两大挑战。为此，本论文提出了一种新颖的基于梯度的方法，用于自动选择机器翻译中的高质量和多样化的指令微调数据。我们的核心创新在于分析单个训练样例如何在训练过程中影响模型。通过结合影响力函数和一小部分高质量种子数据，我们选择对模型产生积极影响的样例作为高质量数据。此外，为了增加数据多样性，我们通过聚类其梯度并重采样，最大化它们对模型产生的影响多样性。在WMT22和FLORES翻译任务上的广泛实验验证了我们方法的优越性，深入分析进一步证实了其效果和泛化能力。|
|**2024-05-21**|**An Empirical Study and Analysis of Text-to-Image Generation Using Large Language Model-Powered Textual Representation**|Zhiyu Tan et.al.|[2405.12914](http://arxiv.org/abs/2405.12914)|**[link](https://github.com/llm-conditioned-diffusion/llm-conditioned-diffusion.github.io)**|一个关键的先决条件是准确理解文本输入，这对于忠实的文本到图像生成至关重要。现有的方法利用CLIP模型的文本编码器来表示提示。然而，预训练的CLIP模型仅能处理英文，且其文本编码器的模型容量相对有限。相比之下，大型语言模型（LLMs）支持多语言输入，能够处理更长的上下文，并提供更优秀的文本表示。本文研究了使用LLMs作为文本编码器以提升文本到图像生成中的语言理解能力。然而，从头开始训练包含LLMs的文本到图像生成模型需要大量的计算资源和数据。  为此，我们提出了一种三阶段训练流程，有效地整合现有文本到图像模型与LLMs，同时保持高效的训练。特别地，我们设计了一个轻量级适配器，使得能够快速使用LLMs生成的文本表示来训练文本到图像模型。大量的实验表明，我们的模型不仅支持多语言输入，还能处理更长的上下文，而且在图像生成质量上表现出色。|
|**2024-05-21**|**Topic Modelling Case Law Using a Large Language Model and a New Taxonomy for UK Law: AI Insights into Summary Judgment**|Holli Sargeant et.al.|[2405.12910](http://arxiv.org/abs/2405.12910)|**[link](https://github.com/AhmedIzzidien/TopicLLM)**|**该论文关注法律分析中的一个重要空白，通过构建和应用一种新颖的判例主题分类法，对英国的简易判决案件进行了探索。利用精心挑选的简易判决案例数据集，我们利用大型语言模型Claude 3 Opus研究功能性话题和趋势。结果显示，Claude 3 Opus在主题分类上的准确率为87.10%，揭示了不同法律领域中简易判决的明显模式。由于英国的判例法并未原始标注关键词或提供主题过滤选项，这项研究不仅深化了我们对简易判决主题本质的理解，还展示了传统方法与人工智能驱动分类方法结合的可能性。因此，本文提供了英国法律的新通用分类框架。这项工作的意义为司法行政领域的进一步研究和计算法学研究方法论讨论奠定了基础。**|
|**2024-05-21**|**Adversarial DPO: Harnessing Harmful Data for Reducing Toxicity with Minimal Impact on Coherence and Evasiveness in Dialogue Agents**|San Kim et.al.|[2405.12900](http://arxiv.org/abs/2405.12900)|null|近期，大规模语言模型（LLMs）和各种有效的训练方法的兴起推动了开放领域对话系统的发展。然而，这些模型中的毒性问题对用户体验构成重大挑战。本文提出了一种创新的训练算法——对抗式直接偏好优化（ADPO），它是在直接偏好优化（DPO）的基础上改进的。ADPO旨在训练模型增加对优选回复的概率分布，同时降低对使用有毒控制令牌生成的不安全回复的概率。研究显示，ADPO能够增强模型抵御有害对话的能力，同时尽量减少性能下降。此外，我们证明ADPO提供了比传统DPO更为稳定的训练流程。据我们所知，这是首次将有害数据直接融入生成模型的DPO变体，从而减少了人工创建安全对话数据的需求。|
|**2024-05-20**|**Adapting Large Multimodal Models to Distribution Shifts: The Role of In-Context Learning**|Guanglin Zhou et.al.|[2405.12217](http://arxiv.org/abs/2405.12217)|**[link](https://github.com/jameszhou-gl/icl-distribution-shift)**|**近期的研究表明，大型多模态模型（LMMs）在应对自然分布变化时表现出极高的鲁棒性，常常超越先前的基准。然而，领域特定的适应仍然是必要的，尤其是在医疗等专业领域。鉴于LMMs庞大的参数空间使其微调不切实际，本研究聚焦于探索上下文学习（ICL）作为一种增强LMM适应性的有效方法。我们发现，ICL的成功在很大程度上依赖于示例的选择，这与大型语言模型类似，但对面临分布变化的LMMs提出了独特挑战。为此，我们评估了一种无监督的ICL方法——TopKNearestPR，该方法通过特征相似性进行最近示例搜索来选择示例。研究揭示了这种方法在处理分布转移场景下的视觉编码器缺陷对其效果的限制。  为解决这些问题，我们提出了一种新颖的方法——InvariantSelectPR，它利用类条件对比不变性（CCI）来提升预训练视觉编码器的稳健性。CCI通过增强不同类别间的区分度并确保对领域特定变化的不变性，提高了编码器识别和检索最有信息价值示例的能力。这种方法有助于引导LMM适应新的查询样本，即使在不同的分布下也是如此。实验结果显示，InvariantSelectPR显著提高了LMM的适应性，在Camelyon17和HAM10000基准数据集上的7-shot任务中，分别实现了34.2%和16.9%的准确率提升，相对于零-shot性能，这是显著的进步。**|
|**2024-05-20**|**MathBench: Evaluating the Theory and Application Proficiency of LLMs with a Hierarchical Mathematics Benchmark**|Hongwei Liu et.al.|[2405.12209](http://arxiv.org/abs/2405.12209)|**[link](https://github.com/open-compass/mathbench)**|**随着大型语言模型（LLMs）的最新进展在数学领域取得了显著进步，传统的数学基准如GSM8k在全面评价这些模型的数学能力方面存在局限。为了弥补这一不足，我们提出了MathBench，这是一个全新基准，旨在严格评估大型语言模型的数学能力。MathBench覆盖广泛的数学学科，对理论理解和实际问题解决能力进行详尽评估。它分为五个阶段，从基础算术到大学数学，结构上设计用于考察模型在不同深度知识的理解。每个阶段包括理论问题和应用题，以衡量模型的数学熟练度及其在实际情境中应用概念的能力。MathBench的目标是提升对LLMs数学能力的评价，提供对其知识理解水平和问题解决技能的细致视角，同时支持双语环境。该项目已发布在https://github.com/open-compass/MathBench。**|
|**2024-05-20**|**Developers' Perceptions on the Impact of ChatGPT in Software Development: A Survey**|Thiago S. Vaillant et.al.|[2405.12195](http://arxiv.org/abs/2405.12195)|**[link](https://github.com/gpt-impact/Paper-content)**|随着大型语言模型（如ChatGPT）的不断发展，其强大的自然语言处理能力和广泛应用引起了广泛关注。尽管人工智能（AI）与软件工程（SE）的融合趋势日益明显，但关于这种融合如何影响软件开发实践和认知的研究仍显不足。为了揭示将AI驱动工具，如ChatGPT，融入软件开发过程的影响和挑战，我们进行了一项调查，针对207名软件开发者进行了研究。调查内容包括ChatGPT对软件质量、生产力以及开发者工作满意度的影响，同时还探讨了他们对未来ChatGPT应用的预期、对可能的工作岗位替代的担忧，以及对监管措施的看法。|
|**2024-05-20**|**CT-Eval: Benchmarking Chinese Text-to-Table Performance in Large Language Models**|Haoxiang Shi et.al.|[2405.12174](http://arxiv.org/abs/2405.12174)|null|该论文介绍了一个名为CT-Eval的中文文本转表格数据集，旨在衡量大语言模型在非英语语言环境下的文本转表格任务性能。由于现有英文文本转表格数据集主要面向英语，CT-Eval填补了这一空白，选择了一种流行的多学科中文在线百科作为来源，涵盖了28个领域以保证数据多样性。为了减少数据虚构（hallucination）问题，研究者首先训练了一个语言模型来识别并过滤掉存在虚构问题的样本，然后人工标注验证集和测试集中的错误。最终，CT-Eval包含了大约88,600个任务样本。通过CT-Eval，研究者评估了开源和闭源大语言模型（如GPT-4）的表现，结果显示零-shot模式下这些模型与人类判断仍有显著差距。经过微调后，开源模型在文本转表格能力上有了显著提升，大幅超越了GPT-4。总之，CT-Eval不仅为评估和理解现有大语言模型的中文文本转表格能力提供了有价值的工具，也为提升这类模型在这项任务上的性能提供了宝贵资源。|
|**2024-05-20**|**Fennec: Fine-grained Language Model Evaluation and Correction Extended through Branching and Bridging**|Xiaobo Liang et.al.|[2405.12163](http://arxiv.org/abs/2405.12163)|**[link](https://github.com/dropreg/fennec)**|**随着大型语言模型的迅速发展，它们在众多现实任务中的应用日益广泛，主要目标是符合人类的意图。然而，理解人类意图的复杂性使得依赖于耗时的人工评估成为必要。为了缓解这一问题，我们探讨了利用开源大型语言模型作为评估者的趋势，特别是在GPT-4的流行背景下。我们提出了一种名为\textbf{Fennec}的框架，专注于\textbf{F}ine-grained \textbf{E}valuation（细致评估）和\textbf{N}eeded \textbf{E}xtension（必要扩展）通过分支（Branching）和连接（Bridging）。分支操作将评估任务分解为不同维度和粒度，从而减轻评估挑战。同时，连接操作融合了多样化的训练数据集，增加了评估任务的多样性。实验结果显示，我们的7B模型在各种常用基准上的\textit{一致性}和\textit{一致同意}性能均优于开源的更大规模评估模型，接近GPT-4的表现。我们利用模型的精细校正功能改进多个模型响应，结果显示，这种优化提升了响应质量，在MT-Bench上提高了1-2分。我们的代码已在GitHub上开源\footnote{\url{https://github.com/dropreg/Fennec}}。**|
|**2024-05-20**|**Eliciting Problem Specifications via Large Language Models**|Robert E. Wray et.al.|[2405.12147](http://arxiv.org/abs/2405.12147)|null|这篇论文探讨了如何利用大型语言模型（LLMs）在认知系统中实现问题定义的转化。通常情况下，人类需要将问题描述转化为认知系统能理解的形式。研究者展示了LLMs能够处理自然语言中定义的问题类别，并将其转换为半形式化规格，这样现有推理和学习系统可以解决这类问题的具体实例。他们设计了一种由LLM驱动的认知任务分析师代理，这种系统能够根据自然语言描述的任务生成问题空间的定义。LLM提示源自人工智能文献中的问题空间概念和通用问题解决策略（如波利亚的《如何解决问题》）。随后，认知系统利用这些问题空间规格，结合领域通用的解决问题策略（如搜索），来解决该类问题的不同实例。这一初步结果表明，通过消除问题表述的中介过程，LLMs有可能加速认知系统的研究，同时保持其核心能力，如稳健的推理和在线学习。|
|**2024-05-20**|**MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning**|Ting Jiang et.al.|[2405.12130](http://arxiv.org/abs/2405.12130)|**[link](https://github.com/kongds/mora)**|**低秩适应是大型语言模型中流行的参数高效微调方法。在这篇论文中，我们研究了低秩更新（如LoRA实现）的影响。我们的发现指出，这种机制可能限制了大语言模型学习和记忆新知识的能力。受此启发，我们提出了一种新的方法MoRA，它利用平方矩阵实现高秩更新，同时保持与LoRA相同的可训练参数数量。为此，我们引入了相应的非参数运算器，以降低输入维度并增加输出维度处理平方矩阵。这些运算器确保权重能无缝融入到大语言模型中，使得我们的方法能够像LoRA一样部署。我们在五个任务上进行了全面评估：指令调整、数学推理、连续预训练、记忆以及预训练。在内存密集型任务上，我们的方法优于LoRA，并在其他任务上表现出相当的性能。**|
|**2024-05-20**|**Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation**|Zhankui He et.al.|[2405.12119](http://arxiv.org/abs/2405.12119)|null|大型语言模型（LLMs）正在通过出色地索引项目内容、理解复杂的对话上下文并生成相关项目标题，革新了对话推荐系统。然而，控制推荐项目的分布仍是一个挑战，导致在针对对话推荐平台的快速变化的数据分布，如项目流行度上，性能欠佳。在对话推荐中，LLMs通过自回归方式生成项目标题（作为多个令牌），这使得获取和控制所有项目推荐变得困难。因此，我们提出了一种名为“重索引-然后适应”（Reindex-Then-Adapt，RTA）的框架，它将多令牌项目标题转换为单个令牌于LLMs内，随后调整这些单令牌项目标题的概率分布。RTA框架结合了LLMs理解和复杂查询的优势，以及传统推荐系统（RecSys）在对话推荐中有效控制推荐项目分布的能力。实验结果表明，我们的框架在三个不同的对话推荐数据集和两种适应设置下，展示了改进的准确性指标。|
|**2024-05-20**|**Imp: Highly Capable Large Multimodal Models for Mobile Devices**|Zhenwei Shao et.al.|[2405.12107](http://arxiv.org/abs/2405.12107)|**[link](https://github.com/milvlg/imp)**|**尽管大型语言模型（LLMs）和大型多模态模型（LMMs）在开放世界多模态理解方面展现出惊人的能力，但它们通常参数量大、计算需求高，限制了在资源受限环境中的应用。为了应对这一问题，研究人员已经提出了一系列轻量级LMM，旨在在有限规模（如30亿参数）下最大化性能。然而，这些方法多数仅关注设计空间的单一或两个方面，对影响模型能力的关键设计选择尚未进行全面探讨。  本文系统地研究了轻量级LMM的设计，包括模型架构、训练策略和训练数据。根据我们的研究结果，我们构建了一套名为Imp的高性能LMM家族，覆盖20亿到40亿参数规模。尤其值得注意的是，我们的Imp-30亿模型在与同类规模的现有轻量级模型相比时持续领先，并超越了130亿参数规模的最新LMM状态。通过低精度量化和分辨率降低技术，Imp模型能够在高通骁龙8Gen3移动芯片上实现高速部署，每秒处理大约13个令牌的推理速度。**|
|**2024-05-20**|**DOP: Diagnostic-Oriented Prompting for Large Language Models in Mathematical Correction**|Hao Chen et.al.|[2405.12100](http://arxiv.org/abs/2405.12100)|null|## 背景 数学世界问题修正（MWPC）是一个专门针对解决数学问题过程中错误推理的修正任务。本文利用大语言模型（LLMs）的进步，关注两点：（1）区分数学推理与错误修正；（2）探索策略以提升LLMs在数学领域的错误修正能力，以应对MWPC任务。我们注意到，在实时教育中，帮助学生识别错误比单纯提供正确答案更为关键。然而，当前研究往往侧重于获取精确的解题答案，而非纠正可能的错误。因此，我们调整了研究范式，表明提升数学推理能力并不等同于精通错误修正。同时，我们提出了一种名为诊断导向提示（DOP）的新方法，旨在促进LLMs在错误修正方面表现出色。实验结果显示，DOP表现出卓越性能，彰显其重要性。我们强调，在数学教育中，对出色修正者的需要超过了对熟练推理者的追求。代码和数据可在<https://github.com/ChenhaoEcnuCS/Reason-Correct>获取。|
|**2024-05-17**|**A Survey on Large Language Models with Multilingualism: Recent Advances and New Frontiers**|Kaiyu Huang et.al.|[2405.10936](http://arxiv.org/abs/2405.10936)|**[link](https://github.com/kaiyuhwang/mllm-survey)**|**随着大型语言模型（LLMs）的快速发展，在自然语言处理领域展现出显著的多语言能力，引起了学术界和业界的广泛关注。为了减少潜在的歧视并提升技术的通用性和可访问性，对于多语言技术的发展至关重要。尽管LLMs取得了突破，但对多语言场景的深入研究仍显不足。因此，迫切需要一份全面的综述，总结近期的方法、进展、局限性和可能的解决方案。本文旨在从多个角度审视LLMs在多语言环境中的应用。我们首先回顾了预训练语言模型研究的历史演变。接着，我们探讨了LLMs的多语言特性，包括训练和推理方法、模型安全、跨领域与文化适应以及数据集使用。我们还分析了这些方面面临的挑战，并提出可能的解决策略。此外，我们指出了未来的研究方向，以进一步提升LLMs的多语言性能。本综述旨在帮助研究界应对多语言问题，提供一个关于基于LLMs的多语言自然语言处理核心概念、关键技术及最新进展的全面理解。**|
|**2024-05-17**|**The Local Interaction Basis: Identifying Computationally-Relevant and Sparsely Interacting Features in Neural Networks**|Lucius Bushnaq et.al.|[2405.10928](http://arxiv.org/abs/2405.10928)|**[link](https://github.com/apolloresearch/rib)**|### 概述  机械解释性目标是通过逆向工程理解神经网络的行为。然而，现有方法在解析神经网络激活方面面临挑战，因为缺乏对激活的分解，使得单个神经元或模型组件无法清晰对应于独特的特征或功能。为此，我们提出了一种新颖的可解释性方法——局部交互基（Local Interaction Basis，LIB）。LIB旨在通过消除无关激活和交互，识别计算特征。该方法摒弃无意义的激活方向，并使基础与相邻层间雅可比矩阵的奇异向量对齐。同时，它根据特征对后续计算的重要性进行缩放，生成一个显示模型中所有计算相关特性和交互的图谱。  我们在模块加法和CIFAR-10模型上评估了LIB的有效性，结果表明，相比于主成分分析，LIB能识别出更多计算相关的特征，并呈现出更稀疏的交互。然而，在应用于语言模型时，LIB并未显著提高可解释性或交互稀疏度。因此，我们得出结论，尽管LIB是一种有前景的理论驱动方法，但当前形式并不适用于大型语言模型。|
|**2024-05-17**|**COGNET-MD, an evaluation framework and dataset for Large Language Model benchmarks in the medical domain**|Dimitrios P. Panagoulias et.al.|[2405.10893](http://arxiv.org/abs/2405.10893)|null|这篇技术论文阐述了COGNET-MD，一个专为医疗领域设计的大型语言模型评估的新基准。我们提出了一种评分框架，旨在评估语言模型理解医学文本的能力，并且设计了一系列难度分级的多项选择题（MCQ）数据库。这个数据库由多个医疗领域的专家合作创建，以反映当前医学趋势，确保安全、实用和适用性。初期版本包含了精神科、牙科、肺病学、皮肤科和内分泌学等领域的题目，但会持续扩展，未来还会加入更多医学学科。|
|**2024-05-17**|**Application of Artificial Intelligence in Schizophrenia Rehabilitation Management: Systematic Literature Review**|Hongyi Yang et.al.|[2405.10883](http://arxiv.org/abs/2405.10883)|null|该综述旨在系统地评估人工智能（AI）在精神分裂症患者康复管理中的现状和前景，以及其对康复过程的影响。我们从2012年至现在筛选了70项研究，重点关注机器学习、深度学习、强化学习等技术在心理健康干预和管理中的应用、技术类别、产品和数据类型，如生态瞬时评估、行为和语音数据的分析。结果显示，AI在症状监测、复发风险预测和康复治疗中具有广泛的应用潜力。此外，本研究还探讨了基于AI的新兴产品、技术和分析方法，如社交媒体分析、严肃游戏和大型语言模型在康复中的潜在挑战和未来发展方向。总的来说，这篇论文系统回顾了AI在精神分裂症康复管理中的应用，并为未来的研究路径提供了有价值的见解和建议。|
|**2024-05-17**|**The Future of Large Language Model Pre-training is Federated**|Lorenzo Sani et.al.|[2405.10853](http://arxiv.org/abs/2405.10853)|null|## 背景  生成式预训练大型语言模型（LLMs）因其在众多任务上的出色表现而备受瞩目，这得益于它们所接受的海量训练数据。根据已建立的规模法则，LLMs未来性能的提升在很大程度上依赖于我们能够利用的计算和数据资源。联邦学习（FL）有可能释放全球大部分未充分利用的数据和计算能力，这些是当前以数据中心为中心的LLM训练方法所忽视的。本文提出了一种稳健、灵活且可复现的FL方法，旨在促进机构间的大规模协作，共同训练LLMs，从而动员更多的计算和数据资源，甚至可能达到或超越中心化的性能。  ## 任务  我们的工作展示了一种FL训练方法，它能够在有限资源下扩展到百亿元级的联邦LLM，使得拥有丰富数据的实体能够成为预训练LLMs的主导力量，而不是仅让计算资源丰富的机构独占鳌头。这种方法强调了联邦训练的规模效益，并为实现这一目标提供了一种实用路径。|
|**2024-05-17**|**Large Language Model (LLM) for Telecommunications: A Comprehensive Survey on Principles, Key Techniques, and Opportunities**|Hao Zhou et.al.|[2405.10825](http://arxiv.org/abs/2405.10825)|null|随着大型语言模型（LLMs）因其卓越的理解和推理能力而备受瞩目，它们在各个领域取得了显著进步，尤其在第六代（6G）通信技术的推动下展现出人工智能通用性（AGI）的潜力。本研究旨在全面概述LLM赋能的电信网络。首先，我们概述了LLMs的基础，包括模型架构、预训练、微调、推理与应用、模型评估，以及在电信部署中的运用。接着，我们将探讨LLM支持的关键技术和电信应用，涉及生成、分类、优化和预测问题。生成应用包括电信领域知识、代码和网络配置自动生成。基于LLM的分类任务涵盖网络安全、文本、图像和流量分类。此外，我们介绍了利用LLMs的自动化优化技术，如强化学习的奖励函数设计和口语强化学习。对于预测问题，LLMs可用于时间序列预测和多模态电信预测。最后，我们指出了LLM赋能电信网络所面临的挑战，并展望了未来的研究方向。|
|**2024-05-17**|**ActiveLLM: Large Language Model-based Active Learning for Textual Few-Shot Scenarios**|Markus Bayer et.al.|[2405.10808](http://arxiv.org/abs/2405.10808)|null|主动学习旨在通过优先处理最能提升学习效果的实例来减少标注工作量。然而，许多主动学习策略面临“冷启动”问题，即在初期需要大量数据才能发挥效能，这限制了它们在预训练模型（如BERT）上的应用，这些模型在少量样本情况下已表现良好。为此，我们提出了一种新颖的主动学习方法——ActiveLLM，它利用大型语言模型（如GPT-4、Llama 3和Mistral Large）进行实例选择。实验证明，ActiveLLM显著提高了BERT分类器在少量样本情况下的性能，超越了传统主动学习方法和SetFit等少数样本学习方法。此外，ActiveLLM还能扩展到非少量样本场景，支持迭代选择，从而帮助其他主动学习策略克服冷启动难题。结果表明，ActiveLLM为改善不同学习环境中的模型性能提供了有前景的解决方案。|
|**2024-05-17**|**Empowering Small-Scale Knowledge Graphs: A Strategy of Leveraging General-Purpose Knowledge Graphs for Enriched Embeddings**|Albert Sawczyn et.al.|[2405.10745](http://arxiv.org/abs/2405.10745)|null|### 翻译  知识密集型任务对机器学习（ML）技术提出了严峻挑战。通常采用的方法，如大型语言模型（LLMs），在处理这类任务时往往存在局限性。然而，人们已经努力通过知识图谱（KG）来弥补这些不足，尤其是通过将小规模的领域特定KG与通用KG相结合。尽管KG在知识表示方面具有优势，但构建它们的成本可能阻碍了广泛的研究和应用。为此，我们提出了一种框架，旨在通过链接到大规模通用KG来提升小型领域特定KG嵌入的学习性能。实验结果显示，这种方法带来了显著的提升，例如，Hits@10指标最高提高了44%。这一相对未被充分探索的研究方向有望促进KG在知识密集型任务中的更频繁运用，从而产生更为稳健、可靠的ML解决方案，它们相较于流行但易出错的LLM方法更具可靠性。关键词：知识图谱、知识图谱补全、实体对齐、表示学习、机器学习|
|**2024-05-17**|**Efficient Multimodal Large Language Models: A Survey**|Yizhang Jin et.al.|[2405.10739](http://arxiv.org/abs/2405.10739)|**[link](https://github.com/lijiannuist/efficient-multimodal-llms-survey)**|**在过去一年里，多模态大型语言模型（Multimodal Large Language Models，MLLMs）在诸如视觉问答、视觉理解和推理等任务上展现出卓越性能。然而，这些模型的庞大规模和高昂的训练与推理成本限制了它们在学术界和工业界的广泛应用。因此，研究高效且轻量级的MLLM具有巨大的潜力，特别是在边缘计算环境中。本综述全面系统地回顾了当前高效MLLM的研究现状。我们概述了代表性高效模型的发展历程，总结了有效结构和策略的研究状态，以及其实用应用。最后，我们讨论了当前高效MLLM研究的局限，并展望了有前景的未来发展方向。如需更多信息，请参考我们的GitHub仓库：https://github.com/lijiannuist/Efficient-Multimodal-LLMs-Survey。**|
|**2024-05-17**|**INDUS: Effective and Efficient Language Models for Scientific Applications**|Bishwaranjan Bhattacharjee et.al.|[2405.10725](http://arxiv.org/abs/2405.10725)|null|大型通用语言模型在自然语言处理任务上表现出色。然而，先前的研究表明，针对特定领域的训练数据可以使模型在专业任务上表现更佳。为此，我们开发了INDUS，一套专为地球科学、生物学、物理学、太阳物理、行星科学和天文学领域设计的定制化语言模型。这些模型基于精心挑选的科学语料库，包括：（1）一个使用领域专用词汇和数据集训练的编码器，用于提升自然语言理解任务的表现；（2）一个基于对比学习的通用文本嵌入模型，利用多源数据集进行训练，以优化信息检索任务；（3）通过知识蒸馏技术缩小规模的模型，适用于对延迟和资源有限的应用。此外，我们创建了三个新的科学基准数据集：CLIMATE-CHANGE-NER（实体识别）、NASA-QA（抽取式问答）和NASA-IR（信息检索），以推动跨学科领域的研究进展。最后，实验结果显示，我们的模型在新任务和相关领域现有基准任务上均优于通用编码器（如RoBERTa）和现有的领域特定编码器（如SciBERT）。|
|**2024-05-16**|**UniRAG: Universal Retrieval Augmentation for Multi-Modal Large Language Models**|Sahel Sharifymoghaddam et.al.|[2405.10311](http://arxiv.org/abs/2405.10311)|null|## 背景  近期，多模态（MM）大型语言模型（LLMs）已经解锁了许多需要多模态理解（如图像描述或视觉问答）和生成（如文本引导的图像生成或编辑）复杂任务。为了进一步提升MM-LLMs的输出质量，我们提出了一种模型通用的UniRAG技术，它在推理阶段将相关检索信息添加到提示中，作为少量样例。与普遍认为检索增强（RA）主要改进罕见实体的生成或理解不同，我们在MSCOCO数据集上对包括GPT4、Gemini-Pro在内的专有模型以及Llava、LaVIT和Emu2等开源小型模型进行了评估，结果显示，这些模型在输入提示通过MM检索器（如UniIR模型）增强后，显著提高了生成质量。|
|**2024-05-16**|**4D Panoptic Scene Graph Generation**|Jingkang Yang et.al.|[2405.10305](http://arxiv.org/abs/2405.10305)|**[link](https://github.com/jingkang50/psg4d)**|**我们生活在一个三维空间中，同时通过第四维时间向前推进。为了使人工智能能够全面理解这种4D环境，我们提出了一种新的表示形式——4D全景场景图（PSG-4D），它将动态4D世界中的原始视觉数据抽象为节点和边，节点代表具有精确位置和状态信息的实体，边捕捉时间关系。为了促进在这一新领域的研究，我们构建了一个丰富的注释PSG-4D数据集，包含3000个RGB-D视频，总计100万帧，每帧都带有4D全景分割掩码以及详细的动态场景图标签。我们为此任务提出了一种名为PSG4DFormer的Transformer模型，该模型能够预测全景分割掩码，沿时间轴跟踪掩码，并通过关系组件生成相应的场景图。在新数据集上的大量实验表明，我们的方法为未来的PSG-4D研究提供了一个强大的基准。最后，我们展示了如何通过将大型语言模型融入我们的PSG-4D系统来实现动态场景理解的一个实际应用示例。**|
|**2024-05-16**|**HW-GPT-Bench: Hardware-Aware Architecture Benchmark for Language Models**|Rhea Sanjay Sukthanker et.al.|[2405.10299](http://arxiv.org/abs/2405.10299)|**[link](https://github.com/automl/hw-aware-llm-bench)**|**随着语言模型的规模不断扩大，对硬件指标（如延迟、能耗、GPU内存使用和性能）之间的权衡需求日益增长。人们正在寻求为不同语言模型配置建立帕累托前沿，以在指定硬件限制下找到最优模型。然而，对多种架构在多台设备上的全面训练和评估在计算上是不可行的。为此，我们提出了HW-GPT-Bench，这是一个基于硬件感知的语言模型代理基准，利用神经架构搜索（NAS）中的权重共享技术，在一个模型中高效地训练包含不同规模语言模型的超网络。我们在13种设备上对这些模型进行了性能剖析，考虑了5种硬件指标和3种不同的模型规模。最后，我们通过8种不同的多目标NAS算法展示了HW-GPT-Bench的可用性，并评估了由此产生的帕累托前沿的质量。我们的目标是推动和加速大型语言模型的多目标方法，如NAS和结构化剪枝的研究。**|
|**2024-05-16**|**Timeline-based Sentence Decomposition with In-Context Learning for Temporal Fact Extraction**|Jianhao Chen et.al.|[2405.10288](http://arxiv.org/abs/2405.10288)|**[link](https://github.com/jianhaochen-nju/tsdre)**|**摘要：**  事实抽取对于构建知识图谱至关重要。随着对时间相关事实在下游任务中的需求增长，出现了时间性事实抽取的任务。本文特别关注从自然语言文本中提取时间性事实。先前的研究未能妥善处理复杂句子中时间与事实对应关系的建立难题。为解决这一挑战，我们提出了一种基于时间线的句子分解策略，利用大语言模型（LLMs）进行上下文学习，以实现对事实相关时间线的精细理解。然而，直接使用LLMs进行时间性事实抽取的性能并不理想。因此，我们引入了TSDRE方法，将LLMs的分解能力融入到小型预训练语言模型（PLMs）的传统微调过程中。  为了支持评估，我们构建了一个复杂的时序事实抽取数据集ComplexTRED。实验结果显示，TSDRE在HyperRED-Temporal和ComplexTRED数据集上实现了最先进的性能。|
|**2024-05-16**|**Revisiting OPRO: The Limitations of Small-Scale LLMs as Optimizers**|Tuo Zhang et.al.|[2405.10276](http://arxiv.org/abs/2405.10276)|null|近年来，许多研究旨在通过策略性提示提升大型语言模型（LLMs）的效能。特别是优化通过prompting（OPRO）方法表现出顶尖性能，它利用LLMs作为优化器，目标是寻找能最大化任务准确性的指令。本论文重新审视了OPRO在小型LLMs（如LaMa-2系列和Mistral 7B）上的自动化提示效果。我们的研究表明，对于小型LLMs，OPRO的效果有限，因为其有限的推理能力限制了优化潜力。因此，我们建议未来的自动提示工程应同时考虑模型能力和计算成本。针对小型LLMs，我们推荐直接提供明确阐述目标和方法的指令，作为稳健的提示基线，以确保在当前研究中实现高效且有效的提示设计。|
|**2024-05-16**|**Keep It Private: Unsupervised Privatization of Online Text**|Calvin Bao et.al.|[2405.10260](http://arxiv.org/abs/2405.10260)|**[link](https://github.com/csbao/kip-privatization)**|**## 背景  作者身份混淆技术有望通过自动重写文本来保护网络通信中的个人隐私。然而，在自然语言处理（NLP）文献中，这些技术的评估大多局限在狭小场景下，主要依赖于表面的编辑操作，可能导致输出不自然。本研究提出了一种自动文本私密化框架，通过强化学习对大型语言模型进行微调，以生成兼顾准确、连贯和隐私的重写。我们在大规模的英语Reddit帖子测试集上进行了详尽的评估，该数据集由68,000名作者撰写，包含短到中等长度的文本。我们探讨了在不同评估条件下，如作者简介长度和作者识别策略，性能的变化。我们的方法在自动化指标和人工评估中保持高文本质量，并成功地规避了几种自动作者识别攻击。**|
|**2024-05-16**|**When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models**|Xianzheng Ma et.al.|[2405.10255](http://arxiv.org/abs/2405.10255)|**[link](https://github.com/activevisionlab/awesome-llm-3d)**|随着大型语言模型（LLMs）的不断发展，它们与三维空间数据（3D-LLMs）的融合取得了显著进步，这极大地增强了理解和互动物理环境的能力。这篇综述详细探讨了使LLMs能够处理、理解并生成三维数据的方法论，强调了LLMs的独特优势，如上下文学习、逐步推理、开放词汇能力和丰富的世界知识，这些将极大地推动人工智能体在空间理解与交互方面的发展。研究覆盖了从点云到神经辐射场（NeRF）等各种三维数据表示，并考察了它们与LLMs在任务中的结合，如三维场景理解、描述、问答和对话，以及基于LLM的代理进行空间推理、规划和导航。此外，我们还简要回顾了其他结合三维和语言的方法。本文的元分析显示了显著的进步，但也指出了挖掘3D-LLMs全部潜力所需的创新方法的必要性。因此，本文旨在为未来的研究方向提供指导，探索和扩展3D-LLMs在理解和互动复杂三维世界的能力。为了支持本调查，我们已在GitHub上建立了一个项目页面，整理并列出了相关论文：https://github.com/ActiveVisionLab/Awesome-LLM-3D。|
|**2024-05-16**|**A Systematic Evaluation of Large Language Models for Natural Language Generation Tasks**|Xuanfan Ni et.al.|[2405.10251](http://arxiv.org/abs/2405.10251)|null|近期的研究已评估了大型语言模型（LLMs）在常识推理、数学推理和代码生成等方面的能力。然而，据我们所知，尚无专门针对自然语言生成（NLG）任务的深入研究，这是衡量模型优秀程度的关键标准。因此，本论文旨在全面评估知名且性能出色的LLMs，包括ChatGPT、ChatGLM、基于T5的模型、基于LLaMA的模型和Pythia模型，在对话生成和文本总结等NLG任务中的表现。我们选择了涵盖英语和中文的数据集，并设计了一种共同的评估框架，包括输入模板和后处理策略。研究结果报告了自动评分，同时进行了详细分析。|
|**2024-05-16**|**IntelliExplain: Enhancing Interactive Code Generation through Natural Language Explanations for Non-Professional Programmers**|Hao Yan et.al.|[2405.10250](http://arxiv.org/abs/2405.10250)|null|大型语言模型（LLMs）在根据自然语言描述自动生成可执行代码方面展现出巨大潜力，特别是通过互动功能，用户可以通过迭代反馈指导模型。然而，当前的互动方式往往假设用户具备调试源代码的专业知识，对非专业程序员不太友好。这使得使互动代码生成对不同编程水平的个体更易于使用成为一个挑战。为解决这个问题，我们提出了IntelliExplain，这是一种创新的人机交互范式，通过让用户通过自然语言解释与源代码互动，提升非专业人士的体验。用户通过提供他们发现错误的自然语言纠正反馈，来指导系统修订代码，直到用户对系统的代码解释感到满意。我们的用户研究显示，使用IntelliExplain的用户在Text-to-SQL和Python代码生成任务中的成功率分别比纯GPT-3.5提高了11.6%和25.3%，同时所需时间分别减少了39.0%和15.6%。|
|**2024-05-16**|**CPsyExam: A Chinese Benchmark for Evaluating Psychology using Examinations**|Jiahao Zhao et.al.|[2405.10212](http://arxiv.org/abs/2405.10212)|null|在这篇论文中，我们提出了一种创新的心理学基准测试——CPsyExam，它源于中国语言考试的问题。CPsyExam旨在分别强调心理学知识和案例分析的重要性，认识到将心理学知识应用于实际情境的价值。从22,000个问题库中，我们精选了4,000个来构建该基准，确保了主题的均衡覆盖，并包含了各种案例分析方法的多样性。此外，我们对一系列现有的大型语言模型（LLMs）进行了评估，包括开源和API基础的模型。实验和分析结果显示，CPsyExam是一个有效的确立语言模型对心理学理解能力的基准，同时支持在不同粒度上比较这些模型。|

<p align=right>(<a href=#updated-on-20240724>back to top</a>)</p>

## rag

|Publish Date|Title|Authors|PDF|Code|abstract|
|---|---|---|---|---|---|
|**2024-07-23**|**Retrieve, Generate, Evaluate: A Case Study for Medical Paraphrases Generation with Small Language Models**|Ioana Buhnila et.al.|[2407.16565](http://arxiv.org/abs/2407.16565)|null|近期，大型语言模型（LLMs）对大众的普及性激增，这可能导致此类模型在医疗建议方面的不可追踪使用。通过LLMs进行语言生成存在两个主要问题：首先，它们容易产生幻觉，因此在任何医疗用途上都需要科学和事实依据；其次，由于其庞大的模型规模，LLMs对计算资源构成了巨大挑战。在此工作中，我们引入了pRAGe，一个用于利用小型语言模型（SLM）进行检索增强生成和医学短语生成评估的管道。我们研究了SLMs在法语医学短语生成中的有效性以及外部知识库的影响。  以下是翻译成中文的摘要：  近期，大型语言模型（LLMs）对公众的广泛可访问性激增，这可能导致这类模型在医疗相关建议方面出现无法追踪的应用。通过大型语言模型进行语言生成存在两大关键问题：首先，这些模型易于产生“幻觉”，因此在任何医疗用途中，它们需要有科学性和事实性的支撑；其次，由于其巨大的模型尺寸，大型语言模型对计算资源带来了严峻挑战。在本工作中，我们引入了pRAGe，这是一个针对小型语言模型（SLM）的检索增强生成及医学短语生成评价流程。我们探讨了小型语言模型在法语医学短语生成上的效果，以及外部知识库对于此过程的影响。|
|**2024-07-22**|**NV-Retriever: Improving text embedding models with effective hard-negative mining**|Gabriel de Souza P. Moreira et.al.|[2407.15831](http://arxiv.org/abs/2407.15831)|null|文本嵌入模型在信息检索应用中非常流行，如语义搜索和基于检索增强生成(RAG)的问题解答系统。这些模型通常是经过对比学习目标微调的Transformer模型。尽管许多论文介绍了新的嵌入模型架构和训练方法，但是一个关键环节——负样本段落的挖掘过程——仍然没有得到充分的探索或描述。微调嵌入模型的一个挑战是选择高质量的硬负样本用于对比学习。在这篇论文中，我们提出了一种正相关感知挖掘方法家族，利用正相关得分来更有效地去除假阴性结果。此外，我们还提供了一个全面的消融研究，探讨了不同配置下的硬负样本挖掘方法，包括不同的教师模型和基础模型。通过引入NV-Retriever-v1模型，我们证明了所提出方法的有效性，该模型在MTEB检索(BEIR)基准测试中得分为60.9，比先前的方法高出0.65分。该模型在2024年7月7日发布到MTEB检索时排名首位。  请注意，上述翻译已尽可能保持原文信息的准确性和完整性，并避免使用","字符。|
|**2024-07-22**|**MoRSE: Bridging the Gap in Cybersecurity Expertise with Retrieval Augmented Generation**|Marco Simoni et.al.|[2407.15748](http://arxiv.org/abs/2407.15748)|null|在这篇论文中，我们介绍了MoRSE（混合RAG安全专家），这是首个专门针对网络安全领域的人工智能聊天机器人。MoRSE旨在提供全面且完整的网络安全知识。它使用了两个RAG（检索增强生成）系统来从多维度的网络安全环境中检索和组织信息。与传统的RAG不同，MoRSE采用并行检索器协同工作，以从不同格式和结构中检索语义相关的信息。与依赖于参数化知识库的传统大型语言模型(LLM)不同，MoRSE根据用户查询从非参数化知识库中检索相关文档，并利用这些信息生成精确答案。此外，MoRSE受益于其知识库的实时更新，无需重新训练即可实现持续的知识丰富。我们已对MoRSE的有效性进行了评估，将其与其他最先进的LLM进行了比较，在600个特定的网络安全问题上测试了该系统。实验评价表明，MoRSE在答案的相关性和正确性方面比已知解决方案如GPT-4和Mixtral 7x8提高了超过10%。|
|**2024-07-22**|**TaskGen: A Task-Based, Memory-Infused Agentic Framework using StrictJSON**|John Chong Min Tan et.al.|[2407.15734](http://arxiv.org/abs/2407.15734)|null|TaskGen是一个开源的代理框架，它通过将任意任务分解成子任务，利用代理来解决这些任务。每个子任务被映射到一个装备函数或另一个代理来执行。为了减少冗余（从而减少令牌使用），TaskGen采用了StrictJSON，确保大型语言模型(LLM)的JSON输出，并具有额外的功能，如类型检查和迭代错误校正。TaskGen的核心理念是基于需求的信息/记忆管理。我们对TaskGen在多种环境中的表现进行了实证评估，包括40x40动态迷宫导航（障碍物位置变化，解决率100%）、TextWorld逃脱房间解决（密集奖励和详细目标，解决率96%）、网络浏览（69%的动作成功）、解决MATH数据集（100个Level-5问题的解决率为71%）、在NaturalQuestions数据集上的检索增强生成（F1分数为47.03%）。|
|**2024-07-22**|**RadioRAG: Factual Large Language Models for Enhanced Diagnostics in Radiology Using Dynamic Retrieval Augmented Generation**|Soroosh Tayebi Arasteh et.al.|[2407.15621](http://arxiv.org/abs/2407.15621)|null|大型语言模型(LLM)在医学人工智能领域取得了显著进展。然而，基于静态训练数据集，LLM往往会产生过时或不准确的信息。检索增强生成(RAG)通过整合外部数据源来缓解这一问题。以往的RAG系统使用预组装、固定数据库，灵活性有限，我们开发了放射学RAG（RadioRAG）作为端到端框架，实现实时从权威放射学在线资源检索数据。我们使用专门的放射学问答数据集（RadioQA）评估RadioRAG，测试不同LLM在接入额外在线信息的情况下回答放射学特定问题的诊断准确性。利用来自RSNA案例集合的80个问题，涵盖放射学亚专科，以及24个额外专家策划的问题，其中正确答案已知，我们对LLM（包括GPT-3.5-turbo、GPT-4、Mistral-7B、Mixtral-8x7B和Llama3[8B和70B]）进行测试，比较是否使用RadioRAG的情况。RadioRAG实时从www.radiopaedia.org检索上下文相关的信息，并将其纳入回复中。在所有LLM中，RadioRAG一致提高了诊断准确性，相对改进幅度从2%到54%不等。它在放射学亚专科中的表现与无RAG情况相当或更优，尤其在乳腺影像学和急诊放射学方面。然而，改进程度在各模型间有所不同；GPT-3.5-turbo和Mixtral-8x7B-instruct-v0.1有显著提升，而Mistral-7B-instruct-v0.2未见改善，这凸显了其效果的变异性。当提供超出训练数据的领域特定数据时，LLM的表现会得到提升。对于放射学而言，RadioRAG建立了一个强大的框架，显著提高了放射学问题解答的诊断准确性和事实性。|
|**2024-07-22**|**An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought**|Yuetong Zhao et.al.|[2407.15569](http://arxiv.org/abs/2407.15569)|null|自2022年末ChatGPT问世以来，以ChatGPT为代表的生成式对话模型迅速成为日常生活中的必备工具。随着用户期望的提升，增强生成式对话模型解决复杂问题的能力已成为当前研究的热点。本文深入探讨了RAFT（检索增强微调）方法在提升生成式对话模型性能方面的有效性。RAFT方法融合了链式思维、模型监督微调(SFT)和检索增强生成(RAG)，显著提升了模型的信息提取与逻辑推理能力。我们对RAFT方法在多个数据集上的表现进行了评估，并分析了其在各类推理任务中的性能，包括长篇问答和短篇问答任务，涵盖中英文任务，以及支持性和比较性推理任务。尤其值得一提的是，它填补了以往研究中关于长篇问答任务和中文数据集的空白。此外，我们还评估了RAFT方法中链式思维(CoT)带来的益处。本研究为致力于提升生成式对话模型性能的研究者提供了宝贵的见解。|
|**2024-07-22**|**Decoding BACnet Packets: A Large Language Model Approach for Packet Interpretation**|Rashi Sharma et.al.|[2407.15428](http://arxiv.org/abs/2407.15428)|null|工业控制系统（ICS）环境涵盖了各种复杂的通信协议，这对负责监控、解析和应对网络活动与安全事件的安全运营中心（SOC）分析师构成了重大挑战。传统监控工具和技术往往难以清晰揭示ICS特定通信的本质和意图。为了提升理解力，我们提出了一种基于大型语言模型（LLM）的软件解决方案。该方案目前专注于BACnet协议，通过处理数据包文件并利用映射数据库及现代上下文检索方法（如增强检索生成（RAG）），提取上下文信息。处理后的数据包信息与提取到的上下文结合，作为输入提供给LLM，进而生成简洁的数据包文件概要供用户参考。该软件能提供清晰、连贯且易于理解的网络活动概览，助力SOC分析师更准确地评估控制系统的当前状态。|
|**2024-07-22**|**Customized Retrieval Augmented Generation and Benchmarking for EDA Tool Documentation QA**|Yuan Pu et.al.|[2407.15353](http://arxiv.org/abs/2407.15353)|null|增强检索生成（RAG）通过从外部数据库获取事实信息，提高了生成式AI模型的准确性和可靠性，在文档支持的问题解答（QA）任务中得到了广泛应用。现成的RAG流程虽然在通用文档上预训练效果良好，但在应用到电子设计自动化（EDA）等知识密集型垂直领域时面临重大挑战。本文针对这一问题，提出了一种定制化的RAG框架，并结合了三种针对EDA工具文档QA的领域特定技术：一种用于文本嵌入模型微调的对比学习方案、一个从专有大型语言模型（LLM）提炼出的重排序器，以及一个使用高质量领域语料库微调的生成式LLM。此外，我们开发并开源了一个针对OpenROAD——一个先进的RTL到GDSII设计平台——的文档QA评估基准，称为ORD-QA。实验结果表明，我们提出的RAG流程和技巧在ORD-QA及商业工具上的表现优于现有技术。我们的ORD-QA基准和定制化RAG流程的训练数据集可在https://github.com/lesliepy99/RAG-EDA公开获取。|
|**2024-07-20**|**Automatic Generation of Fashion Images using Prompting in Generative Machine Learning Models**|Georgia Argyrou et.al.|[2407.14944](http://arxiv.org/abs/2407.14944)|**[link](https://github.com/georgiarg/autofashion)**|**人工智能的兴起在时尚产业引发了革命性的变革，以前所未有的方式重新定义了创新和创造力。本文探讨了使用两种大型语言模型和一种稳定扩散模型生成定制化时尚描述的方法，重点关注AI驱动的时尚创意的灵活性。我们摒弃传统方法，专注于提示技术，如零样本和少量样本学习，以及思维链(Chain-of-Thought, CoT)，这些技术能产生各种颜色和纹理，从而提高输出的多样性。我们方法的核心是检索增强生成(Retrieval-Augmented Generation, RAG)，通过从时尚资源中获取见解来丰富模型，确保其表现的现代性。评估结合了CLIPscore等定量指标与定性的人类判断，突出了在不同风格中创意、连贯性和审美吸引力的优势。在参与者中，RAG和少量样本学习技术因其能够生成更相关和吸引人的时尚描述而受到青睐。我们的代码可在https://github.com/georgiarg/AutoFashion找到。**|
|**2024-07-20**|**Retrieval Augmented Generation Integrated Large Language Models in Smart Contract Vulnerability Detection**|Jeffy Yu et.al.|[2407.14838](http://arxiv.org/abs/2407.14838)|null|去中心化金融（DeFi）的迅速发展伴随着因智能合约漏洞导致的重大财务损失，突显了安全审计的重要性和紧迫性。随着攻击事件的频发，对审计服务的需求和重要性日益增加，尤其是对于独立开发者和小企业来说，他们往往面临资金有限的问题，难以承担这些服务的费用。本研究在此背景下，通过结合检索增强生成（RAG）技术和大型语言模型（LLM），特别是利用GPT-4-1106及其128k令牌上下文窗口的能力，构建了一个包含830个已知易受攻击合约的向量存储库。我们使用Pinecone进行向量存储，OpenAI的text-embedding-ada-002进行嵌入，并借助LangChain搭建RAG-LLM管道。设计的提示旨在提供关于漏洞检测的二元答案。首先，我们对52个智能合约进行了测试，每个合约针对特定的漏洞类型进行了40次测试，以验证RAG-LLM的可重复性和一致性。实验结果令人鼓舞，指导下的漏洞检测成功率达到了62.7%。其次，在一个“盲审”设置下，即不向模型提供漏洞类型的情况下，对219个合约进行了每份40次的测试，以此评估模型在无明确上下文提示下的通用漏洞检测能力。在这种情况下，观察到的成功率为60.71%。尽管这些结果是积极的，但我们仍强调当前阶段人工审计的必要性。本研究提供了一种成本效益高的智能合约审计流程的概念验证，朝着实现更广泛的民主化安全访问迈进。  请注意，尽管结果令人鼓舞，我们仍需强调目前阶段人工审计的重要性。本研究提供了一种概念验证，展示了一条通往更具成本效益、更广泛可及的智能合约安全审计路径，旨在推动安全审计的民主化进程。|
|**2024-07-20**|**Differential Privacy of Cross-Attention with Provable Guarantee**|Jiuxiang Gu et.al.|[2407.14717](http://arxiv.org/abs/2407.14717)|null|交叉注意力机制已成为众多重要人工智能应用中的核心模块，例如检索增强生成(RAG)、系统提示、引导稳定扩散等。保障交叉注意力的隐私安全至关重要且需求迫切，因为其关键和价值矩阵可能包含关于公司及其用户的敏感信息，其中许多公司的盈利完全依赖于其系统提示或RAG数据。在本工作中，我们设计了一种新颖的差分隐私(DP)数据结构，以理论保证解决交叉注意力的隐私安全问题。具体而言，设 $n$为系统提示/RAG数据的输入标记长度，$d$为特征维度，$0 < \alpha \le 1$为相对误差参数，$R$为查询和键矩阵的最大值，$R_w$为价值矩阵的最大值，而$r$,$s$,$\epsilon_s$为多项式核方法的参数。那么，我们的数据结构需要$\widetilde{O}(ndr^2)$内存消耗，具有$\widetilde{O}(nr^2)$初始化时间复杂度和$\widetilde{O}(\alpha^{-1} r^2)$单个标记查询时间复杂度。此外，我们的数据结构可以确保用户查询是$(\epsilon, \delta)$-DP，与真实答案相比，我们的输出具有$\widetilde{O}(n^{-1} \epsilon^{-1} \alpha^{-1/2} R^{2s} R_w r^2)$的加性误差和$n^{-1} (\alpha + \epsilon_s)$ 的相对误差。更进一步，我们的结果对适应性查询具有鲁棒性，即用户可以有意攻击交叉注意力系统。据我们所知，这是首次为交叉注意力提供DP的工作。我们相信这可以激发在大型生成模型(LGMs)中更多的隐私算法设计。|
|**2024-07-19**|**ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities**|Peng Xu et.al.|[2407.14482](http://arxiv.org/abs/2407.14482)|null|在本研究中，我们引入了ChatQA 2，一个基于Llama3的模型，旨在弥合开放访问的大语言模型(LLM)与领先的专有模型（如GPT-4-Turbo）在长文本理解及检索增强生成(RAG)能力方面的差距。这两种能力对于LLM处理无法一次性装入提示的大信息量至关重要，并且根据下游任务和计算预算，它们是互补的。我们详细介绍了一种持续训练方法，该方法将Llama3-70B-base的上下文窗口从8K扩展至128K令牌，并通过三阶段指令调优流程来提升模型的指令遵循、RAG性能以及长文本理解能力。我们的结果显示，Llama3-ChatQA-2-70B模型在许多长文本理解任务上的准确度可与GPT-4-Turbo-2024-0409相媲美，在RAG基准上甚至超越后者。有趣的是，我们发现最先进的长文本检索器可以缓解RAG中的top-k上下文碎片化问题，进一步改善了针对长文本理解任务的RAG结果。此外，我们还提供了使用最先进的长文本LLM对RAG和长文本解决方案进行的广泛比较。|
|**2024-07-19**|**Conditioning Chat-GPT for information retrieval: the Unipa-GPT case study**|Irene Siragusa et.al.|[2407.14246](http://arxiv.org/abs/2407.14246)|null|本论文阐述了Unipa-GPT的架构和训练过程，这是一款基于大型语言模型开发的聊天机器人，旨在帮助学生在巴勒莫大学选择本科或硕士课程。Unipa-GPT基于gpt-3.5-turbo模型，在欧洲研究人员之夜（SHARPER夜）上首次亮相。实验中，我们采用了检索增强生成（RAG）方法以及微调技术来构建系统。本文全面展示了Unipa-GPT的整体架构，对比分析了RAG和微调系统的性能，并进行了简要讨论。此外，还与其他大型语言模型进行了比较，并报告了SHARPER夜期间的实验结果。|
|**2024-07-19**|**AuditNet: A Conversational AI-based Security Assistant [DEMO]**|Shohreh Deldari et.al.|[2407.14116](http://arxiv.org/abs/2407.14116)|null|在信息过载的时代，各领域专业人士面临着处理大量文档和不断演变标准的挑战。确保遵守标准、法规和合同义务是跨专业领域的关键但复杂的任务。我们提出了一种灵活的对话式AI助手框架，旨在促进不同领域的合规性检查，包括但不限于网络基础设施、法律合同、教育标准、环境法规和政府政策。通过利用大型语言模型的检索增强生成技术，我们的框架实现了相关、情境感知信息的自动化审查、索引和检索，简化了验证既定指导原则和要求遵守性的过程。此AI助手不仅减少了合规检查的手动工作量，还提高了准确性和效率，支持各领域专业人士保持高标准实践，并确保其各自领域的法规遵从性。我们提出并展示了AuditNet，这是第一个对话式AI安全助手，旨在协助物联网网络安全专家即时获取安全标准、政策和法规。  在信息爆炸的今天，各行各业的专业人士都面临着海量文件与日新月异的标准所带来的挑战。在众多领域中，确保符合各项标准、规定及合同条款成为了一项至关重要的同时又极其复杂的任务。为此，我们设计了一个多用途的对话型人工智能助手框架，专门用于辅助不同行业的合规性审查工作，适用范围广泛，涵盖了网络基础设施、法律文书、教育规范、环保法规以及政府政策等多个领域。本框架借助大规模语言模型的检索增强生成技术，自动执行信息的审查、分类与检索，且能提供与情境高度匹配的内容，极大地简化了对现有规则与要求遵循情况的核查流程。该人工智能助手不仅能显著降低人工审核合规性的劳动强度，还能提高审核的精确度与效率，帮助各行业专家保持高水平的职业操守，确保其所在领域的法律法规得到严格执行。在此基础上，我们提出了AuditNet——首款针对物联网网络安全专家的对话式AI安全助手，它能够即时提供安全标准、政策法规等信息，以助专家一臂之力。|
|**2024-07-19**|**RAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval Augmented Question Answering**|Rujun Han et.al.|[2407.13998](http://arxiv.org/abs/2407.13998)|**[link](https://github.com/awslabs/rag-qa-arena)**|**基于检索增强生成的问答（RAG-QA）是自然语言处理领域的重要研究课题，在现实世界中有广泛的应用。然而，现有的大多数数据集要么只使用单一来源的语料库构建，要么由简短的摘录答案组成，这在评估大型语言模型（LLM）驱动的RAG-QA系统跨领域泛化能力方面存在不足。为了解决这些限制，我们创建了长形式RobustQA（LFRQA），一个新数据集，它包含人类编写的长形式答案，将来自多个文档的短摘录答案整合成一个连贯的故事，涵盖了26,000个查询和七个不同领域的大型语料库。我们进一步提出了RAG-QA竞技场，通过直接比较模型生成的答案与LFRQA中的答案，使用LLM作为评估者。我们通过广泛的实验表明，RAG-QA竞技场和人类对答案质量的判断高度相关。此外，最具有竞争力的LLM的答案中，只有41.3%被偏好于LFRQA的答案，这证明了RAG-QA竞技场是一个对未来研究充满挑战的评估平台。**|
|**2024-07-18**|**PRAGyan -- Connecting the Dots in Tweets**|Rahul Ravi et.al.|[2407.13909](http://arxiv.org/abs/2407.13909)|null|随着社交媒体平台的不断扩展，理解事件和声明背后的根本原因对商业、政策制定者和研究者变得至关重要。本研究探讨了通过结合知识图谱（KGs）与大型语言模型（LLMs）来对推特数据集进行因果分析的方法。传统的LLM辅助分析技术在揭示驱动观察效果的原因方面往往深度不足。通过利用编码丰富语义关系和时间信息的知识图谱与LLMs，本研究旨在揭示影响因果动态的复杂因素，并与使用GPT-3.5 Turbo的结果进行比较。我们采用了一种基于检索增强生成（RAG）的模型，利用存储在Neo4j（又名PRAGyan）数据格式中的知识图谱来检索因果推理的相关上下文。我们的方法表明，当源语料库的大小增加时，结合了知识图谱的LLM RAG模型能提供更优的结果。我们的定性分析突出了将知识图谱与LLMs结合以提高可解释性和行动指导见解的优势，这促进了不同领域的知情决策制定。同时，定量分析使用诸如BLEU和余弦相似度等指标显示，我们的方法比基线模型高出10%的性能。  请注意，上述内容是对原始英文摘要的翻译。|
|**2024-07-18**|**Visual Haystacks: Answering Harder Questions About Sets of Images**|Tsung-Han Wu et.al.|[2407.13766](http://arxiv.org/abs/2407.13766)|**[link](https://github.com/visual-haystacks/vhs_benchmark)**|**最近，大型多模态模型（LMMs）在单图视觉问答领域取得了显著进展。然而，这些模型在处理跨越大量图像的查询时面临重大挑战，这类似于在现实生活中的大规模相册搜索、跨互联网获取特定信息或通过卫星图像监控环境变化等场景。本文探讨了一种新的任务：多图视觉问答（MIQA），即给定一组大量图像和自然语言问题，任务的目标是生成相关且基于图像的响应。我们提出了一项新的公共基准测试“视觉堆栈（VHs）”，专门用于评估LMMs在面对大量不相关图像集合时的视觉检索和推理能力。我们在该基准上进行了全面评估，结果表明即使是强大的闭源模型也在此类任务上表现不佳。  为了应对这些不足，我们引入了MIRAGE（多图检索增强生成），这是一种专为LMMs设计的新型检索/问答框架，它有效地解决了MIQA的难题，并在准确性和效率方面显著超越了基线方法。我们的评估结果显示，MIRAGE在VHs基准上的表现比闭源GPT-4o模型高出至多11%，并且相比文本中心的多阶段方法，在效率上提高了高达3.4倍。**|
|**2024-07-18**|**Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models**|Zhuo Chen et.al.|[2407.13757](http://arxiv.org/abs/2407.13757)|null|增强生成式（RAG）模型在解决大型语言模型的幻觉问题和实时约束方面展现出优势，但同时也暴露出对检索腐败攻击的脆弱性。先前的研究主要聚焦于RAG在白盒和封闭领域问答任务中的不可靠性。本文旨在揭示RAG模型在面对黑盒攻击进行观点操纵时的脆弱性，深入探讨此类攻击对用户认知与决策的影响，为提升RAG模型的可靠性和安全性提供新视角。我们通过指令操纵RAG模型中的检索排名结果，并利用这些结果训练代理模型。借助针对代理模型的对抗性检索攻击方法，实现了对RAG的黑盒转移攻击。在多个主题的意见数据集上的实验表明，所提出的攻击策略能够显著改变RAG生成内容的观点极性，这不仅揭示了模型的脆弱性，更重要的是，突显了其对用户认知和决策潜在的负面影响，使误导用户接受错误或偏见信息变得更为容易。|
|**2024-07-18**|**Can Open-Source LLMs Compete with Commercial Models? Exploring the Few-Shot Performance of Current GPT Models in Biomedical Tasks**|Samy Ateia et.al.|[2407.13511](http://arxiv.org/abs/2407.13511)|**[link](https://github.com/samyateia/bioasq2024)**|**在自然语言处理（NLP）的各个领域，商业大型语言模型（LLM），如OpenAI的GPT-4支持的ChatGPT和Anthropic的Claude 3 Opus，一直在主导着基准测试。新的开源替代品，如Mixtral 8x7B和Llama 3，已经出现，似乎正在缩小差距，同时往往提供更高的吞吐量和更低的使用成本。开源LLM还可以自我托管，这使它们对需要处理敏感数据但不希望由第三方处理的企业和临床用例具有吸引力。我们参与了第12届BioASQ挑战赛，这是一个检索增强生成（RAG）设置，并探索了当前GPT模型Claude 3 Opus、GPT-3.5-turbo和Mixtral 8x7b在零样本、少量样本学习以及QLoRa微调下的性能。我们还研究了从维基百科添加的相关知识是否能改善LLM在上下文窗口中的表现。在10样本设置下，Mixtral 8x7b与微调或无微调的情况下都具有竞争力，但在零样本设置下未能产生可用结果。QLoRa微调和维基百科上下文并未带来可测量的性能提升。我们的结果表明，在RAG设置中，商业模型和开源模型之间的性能差距主要存在于零样本设置中，而通过简单地为特定领域的用例收集少量样本示例即可弥补这一差距。重新运行这些实验所需的代码可通过GitHub获得。**|
|**2024-07-19**|**Retrieval-Augmented Generation for Natural Language Processing: A Survey**|Shangyu Wu et.al.|[2407.13193](http://arxiv.org/abs/2407.13193)|null|大型语言模型(LLM)在各个领域展现出了巨大成功，这得益于其庞大的参数量能够存储大量知识。然而，LLM仍然面临一些关键问题，如幻觉问题、知识更新难题以及缺乏特定领域的专业知识。检索增强生成(RAG)的出现，通过利用外部知识库来增强LLM，有效弥补了LLM的这些不足。本文全面回顾了RAG的所有重要技术，特别是检索器和检索融合方面。此外，我们提供了实现RAG代表性技术的教程代码。进一步地，本文探讨了RAG的训练方法，包括有无数据仓库更新的RAG。然后，我们介绍了RAG在自然语言处理任务及工业场景中的应用。最后，本文讨论了RAG未来的发展方向和挑战，以促进其持续发展。|
|**2024-07-18**|**Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with an Iterative Approach**|Zhouyu Jiang et.al.|[2407.13101](http://arxiv.org/abs/2407.13101)|null|多轮问题回答是一个具有显著工业应用价值的挑战性任务，基于大型语言模型的检索增强生成(RAG)方法已成为解决这一任务的流行方案。由于在单次迭代中可能无法检索到所有必要信息，一系列迭代式RAG方法应运而生，并展现出显著的性能提升。然而，现有方法仍面临两大关键挑战：多次检索导致的情境过载，以及因缺乏记录的检索轨迹而引发的过度规划和重复规划。本文提出了一种名为ReSP的新型迭代RAG方法，该方法配备了一个双功能摘要器。此摘要器同时针对整体问题与当前子问题，对从检索文档中提取的信息进行压缩。实验结果表明，在HotpotQA和2WikiMultihopQA这两个多轮问题回答数据集上，我们的方法显著超越了现有最佳水平，并在情境长度方面展现了优秀的鲁棒性。|
|**2024-07-17**|**Explainable Biomedical Hypothesis Generation via Retrieval Augmented Generation enabled Large Language Models**|Alexander R. Pelletier et.al.|[2407.12888](http://arxiv.org/abs/2407.12888)|null|当前，海量的生物医学信息对研究者有效消化、处理和理解这些发现构成了重大挑战。大型语言模型(LLMs)作为应对这一复杂数据景观的强大工具应运而生。然而，LLMs可能导致产生幻觉性的回应，因此，增强检索生成(RAG)对于实现信息准确性至关重要。在此协议中，我们介绍了RUGGED(基于图引导的可解释疾病区分的检索增强)，这是一个全面的工作流程，旨在支持研究者整合知识和生成假设，识别经过验证的前进路径。通过文本挖掘关联分析和在疾病节点上的可解释图预测模型，从出版物和知识库中审查、整合和提取相关生物医学信息，预测药物与疾病之间的潜在联系。这些分析，加上生物医学文本，被整合到一个框架中，该框架通过RAG增强的LLMs促进用户指导的机制阐明和假设探索。一个临床案例展示了RUGGED评估和推荐Arrhythmogenic Cardiomyopathy(ACM)和Dilated Cardiomyopathy(DCM)治疗方案的能力，分析了处方药物的分子相互作用和未开发的用途。该平台最小化了LLMs的幻觉现象，提供了可操作的见解，并改进了新型治疗药物的研究过程。|
|**2024-07-17**|**AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases**|Zhaorun Chen et.al.|[2407.12784](http://arxiv.org/abs/2407.12784)|**[link](https://github.com/BillChan226/AgentPoison)**|**大型语言模型(LLM)代理在各种应用中展现出卓越性能，这主要归功于它们在推理、利用外部知识和工具、调用API以及执行与环境交互的动作等方面的先进能力。当前的代理通常采用记忆模块或检索增强生成(RAG)机制，从知识库中检索具有相似嵌入的过往知识和实例，以辅助任务规划和执行。然而，依赖未经验证的知识库引发了对其安全性和可信度的重大担忧。为了揭示这些脆弱性，我们提出了一种新颖的红队方法——AgentPoison，这是首个针对通用和RAG基LLM代理的后门攻击，通过毒化其长期记忆或RAG知识库实现。具体而言，我们将触发生成过程构建成一个受约束优化问题，以优化后门触发器，将其映射到一个独特的嵌入空间，确保一旦用户指令包含优化后的后门触发器，高度可能从被毒化的记忆或知识库中检索到恶意示例。同时，不含触发器的良性指令仍保持正常性能。与传统后门攻击不同，AgentPoison无需额外的模型训练或微调，优化后的后门触发器展现出优越的可转移性、上下文连贯性和隐蔽性。大量实验表明，AgentPoison在攻击三种现实世界中的LLM代理方面有效：基于RAG的自动驾驶代理、知识密集型问答代理以及医疗保健EHR代理。在每种代理上，AgentPoison的平均攻击成功率超过80%，对良性性能的影响微乎其微(不到1%)，且毒化率低于0.1%。**|
|**2024-07-17**|**EchoSight: Advancing Visual-Language Models with Wiki Knowledge**|Yibin Yan et.al.|[2407.12735](http://arxiv.org/abs/2407.12735)|null|基于知识的视觉问答（KVQA）任务要求在广泛背景知识的基础上回答关于图像的问题。尽管取得了显著进展，生成模型在这些任务上往往表现不佳，主要是因为它们难以有效整合外部知识。在这篇论文中，我们引入了一种名为EchoSight的新型多模态检索增强生成（RAG）框架，它使大型语言模型（LLMs）能够利用精细的百科全书式知识来回答视觉问题。为了实现高性能的检索，EchoSight首先使用纯视觉信息搜索维基文章，随后根据与文本-图像查询的关联度对候选文章进行重新排序。这种方法显著提升了多模态知识的融合，从而提高了检索效果和VQA答案的准确性。我们在Encyclopedic VQA和InfoSeek数据集上的实验结果表明，EchoSight在基于知识的VQA领域创下了新的最佳记录，分别在这两个数据集上实现了41.8%和31.3%的准确率。|
|**2024-07-18**|**Search Engines, LLMs or Both? Evaluating Information Seeking Strategies for Answering Health Questions**|Marcos Fernández-Pichel et.al.|[2407.12468](http://arxiv.org/abs/2407.12468)|null|传统的传统的搜索引擎传统的搜索引擎一直是传统的搜索引擎一直是信息搜索的主要工具。传统的搜索引擎一直是信息搜索的主要工具。然而，最近大型语言模型（LL传统的搜索引擎一直是信息搜索的主要工具。然而，最近大型语言模型（LLM）在多种任务上展现出卓越传统的搜索引擎一直是信息搜索的主要工具。然而，最近大型语言模型（LLM）在多种任务上展现出卓越的能力，特别是在作为问答系统方面的应用传统的搜索引擎一直是信息搜索的主要工具。然而，最近大型语言模型（LLM）在多种任务上展现出卓越的能力，特别是在作为问答系统方面的应用正变得越来越普遍。预计基于LL传统的搜索引擎一直是信息搜索的主要工具。然而，最近大型语言模型（LLM）在多种任务上展现出卓越的能力，特别是在作为问答系统方面的应用正变得越来越普遍。预计基于LLM的对话系统和传统的网络引擎传统的搜索引擎一直是信息搜索的主要工具。然而，最近大型语言模型（LLM）在多种任务上展现出卓越的能力，特别是在作为问答系统方面的应用正变得越来越普遍。预计基于LLM的对话系统和传统的网络引擎将在未来共存，以各种方式传统的搜索引擎一直是信息搜索的主要工具。然而，最近大型语言模型（LLM）在多种任务上展现出卓越的能力，特别是在作为问答系统方面的应用正变得越来越普遍。预计基于LLM的对话系统和传统的网络引擎将在未来共存，以各种方式支持终端用户。但关于这两种系统的传统的搜索引擎一直是信息搜索的主要工具。然而，最近大型语言模型（LLM）在多种任务上展现出卓越的能力，特别是在作为问答系统方面的应用正变得越来越普遍。预计基于LLM的对话系统和传统的网络引擎将在未来共存，以各种方式支持终端用户。但关于这两种系统的有效性，特别是在促进准确的信息搜索方面传统的搜索引擎一直是信息搜索的主要工具。然而，最近大型语言模型（LLM）在多种任务上展现出卓越的能力，特别是在作为问答系统方面的应用正变得越来越普遍。预计基于LLM的对话系统和传统的网络引擎将在未来共存，以各种方式支持终端用户。但关于这两种系统的有效性，特别是在促进准确的信息搜索方面，需要更多的科学研究。在这项研究传统的搜索引擎一直是信息搜索的主要工具。然而，最近大型语言模型（LLM）在多种任务上展现出卓越的能力，特别是在作为问答系统方面的应用正变得越来越普遍。预计基于LLM的对话系统和传统的网络引擎将在未来共存，以各种方式支持终端用户。但关于这两种系统的有效性，特别是在促进准确的信息搜索方面，需要更多的科学研究。在这项研究中，我们专注于它们在回答健康传统的搜索引擎一直是信息搜索的主要工具。然而，最近大型语言模型（LLM）在多种任务上展现出卓越的能力，特别是在作为问答系统方面的应用正变得越来越普遍。预计基于LLM的对话系统和传统的网络引擎将在未来共存，以各种方式支持终端用户。但关于这两种系统的有效性，特别是在促进准确的信息搜索方面，需要更多的科学研究。在这项研究中，我们专注于它们在回答健康问题上的优势。我们进行了广泛的研究传统的搜索引擎一直是信息搜索的主要工具。然而，最近大型语言模型（LLM）在多种任务上展现出卓越的能力，特别是在作为问答系统方面的应用正变得越来越普遍。预计基于LLM的对话系统和传统的网络引擎将在未来共存，以各种方式支持终端用户。但关于这两种系统的有效性，特别是在促进准确的信息搜索方面，需要更多的科学研究。在这项研究中，我们专注于它们在回答健康问题上的优势。我们进行了广泛的研究，比较了不同的网络搜索引擎、LL传统的搜索引擎一直是信息搜索的主要工具。然而，最近大型语言模型（LLM）在多种任务上展现出卓越的能力，特别是在作为问答系统方面的应用正变得越来越普遍。预计基于LLM的对话系统和传统的网络引擎将在未来共存，以各种方式支持终端用户。但关于这两种系统的有效性，特别是在促进准确的信息搜索方面，需要更多的科学研究。在这项研究中，我们专注于它们在回答健康问题上的优势。我们进行了广泛的研究，比较了不同的网络搜索引擎、LLM和增强检索（RAG）传统的搜索引擎一直是信息搜索的主要工具。然而，最近大型语言模型（LLM）在多种任务上展现出卓越的能力，特别是在作为问答系统方面的应用正变得越来越普遍。预计基于LLM的对话系统和传统的网络引擎将在未来共存，以各种方式支持终端用户。但关于这两种系统的有效性，特别是在促进准确的信息搜索方面，需要更多的科学研究。在这项研究中，我们专注于它们在回答健康问题上的优势。我们进行了广泛的研究，比较了不同的网络搜索引擎、LLM和增强检索（RAG）方法。我们的研究揭示了一些有趣的结果传统的搜索引擎一直是信息搜索的主要工具。然而，最近大型语言模型（LLM）在多种任务上展现出卓越的能力，特别是在作为问答系统方面的应用正变得越来越普遍。预计基于LLM的对话系统和传统的网络引擎将在未来共存，以各种方式支持终端用户。但关于这两种系统的有效性，特别是在促进准确的信息搜索方面，需要更多的科学研究。在这项研究中，我们专注于它们在回答健康问题上的优势。我们进行了广泛的研究，比较了不同的网络搜索引擎、LLM和增强检索（RAG）方法。我们的研究揭示了一些有趣的结果。例如，我们发现针对健康问题传统的搜索引擎一直是信息搜索的主要工具。然而，最近大型语言模型（LLM）在多种任务上展现出卓越的能力，特别是在作为问答系统方面的应用正变得越来越普遍。预计基于LLM的对话系统和传统的网络引擎将在未来共存，以各种方式支持终端用户。但关于这两种系统的有效性，特别是在促进准确的信息搜索方面，需要更多的科学研究。在这项研究中，我们专注于它们在回答健康问题上的优势。我们进行了广泛的研究，比较了不同的网络搜索引擎、LLM和增强检索（RAG）方法。我们的研究揭示了一些有趣的结果。例如，我们发现针对健康问题可能提供答案的网页质量并不会随着传统的搜索引擎一直是信息搜索的主要工具。然而，最近大型语言模型（LLM）在多种任务上展现出卓越的能力，特别是在作为问答系统方面的应用正变得越来越普遍。预计基于LLM的对话系统和传统的网络引擎将在未来共存，以各种方式支持终端用户。但关于这两种系统的有效性，特别是在促进准确的信息搜索方面，需要更多的科学研究。在这项研究中，我们专注于它们在回答健康问题上的优势。我们进行了广泛的研究，比较了不同的网络搜索引擎、LLM和增强检索（RAG）方法。我们的研究揭示了一些有趣的结果。例如，我们发现针对健康问题可能提供答案的网页质量并不会随着排名列表的下移而下降。传统的搜索引擎一直是信息搜索的主要工具。然而，最近大型语言模型（LLM）在多种任务上展现出卓越的能力，特别是在作为问答系统方面的应用正变得越来越普遍。预计基于LLM的对话系统和传统的网络引擎将在未来共存，以各种方式支持终端用户。但关于这两种系统的有效性，特别是在促进准确的信息搜索方面，需要更多的科学研究。在这项研究中，我们专注于它们在回答健康问题上的优势。我们进行了广泛的研究，比较了不同的网络搜索引擎、LLM和增强检索（RAG）方法。我们的研究揭示了一些有趣的结果。例如，我们发现针对健康问题可能提供答案的网页质量并不会随着排名列表的下移而下降。然而，根据我们的评估，与LL传统的搜索引擎一直是信息搜索的主要工具。然而，最近大型语言模型（LLM）在多种任务上展现出卓越的能力，特别是在作为问答系统方面的应用正变得越来越普遍。预计基于LLM的对话系统和传统的网络引擎将在未来共存，以各种方式支持终端用户。但关于这两种系统的有效性，特别是在促进准确的信息搜索方面，需要更多的科学研究。在这项研究中，我们专注于它们在回答健康问题上的优势。我们进行了广泛的研究，比较了不同的网络搜索引擎、LLM和增强检索（RAG）方法。我们的研究揭示了一些有趣的结果。例如，我们发现针对健康问题可能提供答案的网页质量并不会随着排名列表的下移而下降。然而，根据我们的评估，与LLM相比，网络引擎在寻找正确传统的搜索引擎一直是信息搜索的主要工具。然而，最近大型语言模型（LLM）在多种任务上展现出卓越的能力，特别是在作为问答系统方面的应用正变得越来越普遍。预计基于LLM的对话系统和传统的网络引擎将在未来共存，以各种方式支持终端用户。但关于这两种系统的有效性，特别是在促进准确的信息搜索方面，需要更多的科学研究。在这项研究中，我们专注于它们在回答健康问题上的优势。我们进行了广泛的研究，比较了不同的网络搜索引擎、LLM和增强检索（RAG）方法。我们的研究揭示了一些有趣的结果。例如，我们发现针对健康问题可能提供答案的网页质量并不会随着排名列表的下移而下降。然而，根据我们的评估，与LLM相比，网络引擎在寻找正确答案来回答健康问题上的准确性较低传统的搜索引擎一直是信息搜索的主要工具。然而，最近大型语言模型（LLM）在多种任务上展现出卓越的能力，特别是在作为问答系统方面的应用正变得越来越普遍。预计基于LLM的对话系统和传统的网络引擎将在未来共存，以各种方式支持终端用户。但关于这两种系统的有效性，特别是在促进准确的信息搜索方面，需要更多的科学研究。在这项研究中，我们专注于它们在回答健康问题上的优势。我们进行了广泛的研究，比较了不同的网络搜索引擎、LLM和增强检索（RAG）方法。我们的研究揭示了一些有趣的结果。例如，我们发现针对健康问题可能提供答案的网页质量并不会随着排名列表的下移而下降。然而，根据我们的评估，与LLM相比，网络引擎在寻找正确答案来回答健康问题上的准确性较低。另一方面，我们还发现LLM传统的搜索引擎一直是信息搜索的主要工具。然而，最近大型语言模型（LLM）在多种任务上展现出卓越的能力，特别是在作为问答系统方面的应用正变得越来越普遍。预计基于LLM的对话系统和传统的网络引擎将在未来共存，以各种方式支持终端用户。但关于这两种系统的有效性，特别是在促进准确的信息搜索方面，需要更多的科学研究。在这项研究中，我们专注于它们在回答健康问题上的优势。我们进行了广泛的研究，比较了不同的网络搜索引擎、LLM和增强检索（RAG）方法。我们的研究揭示了一些有趣的结果。例如，我们发现针对健康问题可能提供答案的网页质量并不会随着排名列表的下移而下降。然而，根据我们的评估，与LLM相比，网络引擎在寻找正确答案来回答健康问题上的准确性较低。另一方面，我们还发现LLM对输入提示非常敏感，同时，传统的搜索引擎一直是信息搜索的主要工具。然而，最近大型语言模型（LLM）在多种任务上展现出卓越的能力，特别是在作为问答系统方面的应用正变得越来越普遍。预计基于LLM的对话系统和传统的网络引擎将在未来共存，以各种方式支持终端用户。但关于这两种系统的有效性，特别是在促进准确的信息搜索方面，需要更多的科学研究。在这项研究中，我们专注于它们在回答健康问题上的优势。我们进行了广泛的研究，比较了不同的网络搜索引擎、LLM和增强检索（RAG）方法。我们的研究揭示了一些有趣的结果。例如，我们发现针对健康问题可能提供答案的网页质量并不会随着排名列表的下移而下降。然而，根据我们的评估，与LLM相比，网络引擎在寻找正确答案来回答健康问题上的准确性较低。另一方面，我们还发现LLM对输入提示非常敏感，同时，我们也发现RAG导致了高度有效的传统的搜索引擎一直是信息搜索的主要工具。然而，最近大型语言模型（LLM）在多种任务上展现出卓越的能力，特别是在作为问答系统方面的应用正变得越来越普遍。预计基于LLM的对话系统和传统的网络引擎将在未来共存，以各种方式支持终端用户。但关于这两种系统的有效性，特别是在促进准确的信息搜索方面，需要更多的科学研究。在这项研究中，我们专注于它们在回答健康问题上的优势。我们进行了广泛的研究，比较了不同的网络搜索引擎、LLM和增强检索（RAG）方法。我们的研究揭示了一些有趣的结果。例如，我们发现针对健康问题可能提供答案的网页质量并不会随着排名列表的下移而下降。然而，根据我们的评估，与LLM相比，网络引擎在寻找正确答案来回答健康问题上的准确性较低。另一方面，我们还发现LLM对输入提示非常敏感，同时，我们也发现RAG导致了高度有效的信息搜索方法。|
|**2024-07-17**|**Optimizing Query Generation for Enhanced Document Retrieval in RAG**|Hamin Koo et.al.|[2407.12325](http://arxiv.org/abs/2407.12325)|null|大型语言模型（LLMs）在各种语言任务中表现出色，但它们经常生成不正确的信息，这种现象被称为“幻觉”。增强检索生成（RAG）旨在通过使用文档检索来提供准确的回答来缓解这一问题。然而，即使使用RAG，由于查询的模糊性，幻觉仍然存在。本研究的目标是通过优化查询生成，使用查询-文档对齐分数，利用大型语言模型细化查询以提高文档检索的准确性和效率，从而改进RAG。实验表明，我们的方法提高了文档检索的质量，平均准确率提高了1.6%。|
|**2024-07-16**|**Mindful-RAG: A Study of Points of Failure in Retrieval Augmented Generation**|Garima Agrawal et.al.|[2407.12216](http://arxiv.org/abs/2407.12216)|null|大型语言模型（LLMs）擅长生成连贯且与上下文相关联的文本，但在处理知识密集型查询和特定领域的事实性问答任务时面临挑战。通过融合外部知识源，如结构化知识图谱（KGs），检索增强生成（RAG）系统缓解了这一问题。然而，即使能够访问从知识图谱中提取的包含必要事实的信息，LLMs在生成准确答案时仍显得力不从心。本研究针对这一困境，通过对现有基于KG的RAG方法中的错误模式进行分析，识别出了八个关键的失败点。我们发现，这些错误主要源于未能充分理解问题意图以及未能从知识图谱事实中有效收集相关背景信息。基于这一分析，我们提出了Mindful-RAG方法，这是一种旨在实现基于意图和语境对齐的知识检索的框架。该方法直接针对已识别的失败点，提高了LLMs提供回答的正确性和相关性，标志着相较于现有方法的重要进步。  请注意，上述内容是根据您的要求翻译的论文摘要的中文版本，未包含任何无关内容，并且确保了输出内容中没有包含","字符。如果您需要进一步的帮助或有其他要求，请随时告诉我。|

<p align=right>(<a href=#updated-on-20240724>back to top</a>)</p>

